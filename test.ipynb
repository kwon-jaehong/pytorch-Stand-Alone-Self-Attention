{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from model import ResNet50, ResNet38, ResNet26\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## 데이터 셋은 CIFAR10으로\n",
    "num_classes = 10\n",
    "batch_size = 10\n",
    "num_workers = 4\n",
    "\n",
    "## 데이터셋 로드\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=(0.4914, 0.4822, 0.4465),\n",
    "        std=(0.2470, 0.2435, 0.2616)\n",
    "    )\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=(0.4914, 0.4822, 0.4465),\n",
    "        std=(0.2470, 0.2435, 0.2616)\n",
    "    )\n",
    "])\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('data', train=True, download=True, transform=transform_train),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('data', train=False, transform=transform_test),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem = False\n",
    "\n",
    "model = ResNet26(num_classes=num_classes, stem=stem)\n",
    "model = model.cuda()\n",
    "\n",
    "# {'img_size': 32, 'batch_size': 10, 'num_workers': 0, 'epochs': 100, 'print_interval': 100, 'cuda': True,  'gpu_devices': None, 'gpu': None, 'rank': 0, 'world_size': 1, 'dist_backend': 'nccl', 'dist_url': 'tcp://127.0.0.1:3456'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 1\n",
    "best_acc = 0.0\n",
    "\n",
    "lr = 0.1\n",
    "momentum =0.9\n",
    "weight_decay = 0.0001\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1   5000  /  20 batch Loss:  17.49565315246582   ACC:  0.0 %\n",
      "Epoch  1   5000  /  40 batch Loss:  3.066394329071045   ACC:  10.0 %\n",
      "Epoch  1   5000  /  60 batch Loss:  2.4563772678375244   ACC:  0.0 %\n",
      "Epoch  1   5000  /  80 batch Loss:  2.42293119430542   ACC:  0.0 %\n",
      "Epoch  1   5000  /  100 batch Loss:  2.229726552963257   ACC:  30.0 %\n",
      "Epoch  1   5000  /  120 batch Loss:  2.2393100261688232   ACC:  10.0 %\n",
      "Epoch  1   5000  /  140 batch Loss:  2.459568977355957   ACC:  10.0 %\n",
      "Epoch  1   5000  /  160 batch Loss:  2.280266284942627   ACC:  10.0 %\n",
      "Epoch  1   5000  /  180 batch Loss:  2.539658546447754   ACC:  0.0 %\n",
      "Epoch  1   5000  /  200 batch Loss:  2.365104913711548   ACC:  10.0 %\n",
      "Epoch  1   5000  /  220 batch Loss:  2.332296371459961   ACC:  0.0 %\n",
      "Epoch  1   5000  /  240 batch Loss:  2.2190282344818115   ACC:  30.0 %\n",
      "Epoch  1   5000  /  260 batch Loss:  2.2361652851104736   ACC:  0.0 %\n",
      "Epoch  1   5000  /  280 batch Loss:  2.38037371635437   ACC:  20.0 %\n",
      "Epoch  1   5000  /  300 batch Loss:  2.427809715270996   ACC:  10.0 %\n",
      "Epoch  1   5000  /  320 batch Loss:  2.2206404209136963   ACC:  20.0 %\n",
      "Epoch  1   5000  /  340 batch Loss:  2.1859564781188965   ACC:  20.0 %\n",
      "Epoch  1   5000  /  360 batch Loss:  2.201122760772705   ACC:  0.0 %\n",
      "Epoch  1   5000  /  380 batch Loss:  2.143216609954834   ACC:  30.0 %\n",
      "Epoch  1   5000  /  400 batch Loss:  2.1869003772735596   ACC:  10.0 %\n",
      "Epoch  1   5000  /  420 batch Loss:  2.04571533203125   ACC:  10.0 %\n",
      "Epoch  1   5000  /  440 batch Loss:  2.144973039627075   ACC:  10.0 %\n",
      "Epoch  1   5000  /  460 batch Loss:  2.4286410808563232   ACC:  30.0 %\n",
      "Epoch  1   5000  /  480 batch Loss:  2.3160452842712402   ACC:  0.0 %\n",
      "Epoch  1   5000  /  500 batch Loss:  2.171839475631714   ACC:  10.0 %\n",
      "Epoch  1   5000  /  520 batch Loss:  2.006542682647705   ACC:  20.0 %\n",
      "Epoch  1   5000  /  540 batch Loss:  2.4834513664245605   ACC:  0.0 %\n",
      "Epoch  1   5000  /  560 batch Loss:  1.9863494634628296   ACC:  10.0 %\n",
      "Epoch  1   5000  /  580 batch Loss:  2.2794601917266846   ACC:  0.0 %\n",
      "Epoch  1   5000  /  600 batch Loss:  2.3993124961853027   ACC:  0.0 %\n",
      "Epoch  1   5000  /  620 batch Loss:  2.291398286819458   ACC:  10.0 %\n",
      "Epoch  1   5000  /  640 batch Loss:  2.314581871032715   ACC:  30.0 %\n",
      "Epoch  1   5000  /  660 batch Loss:  2.050203800201416   ACC:  20.0 %\n",
      "Epoch  1   5000  /  680 batch Loss:  2.204510450363159   ACC:  30.0 %\n",
      "Epoch  1   5000  /  700 batch Loss:  1.8248872756958008   ACC:  40.0 %\n",
      "Epoch  1   5000  /  720 batch Loss:  2.315432071685791   ACC:  20.0 %\n",
      "Epoch  1   5000  /  740 batch Loss:  2.130964994430542   ACC:  10.0 %\n",
      "Epoch  1   5000  /  760 batch Loss:  2.2069849967956543   ACC:  20.0 %\n",
      "Epoch  1   5000  /  780 batch Loss:  1.8880329132080078   ACC:  40.0 %\n",
      "Epoch  1   5000  /  800 batch Loss:  1.9570327997207642   ACC:  30.0 %\n",
      "Epoch  1   5000  /  820 batch Loss:  2.6135687828063965   ACC:  0.0 %\n",
      "Epoch  1   5000  /  840 batch Loss:  2.1622581481933594   ACC:  30.0 %\n",
      "Epoch  1   5000  /  860 batch Loss:  1.9036277532577515   ACC:  30.0 %\n",
      "Epoch  1   5000  /  880 batch Loss:  2.083369493484497   ACC:  20.0 %\n",
      "Epoch  1   5000  /  900 batch Loss:  2.03570818901062   ACC:  20.0 %\n",
      "Epoch  1   5000  /  920 batch Loss:  1.7683942317962646   ACC:  30.0 %\n",
      "Epoch  1   5000  /  940 batch Loss:  1.9321867227554321   ACC:  30.0 %\n",
      "Epoch  1   5000  /  960 batch Loss:  2.4750075340270996   ACC:  0.0 %\n",
      "Epoch  1   5000  /  980 batch Loss:  1.992008924484253   ACC:  10.0 %\n",
      "Epoch  1   5000  /  1000 batch Loss:  2.233781337738037   ACC:  10.0 %\n",
      "Epoch  1   5000  /  1020 batch Loss:  2.6830577850341797   ACC:  0.0 %\n",
      "Epoch  1   5000  /  1040 batch Loss:  1.9380909204483032   ACC:  30.0 %\n",
      "Epoch  1   5000  /  1060 batch Loss:  2.0941267013549805   ACC:  20.0 %\n",
      "Epoch  1   5000  /  1080 batch Loss:  2.1222243309020996   ACC:  10.0 %\n",
      "Epoch  1   5000  /  1100 batch Loss:  2.0830318927764893   ACC:  20.0 %\n",
      "Epoch  1   5000  /  1120 batch Loss:  1.8670657873153687   ACC:  20.0 %\n",
      "Epoch  1   5000  /  1140 batch Loss:  2.0577244758605957   ACC:  20.0 %\n",
      "Epoch  1   5000  /  1160 batch Loss:  1.875420331954956   ACC:  40.0 %\n",
      "Epoch  1   5000  /  1180 batch Loss:  2.0493319034576416   ACC:  40.0 %\n",
      "Epoch  1   5000  /  1200 batch Loss:  2.2553763389587402   ACC:  20.0 %\n",
      "Epoch  1   5000  /  1220 batch Loss:  2.101266622543335   ACC:  30.0 %\n",
      "Epoch  1   5000  /  1240 batch Loss:  2.5746912956237793   ACC:  20.0 %\n",
      "Epoch  1   5000  /  1260 batch Loss:  1.9648555517196655   ACC:  30.0 %\n",
      "Epoch  1   5000  /  1280 batch Loss:  2.7347724437713623   ACC:  10.0 %\n",
      "Epoch  1   5000  /  1300 batch Loss:  1.9900233745574951   ACC:  20.0 %\n",
      "Epoch  1   5000  /  1320 batch Loss:  2.079843521118164   ACC:  10.0 %\n",
      "Epoch  1   5000  /  1340 batch Loss:  2.223397731781006   ACC:  10.0 %\n",
      "Epoch  1   5000  /  1360 batch Loss:  1.8547550439834595   ACC:  20.0 %\n",
      "Epoch  1   5000  /  1380 batch Loss:  1.8848098516464233   ACC:  40.0 %\n",
      "Epoch  1   5000  /  1400 batch Loss:  2.0276193618774414   ACC:  30.0 %\n",
      "Epoch  1   5000  /  1420 batch Loss:  1.7939398288726807   ACC:  60.0 %\n",
      "Epoch  1   5000  /  1440 batch Loss:  1.8693357706069946   ACC:  30.0 %\n",
      "Epoch  1   5000  /  1460 batch Loss:  2.104138135910034   ACC:  20.0 %\n",
      "Epoch  1   5000  /  1480 batch Loss:  1.830190896987915   ACC:  20.0 %\n",
      "Epoch  1   5000  /  1500 batch Loss:  1.9563357830047607   ACC:  20.0 %\n",
      "Epoch  1   5000  /  1520 batch Loss:  2.169098377227783   ACC:  10.0 %\n",
      "Epoch  1   5000  /  1540 batch Loss:  1.856736421585083   ACC:  40.0 %\n",
      "Epoch  1   5000  /  1560 batch Loss:  1.8571827411651611   ACC:  40.0 %\n",
      "Epoch  1   5000  /  1580 batch Loss:  1.9356781244277954   ACC:  0.0 %\n",
      "Epoch  1   5000  /  1600 batch Loss:  2.175534725189209   ACC:  40.0 %\n",
      "Epoch  1   5000  /  1620 batch Loss:  2.162811756134033   ACC:  0.0 %\n",
      "Epoch  1   5000  /  1640 batch Loss:  1.8753154277801514   ACC:  20.0 %\n",
      "Epoch  1   5000  /  1660 batch Loss:  2.1679000854492188   ACC:  30.0 %\n",
      "Epoch  1   5000  /  1680 batch Loss:  2.4259815216064453   ACC:  20.0 %\n",
      "Epoch  1   5000  /  1700 batch Loss:  2.320134162902832   ACC:  30.0 %\n",
      "Epoch  1   5000  /  1720 batch Loss:  2.0038273334503174   ACC:  20.0 %\n",
      "Epoch  1   5000  /  1740 batch Loss:  2.1086232662200928   ACC:  40.0 %\n",
      "Epoch  1   5000  /  1760 batch Loss:  2.237713098526001   ACC:  30.0 %\n",
      "Epoch  1   5000  /  1780 batch Loss:  2.1614506244659424   ACC:  30.0 %\n",
      "Epoch  1   5000  /  1800 batch Loss:  2.190891742706299   ACC:  20.0 %\n",
      "Epoch  1   5000  /  1820 batch Loss:  2.454317331314087   ACC:  20.0 %\n",
      "Epoch  1   5000  /  1840 batch Loss:  2.1199333667755127   ACC:  0.0 %\n",
      "Epoch  1   5000  /  1860 batch Loss:  2.083932399749756   ACC:  20.0 %\n",
      "Epoch  1   5000  /  1880 batch Loss:  2.421630859375   ACC:  20.0 %\n",
      "Epoch  1   5000  /  1900 batch Loss:  2.3763394355773926   ACC:  20.0 %\n",
      "Epoch  1   5000  /  1920 batch Loss:  2.1887545585632324   ACC:  20.0 %\n",
      "Epoch  1   5000  /  1940 batch Loss:  1.777341604232788   ACC:  20.0 %\n",
      "Epoch  1   5000  /  1960 batch Loss:  2.0341484546661377   ACC:  20.0 %\n",
      "Epoch  1   5000  /  1980 batch Loss:  2.0454366207122803   ACC:  30.0 %\n",
      "Epoch  1   5000  /  2000 batch Loss:  2.3569376468658447   ACC:  10.0 %\n",
      "Epoch  1   5000  /  2020 batch Loss:  1.8538891077041626   ACC:  30.0 %\n",
      "Epoch  1   5000  /  2040 batch Loss:  2.00752329826355   ACC:  40.0 %\n",
      "Epoch  1   5000  /  2060 batch Loss:  1.9169690608978271   ACC:  30.0 %\n",
      "Epoch  1   5000  /  2080 batch Loss:  2.1527981758117676   ACC:  40.0 %\n",
      "Epoch  1   5000  /  2100 batch Loss:  1.77239990234375   ACC:  40.0 %\n",
      "Epoch  1   5000  /  2120 batch Loss:  1.7235406637191772   ACC:  30.0 %\n",
      "Epoch  1   5000  /  2140 batch Loss:  2.356466770172119   ACC:  10.0 %\n",
      "Epoch  1   5000  /  2160 batch Loss:  2.1481542587280273   ACC:  10.0 %\n",
      "Epoch  1   5000  /  2180 batch Loss:  1.8502609729766846   ACC:  10.0 %\n",
      "Epoch  1   5000  /  2200 batch Loss:  2.038752794265747   ACC:  0.0 %\n",
      "Epoch  1   5000  /  2220 batch Loss:  1.6116880178451538   ACC:  40.0 %\n",
      "Epoch  1   5000  /  2240 batch Loss:  2.107520580291748   ACC:  40.0 %\n",
      "Epoch  1   5000  /  2260 batch Loss:  1.7358062267303467   ACC:  60.0 %\n",
      "Epoch  1   5000  /  2280 batch Loss:  1.9897361993789673   ACC:  40.0 %\n",
      "Epoch  1   5000  /  2300 batch Loss:  1.808444619178772   ACC:  30.0 %\n",
      "Epoch  1   5000  /  2320 batch Loss:  2.0787360668182373   ACC:  20.0 %\n",
      "Epoch  1   5000  /  2340 batch Loss:  1.8378585577011108   ACC:  40.0 %\n",
      "Epoch  1   5000  /  2360 batch Loss:  2.6443488597869873   ACC:  10.0 %\n",
      "Epoch  1   5000  /  2380 batch Loss:  1.969143271446228   ACC:  40.0 %\n",
      "Epoch  1   5000  /  2400 batch Loss:  1.8729270696640015   ACC:  30.0 %\n",
      "Epoch  1   5000  /  2420 batch Loss:  1.9423710107803345   ACC:  20.0 %\n",
      "Epoch  1   5000  /  2440 batch Loss:  2.5945448875427246   ACC:  0.0 %\n",
      "Epoch  1   5000  /  2460 batch Loss:  1.8137178421020508   ACC:  60.0 %\n",
      "Epoch  1   5000  /  2480 batch Loss:  2.1444621086120605   ACC:  20.0 %\n",
      "Epoch  1   5000  /  2500 batch Loss:  1.9117629528045654   ACC:  30.0 %\n",
      "Epoch  1   5000  /  2520 batch Loss:  2.290393590927124   ACC:  10.0 %\n",
      "Epoch  1   5000  /  2540 batch Loss:  1.7757294178009033   ACC:  30.0 %\n",
      "Epoch  1   5000  /  2560 batch Loss:  2.534374237060547   ACC:  30.0 %\n",
      "Epoch  1   5000  /  2580 batch Loss:  2.1231143474578857   ACC:  20.0 %\n",
      "Epoch  1   5000  /  2600 batch Loss:  2.3030903339385986   ACC:  0.0 %\n",
      "Epoch  1   5000  /  2620 batch Loss:  1.7810089588165283   ACC:  40.0 %\n",
      "Epoch  1   5000  /  2640 batch Loss:  2.0935070514678955   ACC:  10.0 %\n",
      "Epoch  1   5000  /  2660 batch Loss:  2.279236316680908   ACC:  20.0 %\n",
      "Epoch  1   5000  /  2680 batch Loss:  1.7119029760360718   ACC:  40.0 %\n",
      "Epoch  1   5000  /  2700 batch Loss:  1.9087989330291748   ACC:  20.0 %\n",
      "Epoch  1   5000  /  2720 batch Loss:  2.274545431137085   ACC:  20.0 %\n",
      "Epoch  1   5000  /  2740 batch Loss:  1.7281001806259155   ACC:  50.0 %\n",
      "Epoch  1   5000  /  2760 batch Loss:  1.7419523000717163   ACC:  50.0 %\n",
      "Epoch  1   5000  /  2780 batch Loss:  1.5923869609832764   ACC:  30.0 %\n",
      "Epoch  1   5000  /  2800 batch Loss:  1.9934520721435547   ACC:  20.0 %\n",
      "Epoch  1   5000  /  2820 batch Loss:  1.6913620233535767   ACC:  60.0 %\n",
      "Epoch  1   5000  /  2840 batch Loss:  1.4925024509429932   ACC:  40.0 %\n",
      "Epoch  1   5000  /  2860 batch Loss:  1.6285760402679443   ACC:  40.0 %\n",
      "Epoch  1   5000  /  2880 batch Loss:  1.8193635940551758   ACC:  30.0 %\n",
      "Epoch  1   5000  /  2900 batch Loss:  1.9612772464752197   ACC:  20.0 %\n",
      "Epoch  1   5000  /  2920 batch Loss:  1.6109641790390015   ACC:  40.0 %\n",
      "Epoch  1   5000  /  2940 batch Loss:  2.4516079425811768   ACC:  30.0 %\n",
      "Epoch  1   5000  /  2960 batch Loss:  2.3216300010681152   ACC:  30.0 %\n",
      "Epoch  1   5000  /  2980 batch Loss:  1.5586097240447998   ACC:  50.0 %\n",
      "Epoch  1   5000  /  3000 batch Loss:  2.1205737590789795   ACC:  50.0 %\n",
      "Epoch  1   5000  /  3020 batch Loss:  1.5949556827545166   ACC:  60.0 %\n",
      "Epoch  1   5000  /  3040 batch Loss:  1.7539218664169312   ACC:  30.0 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9686/2935631994.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/project/Stand-Alone-Self-Attention/env/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/project/Stand-Alone-Self-Attention/env/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "print_interval = 20\n",
    "\n",
    "## 30에폭 마다 러닝레이트 줄임\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    decayed_lr = lr * (0.1 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = decayed_lr\n",
    "\n",
    "for epoch in range(1,epochs+1):\n",
    "    model.train()\n",
    "    train_acc = 0.0\n",
    "    step = 0\n",
    "    for data, target in train_loader:\n",
    "        adjust_learning_rate(optimizer, epoch)\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        ## onnx 저장\n",
    "\n",
    "\n",
    "        y_pred = output.data.max(1)[1]\n",
    "        \n",
    "        acc = float(y_pred.eq(target.data).sum()) / len(data) * 100.\n",
    "        train_acc += acc\n",
    "        step += 1\n",
    "        if step % print_interval == 0:\n",
    "            # print(\"[Epoch {0:4d}] {1:4d}/{1:4d} Loss: {1:2.3f} Acc: {2:.3f}% \\n\".format(epoch,len(train_loader),step, loss.data, acc), end='')\n",
    "            print(\"Epoch \",epoch,\" \",len(train_loader),\" / \",step,\"batch Loss: \",loss.item(),\"  ACC: \",acc,\"%\")\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            output = model(data)\n",
    "            prediction = output.data.max(1)[1]\n",
    "            correct += prediction.eq(target.data).sum()\n",
    "\n",
    "    acc = 100. * float(correct) / len(test_loader.dataset)\n",
    "    print('Test acc: {0:.2f} \\n'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## onnx 저장\n",
    "## operator_export_type=torch.onnx.OperatorExportTypes.ONNX_ATEN_FALLBACK 꼭 해줘야함\n",
    "## unfold 연산을 \n",
    "# https://github.com/pytorch/pytorch/issues/14395#issuecomment-444697403\n",
    "\n",
    "\n",
    "# import torch.onnx\n",
    "# torch.onnx.export(model.cpu(),data.cpu(),'./stand_alone_self.onnx',export_params=False,opset_version=12,\n",
    "#                   operator_export_type=torch.onnx.OperatorExportTypes.ONNX_ATEN_FALLBACK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'generator' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9686/1319817176.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'generator' has no len()"
     ]
    }
   ],
   "source": [
    "print(model.ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "07b1216af5147bda247a947cdd561e2b589c1fe1b25ba98cf598d38fde5bfe85"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
