{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from model import ResNet50, ResNet38, ResNet26\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## 데이터 셋은 CIFAR10으로\n",
    "num_classes = 10\n",
    "batch_size = 32\n",
    "num_workers = 4\n",
    "\n",
    "## 데이터셋 로드\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=(0.4914, 0.4822, 0.4465),\n",
    "        std=(0.2470, 0.2435, 0.2616)\n",
    "    )\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=(0.4914, 0.4822, 0.4465),\n",
    "        std=(0.2470, 0.2435, 0.2616)\n",
    "    )\n",
    "])\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('data', train=True, download=True, transform=transform_train),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('data', train=False, transform=transform_test),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem = False\n",
    "model = ResNet26(num_classes=num_classes, stem=stem)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 1\n",
    "best_acc = 0.0\n",
    "\n",
    "lr = 0.01\n",
    "momentum =0.9\n",
    "weight_decay = 0.0001\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1   1563  /  20 batch Loss:  3.2431983947753906   ACC:  21.875 %\n",
      "Epoch  1   1563  /  40 batch Loss:  2.280831813812256   ACC:  15.625 %\n",
      "Epoch  1   1563  /  60 batch Loss:  2.4326906204223633   ACC:  21.875 %\n",
      "Epoch  1   1563  /  80 batch Loss:  2.344860553741455   ACC:  25.0 %\n",
      "Epoch  1   1563  /  100 batch Loss:  2.339205741882324   ACC:  12.5 %\n",
      "Epoch  1   1563  /  120 batch Loss:  2.096317768096924   ACC:  21.875 %\n",
      "Epoch  1   1563  /  140 batch Loss:  2.139641523361206   ACC:  9.375 %\n",
      "Epoch  1   1563  /  160 batch Loss:  2.032835006713867   ACC:  18.75 %\n",
      "Epoch  1   1563  /  180 batch Loss:  2.1409754753112793   ACC:  18.75 %\n",
      "Epoch  1   1563  /  200 batch Loss:  2.08933687210083   ACC:  31.25 %\n",
      "Epoch  1   1563  /  220 batch Loss:  2.170435905456543   ACC:  15.625 %\n",
      "Epoch  1   1563  /  240 batch Loss:  1.9093222618103027   ACC:  21.875 %\n",
      "Epoch  1   1563  /  260 batch Loss:  2.041768789291382   ACC:  21.875 %\n",
      "Epoch  1   1563  /  280 batch Loss:  1.6941570043563843   ACC:  31.25 %\n",
      "Epoch  1   1563  /  300 batch Loss:  1.85332453250885   ACC:  34.375 %\n",
      "Epoch  1   1563  /  320 batch Loss:  1.9235435724258423   ACC:  28.125 %\n",
      "Epoch  1   1563  /  340 batch Loss:  1.9127873182296753   ACC:  12.5 %\n",
      "Epoch  1   1563  /  360 batch Loss:  2.2166528701782227   ACC:  21.875 %\n",
      "Epoch  1   1563  /  380 batch Loss:  1.8209537267684937   ACC:  25.0 %\n",
      "Epoch  1   1563  /  400 batch Loss:  1.681062936782837   ACC:  34.375 %\n",
      "Epoch  1   1563  /  420 batch Loss:  1.9000693559646606   ACC:  31.25 %\n",
      "Epoch  1   1563  /  440 batch Loss:  1.9273344278335571   ACC:  18.75 %\n",
      "Epoch  1   1563  /  460 batch Loss:  1.6965833902359009   ACC:  53.125 %\n",
      "Epoch  1   1563  /  480 batch Loss:  1.7674248218536377   ACC:  18.75 %\n",
      "Epoch  1   1563  /  500 batch Loss:  1.8949642181396484   ACC:  28.125 %\n",
      "Epoch  1   1563  /  520 batch Loss:  1.785286784172058   ACC:  37.5 %\n",
      "Epoch  1   1563  /  540 batch Loss:  1.8000777959823608   ACC:  21.875 %\n",
      "Epoch  1   1563  /  560 batch Loss:  2.05317759513855   ACC:  21.875 %\n",
      "Epoch  1   1563  /  580 batch Loss:  1.7455247640609741   ACC:  28.125 %\n",
      "Epoch  1   1563  /  600 batch Loss:  1.7910135984420776   ACC:  46.875 %\n",
      "Epoch  1   1563  /  620 batch Loss:  2.0831546783447266   ACC:  21.875 %\n",
      "Epoch  1   1563  /  640 batch Loss:  1.3548500537872314   ACC:  62.5 %\n",
      "Epoch  1   1563  /  660 batch Loss:  1.8937289714813232   ACC:  34.375 %\n",
      "Epoch  1   1563  /  680 batch Loss:  1.7262580394744873   ACC:  34.375 %\n",
      "Epoch  1   1563  /  700 batch Loss:  1.8359543085098267   ACC:  25.0 %\n",
      "Epoch  1   1563  /  720 batch Loss:  1.5682764053344727   ACC:  37.5 %\n",
      "Epoch  1   1563  /  740 batch Loss:  1.7422091960906982   ACC:  34.375 %\n",
      "Epoch  1   1563  /  760 batch Loss:  1.7064332962036133   ACC:  34.375 %\n",
      "Epoch  1   1563  /  780 batch Loss:  1.6561111211776733   ACC:  37.5 %\n",
      "Epoch  1   1563  /  800 batch Loss:  1.6738642454147339   ACC:  46.875 %\n",
      "Epoch  1   1563  /  820 batch Loss:  1.2812042236328125   ACC:  50.0 %\n",
      "Epoch  1   1563  /  840 batch Loss:  1.7357091903686523   ACC:  40.625 %\n",
      "Epoch  1   1563  /  860 batch Loss:  1.6530617475509644   ACC:  28.125 %\n",
      "Epoch  1   1563  /  880 batch Loss:  1.9264495372772217   ACC:  21.875 %\n",
      "Epoch  1   1563  /  900 batch Loss:  1.880588412284851   ACC:  37.5 %\n",
      "Epoch  1   1563  /  920 batch Loss:  1.9605607986450195   ACC:  28.125 %\n",
      "Epoch  1   1563  /  940 batch Loss:  1.802380084991455   ACC:  40.625 %\n",
      "Epoch  1   1563  /  960 batch Loss:  1.5198723077774048   ACC:  37.5 %\n",
      "Epoch  1   1563  /  980 batch Loss:  1.4617358446121216   ACC:  37.5 %\n",
      "Epoch  1   1563  /  1000 batch Loss:  1.6340434551239014   ACC:  46.875 %\n",
      "Epoch  1   1563  /  1020 batch Loss:  1.7306548357009888   ACC:  21.875 %\n",
      "Epoch  1   1563  /  1040 batch Loss:  1.8019474744796753   ACC:  25.0 %\n",
      "Epoch  1   1563  /  1060 batch Loss:  1.8907310962677002   ACC:  21.875 %\n",
      "Epoch  1   1563  /  1080 batch Loss:  1.8889654874801636   ACC:  21.875 %\n",
      "Epoch  1   1563  /  1100 batch Loss:  2.000622034072876   ACC:  34.375 %\n",
      "Epoch  1   1563  /  1120 batch Loss:  1.5952407121658325   ACC:  37.5 %\n",
      "Epoch  1   1563  /  1140 batch Loss:  1.6545199155807495   ACC:  40.625 %\n",
      "Epoch  1   1563  /  1160 batch Loss:  1.786590337753296   ACC:  28.125 %\n",
      "Epoch  1   1563  /  1180 batch Loss:  1.4590367078781128   ACC:  40.625 %\n",
      "Epoch  1   1563  /  1200 batch Loss:  1.5796802043914795   ACC:  46.875 %\n",
      "Epoch  1   1563  /  1220 batch Loss:  1.552903175354004   ACC:  34.375 %\n",
      "Epoch  1   1563  /  1240 batch Loss:  1.3159496784210205   ACC:  50.0 %\n",
      "Epoch  1   1563  /  1260 batch Loss:  1.2738735675811768   ACC:  46.875 %\n",
      "Epoch  1   1563  /  1280 batch Loss:  1.5396453142166138   ACC:  43.75 %\n",
      "Epoch  1   1563  /  1300 batch Loss:  1.4907042980194092   ACC:  53.125 %\n",
      "Epoch  1   1563  /  1320 batch Loss:  1.7941917181015015   ACC:  40.625 %\n",
      "Epoch  1   1563  /  1340 batch Loss:  1.508817195892334   ACC:  43.75 %\n",
      "Epoch  1   1563  /  1360 batch Loss:  1.4471594095230103   ACC:  53.125 %\n",
      "Epoch  1   1563  /  1380 batch Loss:  1.495215654373169   ACC:  40.625 %\n",
      "Epoch  1   1563  /  1400 batch Loss:  1.6255794763565063   ACC:  43.75 %\n",
      "Epoch  1   1563  /  1420 batch Loss:  1.4948971271514893   ACC:  37.5 %\n",
      "Epoch  1   1563  /  1440 batch Loss:  1.6150717735290527   ACC:  46.875 %\n",
      "Epoch  1   1563  /  1460 batch Loss:  1.3641843795776367   ACC:  43.75 %\n",
      "Epoch  1   1563  /  1480 batch Loss:  1.654221773147583   ACC:  43.75 %\n",
      "Epoch  1   1563  /  1500 batch Loss:  1.0981254577636719   ACC:  59.375 %\n",
      "Epoch  1   1563  /  1520 batch Loss:  1.4479045867919922   ACC:  46.875 %\n",
      "Epoch  1   1563  /  1540 batch Loss:  1.5705238580703735   ACC:  34.375 %\n",
      "Epoch  1   1563  /  1560 batch Loss:  1.531965732574463   ACC:  40.625 %\n",
      "Test acc: 46.44 \n",
      "\n",
      "Epoch  2   1563  /  20 batch Loss:  1.733567476272583   ACC:  37.5 %\n",
      "Epoch  2   1563  /  40 batch Loss:  1.3709266185760498   ACC:  50.0 %\n",
      "Epoch  2   1563  /  60 batch Loss:  1.5321532487869263   ACC:  37.5 %\n",
      "Epoch  2   1563  /  80 batch Loss:  1.4370917081832886   ACC:  50.0 %\n",
      "Epoch  2   1563  /  100 batch Loss:  1.5587663650512695   ACC:  53.125 %\n",
      "Epoch  2   1563  /  120 batch Loss:  1.3077151775360107   ACC:  53.125 %\n",
      "Epoch  2   1563  /  140 batch Loss:  1.3986295461654663   ACC:  43.75 %\n",
      "Epoch  2   1563  /  160 batch Loss:  1.3627997636795044   ACC:  50.0 %\n",
      "Epoch  2   1563  /  180 batch Loss:  1.5143028497695923   ACC:  34.375 %\n",
      "Epoch  2   1563  /  200 batch Loss:  1.526911973953247   ACC:  46.875 %\n",
      "Epoch  2   1563  /  220 batch Loss:  1.3287469148635864   ACC:  40.625 %\n",
      "Epoch  2   1563  /  240 batch Loss:  1.2492352724075317   ACC:  56.25 %\n",
      "Epoch  2   1563  /  260 batch Loss:  1.4616397619247437   ACC:  34.375 %\n",
      "Epoch  2   1563  /  280 batch Loss:  1.1253252029418945   ACC:  62.5 %\n",
      "Epoch  2   1563  /  300 batch Loss:  1.4569779634475708   ACC:  50.0 %\n",
      "Epoch  2   1563  /  320 batch Loss:  1.8908288478851318   ACC:  31.25 %\n",
      "Epoch  2   1563  /  340 batch Loss:  1.3646109104156494   ACC:  53.125 %\n",
      "Epoch  2   1563  /  360 batch Loss:  1.3635482788085938   ACC:  50.0 %\n",
      "Epoch  2   1563  /  380 batch Loss:  1.939894199371338   ACC:  25.0 %\n",
      "Epoch  2   1563  /  400 batch Loss:  1.3685449361801147   ACC:  56.25 %\n",
      "Epoch  2   1563  /  420 batch Loss:  1.5541889667510986   ACC:  50.0 %\n",
      "Epoch  2   1563  /  440 batch Loss:  1.8486775159835815   ACC:  43.75 %\n",
      "Epoch  2   1563  /  460 batch Loss:  0.9506983160972595   ACC:  56.25 %\n",
      "Epoch  2   1563  /  480 batch Loss:  1.435184359550476   ACC:  56.25 %\n",
      "Epoch  2   1563  /  500 batch Loss:  1.4363335371017456   ACC:  50.0 %\n",
      "Epoch  2   1563  /  520 batch Loss:  1.490172028541565   ACC:  43.75 %\n",
      "Epoch  2   1563  /  540 batch Loss:  1.348838210105896   ACC:  56.25 %\n",
      "Epoch  2   1563  /  560 batch Loss:  1.0795471668243408   ACC:  59.375 %\n",
      "Epoch  2   1563  /  580 batch Loss:  1.2441445589065552   ACC:  53.125 %\n",
      "Epoch  2   1563  /  600 batch Loss:  1.4541044235229492   ACC:  43.75 %\n",
      "Epoch  2   1563  /  620 batch Loss:  1.590463399887085   ACC:  40.625 %\n",
      "Epoch  2   1563  /  640 batch Loss:  1.5037964582443237   ACC:  50.0 %\n",
      "Epoch  2   1563  /  660 batch Loss:  1.1838467121124268   ACC:  56.25 %\n",
      "Epoch  2   1563  /  680 batch Loss:  1.2224316596984863   ACC:  53.125 %\n",
      "Epoch  2   1563  /  700 batch Loss:  1.8352758884429932   ACC:  31.25 %\n",
      "Epoch  2   1563  /  720 batch Loss:  1.2894474267959595   ACC:  53.125 %\n",
      "Epoch  2   1563  /  740 batch Loss:  1.1732197999954224   ACC:  59.375 %\n",
      "Epoch  2   1563  /  760 batch Loss:  1.2192600965499878   ACC:  65.625 %\n",
      "Epoch  2   1563  /  780 batch Loss:  0.9743796586990356   ACC:  75.0 %\n",
      "Epoch  2   1563  /  800 batch Loss:  1.3992350101470947   ACC:  46.875 %\n",
      "Epoch  2   1563  /  820 batch Loss:  1.4826287031173706   ACC:  50.0 %\n",
      "Epoch  2   1563  /  840 batch Loss:  1.5371367931365967   ACC:  40.625 %\n",
      "Epoch  2   1563  /  860 batch Loss:  1.4460293054580688   ACC:  40.625 %\n",
      "Epoch  2   1563  /  880 batch Loss:  1.6361730098724365   ACC:  37.5 %\n",
      "Epoch  2   1563  /  900 batch Loss:  1.340917944908142   ACC:  59.375 %\n",
      "Epoch  2   1563  /  920 batch Loss:  1.1620246171951294   ACC:  53.125 %\n",
      "Epoch  2   1563  /  940 batch Loss:  1.5036704540252686   ACC:  53.125 %\n",
      "Epoch  2   1563  /  960 batch Loss:  1.2809808254241943   ACC:  50.0 %\n",
      "Epoch  2   1563  /  980 batch Loss:  1.3587100505828857   ACC:  53.125 %\n",
      "Epoch  2   1563  /  1000 batch Loss:  1.3156737089157104   ACC:  56.25 %\n",
      "Epoch  2   1563  /  1020 batch Loss:  1.603756070137024   ACC:  46.875 %\n",
      "Epoch  2   1563  /  1040 batch Loss:  1.3395048379898071   ACC:  53.125 %\n",
      "Epoch  2   1563  /  1060 batch Loss:  1.3227362632751465   ACC:  56.25 %\n",
      "Epoch  2   1563  /  1080 batch Loss:  1.3575587272644043   ACC:  50.0 %\n",
      "Epoch  2   1563  /  1100 batch Loss:  1.343874454498291   ACC:  46.875 %\n",
      "Epoch  2   1563  /  1120 batch Loss:  1.2630116939544678   ACC:  50.0 %\n",
      "Epoch  2   1563  /  1140 batch Loss:  1.2501623630523682   ACC:  53.125 %\n",
      "Epoch  2   1563  /  1160 batch Loss:  1.1386456489562988   ACC:  59.375 %\n",
      "Epoch  2   1563  /  1180 batch Loss:  1.4574973583221436   ACC:  53.125 %\n",
      "Epoch  2   1563  /  1200 batch Loss:  1.1253317594528198   ACC:  59.375 %\n",
      "Epoch  2   1563  /  1220 batch Loss:  1.3969824314117432   ACC:  59.375 %\n",
      "Epoch  2   1563  /  1240 batch Loss:  1.4858851432800293   ACC:  50.0 %\n",
      "Epoch  2   1563  /  1260 batch Loss:  1.2025487422943115   ACC:  56.25 %\n",
      "Epoch  2   1563  /  1280 batch Loss:  1.289199948310852   ACC:  43.75 %\n",
      "Epoch  2   1563  /  1300 batch Loss:  0.9772962927818298   ACC:  56.25 %\n",
      "Epoch  2   1563  /  1320 batch Loss:  1.207532525062561   ACC:  50.0 %\n",
      "Epoch  2   1563  /  1340 batch Loss:  1.3022760152816772   ACC:  59.375 %\n",
      "Epoch  2   1563  /  1360 batch Loss:  1.5750490427017212   ACC:  43.75 %\n",
      "Epoch  2   1563  /  1380 batch Loss:  1.2192896604537964   ACC:  40.625 %\n",
      "Epoch  2   1563  /  1400 batch Loss:  1.3434146642684937   ACC:  53.125 %\n",
      "Epoch  2   1563  /  1420 batch Loss:  1.281479835510254   ACC:  53.125 %\n",
      "Epoch  2   1563  /  1440 batch Loss:  1.6898139715194702   ACC:  46.875 %\n",
      "Epoch  2   1563  /  1460 batch Loss:  1.3152110576629639   ACC:  56.25 %\n",
      "Epoch  2   1563  /  1480 batch Loss:  1.2729742527008057   ACC:  62.5 %\n",
      "Epoch  2   1563  /  1500 batch Loss:  1.0546295642852783   ACC:  75.0 %\n",
      "Epoch  2   1563  /  1520 batch Loss:  1.2573955059051514   ACC:  53.125 %\n",
      "Epoch  2   1563  /  1540 batch Loss:  1.263692855834961   ACC:  53.125 %\n",
      "Epoch  2   1563  /  1560 batch Loss:  1.55963933467865   ACC:  37.5 %\n",
      "Test acc: 56.09 \n",
      "\n",
      "Epoch  3   1563  /  20 batch Loss:  0.778043270111084   ACC:  71.875 %\n",
      "Epoch  3   1563  /  40 batch Loss:  1.6721558570861816   ACC:  50.0 %\n",
      "Epoch  3   1563  /  60 batch Loss:  0.7892813682556152   ACC:  84.375 %\n",
      "Epoch  3   1563  /  80 batch Loss:  1.1307011842727661   ACC:  56.25 %\n",
      "Epoch  3   1563  /  100 batch Loss:  1.1380051374435425   ACC:  62.5 %\n",
      "Epoch  3   1563  /  120 batch Loss:  1.0958447456359863   ACC:  59.375 %\n",
      "Epoch  3   1563  /  140 batch Loss:  1.356895089149475   ACC:  53.125 %\n",
      "Epoch  3   1563  /  160 batch Loss:  1.2857861518859863   ACC:  50.0 %\n",
      "Epoch  3   1563  /  180 batch Loss:  1.6428825855255127   ACC:  50.0 %\n",
      "Epoch  3   1563  /  200 batch Loss:  1.3643417358398438   ACC:  46.875 %\n",
      "Epoch  3   1563  /  220 batch Loss:  0.9484525918960571   ACC:  65.625 %\n",
      "Epoch  3   1563  /  240 batch Loss:  1.3946294784545898   ACC:  56.25 %\n",
      "Epoch  3   1563  /  260 batch Loss:  1.0987703800201416   ACC:  53.125 %\n",
      "Epoch  3   1563  /  280 batch Loss:  1.26066255569458   ACC:  46.875 %\n",
      "Epoch  3   1563  /  300 batch Loss:  1.1112555265426636   ACC:  62.5 %\n",
      "Epoch  3   1563  /  320 batch Loss:  0.8324355483055115   ACC:  75.0 %\n",
      "Epoch  3   1563  /  340 batch Loss:  1.3397564888000488   ACC:  46.875 %\n",
      "Epoch  3   1563  /  360 batch Loss:  1.226040244102478   ACC:  53.125 %\n",
      "Epoch  3   1563  /  380 batch Loss:  1.2849317789077759   ACC:  40.625 %\n",
      "Epoch  3   1563  /  400 batch Loss:  0.9614101648330688   ACC:  71.875 %\n",
      "Epoch  3   1563  /  420 batch Loss:  1.229330062866211   ACC:  53.125 %\n",
      "Epoch  3   1563  /  440 batch Loss:  1.4914014339447021   ACC:  53.125 %\n",
      "Epoch  3   1563  /  460 batch Loss:  1.0716477632522583   ACC:  56.25 %\n",
      "Epoch  3   1563  /  480 batch Loss:  1.216052532196045   ACC:  50.0 %\n",
      "Epoch  3   1563  /  500 batch Loss:  1.0404973030090332   ACC:  56.25 %\n",
      "Epoch  3   1563  /  520 batch Loss:  1.170098900794983   ACC:  59.375 %\n",
      "Epoch  3   1563  /  540 batch Loss:  1.1736255884170532   ACC:  65.625 %\n",
      "Epoch  3   1563  /  560 batch Loss:  0.8087535500526428   ACC:  65.625 %\n",
      "Epoch  3   1563  /  580 batch Loss:  1.3481286764144897   ACC:  53.125 %\n",
      "Epoch  3   1563  /  600 batch Loss:  1.061605453491211   ACC:  59.375 %\n",
      "Epoch  3   1563  /  620 batch Loss:  1.3820579051971436   ACC:  50.0 %\n",
      "Epoch  3   1563  /  640 batch Loss:  0.77122962474823   ACC:  75.0 %\n",
      "Epoch  3   1563  /  660 batch Loss:  1.2839090824127197   ACC:  56.25 %\n",
      "Epoch  3   1563  /  680 batch Loss:  1.073171615600586   ACC:  71.875 %\n",
      "Epoch  3   1563  /  700 batch Loss:  1.23723304271698   ACC:  53.125 %\n",
      "Epoch  3   1563  /  720 batch Loss:  1.2016278505325317   ACC:  65.625 %\n",
      "Epoch  3   1563  /  740 batch Loss:  1.2524545192718506   ACC:  56.25 %\n",
      "Epoch  3   1563  /  760 batch Loss:  1.047919750213623   ACC:  62.5 %\n",
      "Epoch  3   1563  /  780 batch Loss:  1.1877217292785645   ACC:  59.375 %\n",
      "Epoch  3   1563  /  800 batch Loss:  0.9475446939468384   ACC:  65.625 %\n",
      "Epoch  3   1563  /  820 batch Loss:  1.3139375448226929   ACC:  50.0 %\n",
      "Epoch  3   1563  /  840 batch Loss:  1.122128963470459   ACC:  53.125 %\n",
      "Epoch  3   1563  /  860 batch Loss:  1.049625039100647   ACC:  53.125 %\n",
      "Epoch  3   1563  /  880 batch Loss:  1.5738860368728638   ACC:  43.75 %\n",
      "Epoch  3   1563  /  900 batch Loss:  1.244147777557373   ACC:  59.375 %\n",
      "Epoch  3   1563  /  920 batch Loss:  1.0032448768615723   ACC:  71.875 %\n",
      "Epoch  3   1563  /  940 batch Loss:  1.1452772617340088   ACC:  62.5 %\n",
      "Epoch  3   1563  /  960 batch Loss:  1.020321249961853   ACC:  65.625 %\n",
      "Epoch  3   1563  /  980 batch Loss:  1.19185471534729   ACC:  56.25 %\n",
      "Epoch  3   1563  /  1000 batch Loss:  1.1602874994277954   ACC:  62.5 %\n",
      "Epoch  3   1563  /  1020 batch Loss:  1.068132996559143   ACC:  68.75 %\n",
      "Epoch  3   1563  /  1040 batch Loss:  0.6411346197128296   ACC:  78.125 %\n",
      "Epoch  3   1563  /  1060 batch Loss:  1.2712048292160034   ACC:  53.125 %\n",
      "Epoch  3   1563  /  1080 batch Loss:  1.0993000268936157   ACC:  68.75 %\n",
      "Epoch  3   1563  /  1100 batch Loss:  1.2538853883743286   ACC:  59.375 %\n",
      "Epoch  3   1563  /  1120 batch Loss:  1.5173317193984985   ACC:  50.0 %\n",
      "Epoch  3   1563  /  1140 batch Loss:  1.1528644561767578   ACC:  50.0 %\n",
      "Epoch  3   1563  /  1160 batch Loss:  1.1128928661346436   ACC:  71.875 %\n",
      "Epoch  3   1563  /  1180 batch Loss:  1.0987954139709473   ACC:  59.375 %\n",
      "Epoch  3   1563  /  1200 batch Loss:  0.9028484225273132   ACC:  59.375 %\n",
      "Epoch  3   1563  /  1220 batch Loss:  0.874026894569397   ACC:  71.875 %\n",
      "Epoch  3   1563  /  1240 batch Loss:  1.43071711063385   ACC:  62.5 %\n",
      "Epoch  3   1563  /  1260 batch Loss:  1.2907710075378418   ACC:  68.75 %\n",
      "Epoch  3   1563  /  1280 batch Loss:  1.2988240718841553   ACC:  56.25 %\n",
      "Epoch  3   1563  /  1300 batch Loss:  1.1755485534667969   ACC:  43.75 %\n",
      "Epoch  3   1563  /  1320 batch Loss:  0.8477590680122375   ACC:  71.875 %\n",
      "Epoch  3   1563  /  1340 batch Loss:  1.2098979949951172   ACC:  56.25 %\n",
      "Epoch  3   1563  /  1360 batch Loss:  1.1652876138687134   ACC:  56.25 %\n",
      "Epoch  3   1563  /  1380 batch Loss:  1.1782643795013428   ACC:  59.375 %\n",
      "Epoch  3   1563  /  1400 batch Loss:  1.1338586807250977   ACC:  65.625 %\n",
      "Epoch  3   1563  /  1420 batch Loss:  1.25251305103302   ACC:  56.25 %\n",
      "Epoch  3   1563  /  1440 batch Loss:  1.4271881580352783   ACC:  46.875 %\n",
      "Epoch  3   1563  /  1460 batch Loss:  0.9268213510513306   ACC:  71.875 %\n",
      "Epoch  3   1563  /  1480 batch Loss:  1.1356920003890991   ACC:  53.125 %\n",
      "Epoch  3   1563  /  1500 batch Loss:  0.822751522064209   ACC:  75.0 %\n",
      "Epoch  3   1563  /  1520 batch Loss:  0.822563886642456   ACC:  68.75 %\n",
      "Epoch  3   1563  /  1540 batch Loss:  0.8271902799606323   ACC:  68.75 %\n",
      "Epoch  3   1563  /  1560 batch Loss:  1.1964408159255981   ACC:  59.375 %\n",
      "Test acc: 62.69 \n",
      "\n",
      "Epoch  4   1563  /  20 batch Loss:  1.1164451837539673   ACC:  59.375 %\n",
      "Epoch  4   1563  /  40 batch Loss:  0.8391416668891907   ACC:  75.0 %\n",
      "Epoch  4   1563  /  60 batch Loss:  0.7283310294151306   ACC:  68.75 %\n",
      "Epoch  4   1563  /  80 batch Loss:  0.9698003530502319   ACC:  59.375 %\n",
      "Epoch  4   1563  /  100 batch Loss:  1.1682575941085815   ACC:  53.125 %\n",
      "Epoch  4   1563  /  120 batch Loss:  1.0517683029174805   ACC:  62.5 %\n",
      "Epoch  4   1563  /  140 batch Loss:  1.0200375318527222   ACC:  56.25 %\n",
      "Epoch  4   1563  /  160 batch Loss:  1.2768408060073853   ACC:  53.125 %\n",
      "Epoch  4   1563  /  180 batch Loss:  1.0241948366165161   ACC:  62.5 %\n",
      "Epoch  4   1563  /  200 batch Loss:  1.180098056793213   ACC:  56.25 %\n",
      "Epoch  4   1563  /  220 batch Loss:  0.9773766398429871   ACC:  71.875 %\n",
      "Epoch  4   1563  /  240 batch Loss:  0.7508896589279175   ACC:  65.625 %\n",
      "Epoch  4   1563  /  260 batch Loss:  1.112075686454773   ACC:  56.25 %\n",
      "Epoch  4   1563  /  280 batch Loss:  1.1734744310379028   ACC:  59.375 %\n",
      "Epoch  4   1563  /  300 batch Loss:  0.7579344511032104   ACC:  71.875 %\n",
      "Epoch  4   1563  /  320 batch Loss:  0.9647441506385803   ACC:  62.5 %\n",
      "Epoch  4   1563  /  340 batch Loss:  1.0398328304290771   ACC:  65.625 %\n",
      "Epoch  4   1563  /  360 batch Loss:  1.0072754621505737   ACC:  59.375 %\n",
      "Epoch  4   1563  /  380 batch Loss:  0.9961397051811218   ACC:  59.375 %\n",
      "Epoch  4   1563  /  400 batch Loss:  0.9279817342758179   ACC:  62.5 %\n",
      "Epoch  4   1563  /  420 batch Loss:  0.8109089136123657   ACC:  68.75 %\n",
      "Epoch  4   1563  /  440 batch Loss:  1.3683615922927856   ACC:  40.625 %\n",
      "Epoch  4   1563  /  460 batch Loss:  1.3353850841522217   ACC:  62.5 %\n",
      "Epoch  4   1563  /  480 batch Loss:  0.8151949644088745   ACC:  75.0 %\n",
      "Epoch  4   1563  /  500 batch Loss:  1.0181204080581665   ACC:  71.875 %\n",
      "Epoch  4   1563  /  520 batch Loss:  1.2901666164398193   ACC:  56.25 %\n",
      "Epoch  4   1563  /  540 batch Loss:  0.9299460649490356   ACC:  65.625 %\n",
      "Epoch  4   1563  /  560 batch Loss:  0.7922823429107666   ACC:  71.875 %\n",
      "Epoch  4   1563  /  580 batch Loss:  0.9124555587768555   ACC:  65.625 %\n",
      "Epoch  4   1563  /  600 batch Loss:  1.218265175819397   ACC:  53.125 %\n",
      "Epoch  4   1563  /  620 batch Loss:  1.0214521884918213   ACC:  59.375 %\n",
      "Epoch  4   1563  /  640 batch Loss:  1.173677921295166   ACC:  59.375 %\n",
      "Epoch  4   1563  /  660 batch Loss:  0.8971582055091858   ACC:  68.75 %\n",
      "Epoch  4   1563  /  680 batch Loss:  1.0583776235580444   ACC:  65.625 %\n",
      "Epoch  4   1563  /  700 batch Loss:  0.927459716796875   ACC:  65.625 %\n",
      "Epoch  4   1563  /  720 batch Loss:  1.1629317998886108   ACC:  65.625 %\n",
      "Epoch  4   1563  /  740 batch Loss:  0.8881227374076843   ACC:  71.875 %\n",
      "Epoch  4   1563  /  760 batch Loss:  1.3190211057662964   ACC:  53.125 %\n",
      "Epoch  4   1563  /  780 batch Loss:  1.0780017375946045   ACC:  62.5 %\n",
      "Epoch  4   1563  /  800 batch Loss:  0.816529393196106   ACC:  68.75 %\n",
      "Epoch  4   1563  /  820 batch Loss:  1.120446801185608   ACC:  59.375 %\n",
      "Epoch  4   1563  /  840 batch Loss:  1.050515055656433   ACC:  65.625 %\n",
      "Epoch  4   1563  /  860 batch Loss:  1.1452162265777588   ACC:  53.125 %\n",
      "Epoch  4   1563  /  880 batch Loss:  0.9004735350608826   ACC:  71.875 %\n",
      "Epoch  4   1563  /  900 batch Loss:  1.2087018489837646   ACC:  53.125 %\n",
      "Epoch  4   1563  /  920 batch Loss:  0.7877885103225708   ACC:  75.0 %\n",
      "Epoch  4   1563  /  940 batch Loss:  0.9384435415267944   ACC:  65.625 %\n",
      "Epoch  4   1563  /  960 batch Loss:  0.9696133136749268   ACC:  68.75 %\n",
      "Epoch  4   1563  /  980 batch Loss:  1.0837482213974   ACC:  56.25 %\n",
      "Epoch  4   1563  /  1000 batch Loss:  0.6874624490737915   ACC:  78.125 %\n",
      "Epoch  4   1563  /  1020 batch Loss:  0.9378446340560913   ACC:  68.75 %\n",
      "Epoch  4   1563  /  1040 batch Loss:  1.0753955841064453   ACC:  59.375 %\n",
      "Epoch  4   1563  /  1060 batch Loss:  1.0676921606063843   ACC:  68.75 %\n",
      "Epoch  4   1563  /  1080 batch Loss:  0.9527314305305481   ACC:  68.75 %\n",
      "Epoch  4   1563  /  1100 batch Loss:  1.1243259906768799   ACC:  62.5 %\n",
      "Epoch  4   1563  /  1120 batch Loss:  0.8867411613464355   ACC:  65.625 %\n",
      "Epoch  4   1563  /  1140 batch Loss:  0.8572525382041931   ACC:  71.875 %\n",
      "Epoch  4   1563  /  1160 batch Loss:  0.9488940238952637   ACC:  65.625 %\n",
      "Epoch  4   1563  /  1180 batch Loss:  1.2916709184646606   ACC:  50.0 %\n",
      "Epoch  4   1563  /  1200 batch Loss:  0.681459903717041   ACC:  71.875 %\n",
      "Epoch  4   1563  /  1220 batch Loss:  0.6536047458648682   ACC:  75.0 %\n",
      "Epoch  4   1563  /  1240 batch Loss:  1.067179560661316   ACC:  65.625 %\n",
      "Epoch  4   1563  /  1260 batch Loss:  1.248415470123291   ACC:  53.125 %\n",
      "Epoch  4   1563  /  1280 batch Loss:  0.6910462379455566   ACC:  68.75 %\n",
      "Epoch  4   1563  /  1300 batch Loss:  1.2672725915908813   ACC:  59.375 %\n",
      "Epoch  4   1563  /  1320 batch Loss:  0.9545282125473022   ACC:  68.75 %\n",
      "Epoch  4   1563  /  1340 batch Loss:  1.1052947044372559   ACC:  50.0 %\n",
      "Epoch  4   1563  /  1360 batch Loss:  0.8703491687774658   ACC:  62.5 %\n",
      "Epoch  4   1563  /  1380 batch Loss:  0.8049206137657166   ACC:  71.875 %\n",
      "Epoch  4   1563  /  1400 batch Loss:  0.8400052189826965   ACC:  68.75 %\n",
      "Epoch  4   1563  /  1420 batch Loss:  1.3930974006652832   ACC:  53.125 %\n",
      "Epoch  4   1563  /  1440 batch Loss:  1.1436723470687866   ACC:  62.5 %\n",
      "Epoch  4   1563  /  1460 batch Loss:  0.98873370885849   ACC:  68.75 %\n",
      "Epoch  4   1563  /  1480 batch Loss:  0.8981637954711914   ACC:  65.625 %\n",
      "Epoch  4   1563  /  1500 batch Loss:  1.060796856880188   ACC:  56.25 %\n",
      "Epoch  4   1563  /  1520 batch Loss:  0.8244252800941467   ACC:  78.125 %\n",
      "Epoch  4   1563  /  1540 batch Loss:  1.3527345657348633   ACC:  56.25 %\n",
      "Epoch  4   1563  /  1560 batch Loss:  1.0414509773254395   ACC:  68.75 %\n",
      "Test acc: 67.31 \n",
      "\n",
      "Epoch  5   1563  /  20 batch Loss:  0.8243874907493591   ACC:  75.0 %\n",
      "Epoch  5   1563  /  40 batch Loss:  1.0367388725280762   ACC:  62.5 %\n",
      "Epoch  5   1563  /  60 batch Loss:  0.9344005584716797   ACC:  65.625 %\n",
      "Epoch  5   1563  /  80 batch Loss:  0.8956466317176819   ACC:  68.75 %\n",
      "Epoch  5   1563  /  100 batch Loss:  1.2142961025238037   ACC:  50.0 %\n",
      "Epoch  5   1563  /  120 batch Loss:  0.6530599594116211   ACC:  81.25 %\n",
      "Epoch  5   1563  /  140 batch Loss:  1.3657472133636475   ACC:  56.25 %\n",
      "Epoch  5   1563  /  160 batch Loss:  1.0976251363754272   ACC:  62.5 %\n",
      "Epoch  5   1563  /  180 batch Loss:  0.85459303855896   ACC:  78.125 %\n",
      "Epoch  5   1563  /  200 batch Loss:  1.0555229187011719   ACC:  59.375 %\n",
      "Epoch  5   1563  /  220 batch Loss:  1.2429760694503784   ACC:  62.5 %\n",
      "Epoch  5   1563  /  240 batch Loss:  1.1066405773162842   ACC:  53.125 %\n",
      "Epoch  5   1563  /  260 batch Loss:  0.70896977186203   ACC:  71.875 %\n",
      "Epoch  5   1563  /  280 batch Loss:  1.1082971096038818   ACC:  68.75 %\n",
      "Epoch  5   1563  /  300 batch Loss:  0.625715970993042   ACC:  75.0 %\n",
      "Epoch  5   1563  /  320 batch Loss:  1.1367441415786743   ACC:  62.5 %\n",
      "Epoch  5   1563  /  340 batch Loss:  1.1748696565628052   ACC:  62.5 %\n",
      "Epoch  5   1563  /  360 batch Loss:  0.7927969694137573   ACC:  68.75 %\n",
      "Epoch  5   1563  /  380 batch Loss:  0.8332287669181824   ACC:  68.75 %\n",
      "Epoch  5   1563  /  400 batch Loss:  0.8702881932258606   ACC:  65.625 %\n",
      "Epoch  5   1563  /  420 batch Loss:  1.2241532802581787   ACC:  62.5 %\n",
      "Epoch  5   1563  /  440 batch Loss:  0.6737266182899475   ACC:  65.625 %\n",
      "Epoch  5   1563  /  460 batch Loss:  0.8534192442893982   ACC:  78.125 %\n",
      "Epoch  5   1563  /  480 batch Loss:  1.0092588663101196   ACC:  81.25 %\n",
      "Epoch  5   1563  /  500 batch Loss:  0.7157586812973022   ACC:  78.125 %\n",
      "Epoch  5   1563  /  520 batch Loss:  0.6654943823814392   ACC:  81.25 %\n",
      "Epoch  5   1563  /  540 batch Loss:  0.9023181200027466   ACC:  56.25 %\n",
      "Epoch  5   1563  /  560 batch Loss:  0.8207546472549438   ACC:  68.75 %\n",
      "Epoch  5   1563  /  580 batch Loss:  0.7947397232055664   ACC:  68.75 %\n",
      "Epoch  5   1563  /  600 batch Loss:  0.8325544595718384   ACC:  71.875 %\n",
      "Epoch  5   1563  /  620 batch Loss:  0.9484910368919373   ACC:  65.625 %\n",
      "Epoch  5   1563  /  640 batch Loss:  0.8924232125282288   ACC:  68.75 %\n",
      "Epoch  5   1563  /  660 batch Loss:  0.9898473024368286   ACC:  59.375 %\n",
      "Epoch  5   1563  /  680 batch Loss:  0.8656291365623474   ACC:  65.625 %\n",
      "Epoch  5   1563  /  700 batch Loss:  1.3138257265090942   ACC:  43.75 %\n",
      "Epoch  5   1563  /  720 batch Loss:  1.2736659049987793   ACC:  56.25 %\n",
      "Epoch  5   1563  /  740 batch Loss:  0.9419739842414856   ACC:  62.5 %\n",
      "Epoch  5   1563  /  760 batch Loss:  0.8099852800369263   ACC:  68.75 %\n",
      "Epoch  5   1563  /  780 batch Loss:  1.0739610195159912   ACC:  59.375 %\n",
      "Epoch  5   1563  /  800 batch Loss:  0.8670874834060669   ACC:  65.625 %\n",
      "Epoch  5   1563  /  820 batch Loss:  1.0319268703460693   ACC:  62.5 %\n",
      "Epoch  5   1563  /  840 batch Loss:  0.9725890159606934   ACC:  65.625 %\n",
      "Epoch  5   1563  /  860 batch Loss:  0.7810631394386292   ACC:  71.875 %\n",
      "Epoch  5   1563  /  880 batch Loss:  1.1818474531173706   ACC:  62.5 %\n",
      "Epoch  5   1563  /  900 batch Loss:  0.7126385569572449   ACC:  78.125 %\n",
      "Epoch  5   1563  /  920 batch Loss:  1.0263490676879883   ACC:  53.125 %\n",
      "Epoch  5   1563  /  940 batch Loss:  0.990978479385376   ACC:  62.5 %\n",
      "Epoch  5   1563  /  960 batch Loss:  0.8963000774383545   ACC:  71.875 %\n",
      "Epoch  5   1563  /  980 batch Loss:  0.9067078232765198   ACC:  56.25 %\n",
      "Epoch  5   1563  /  1000 batch Loss:  0.7822527885437012   ACC:  71.875 %\n",
      "Epoch  5   1563  /  1020 batch Loss:  0.5639762282371521   ACC:  81.25 %\n",
      "Epoch  5   1563  /  1040 batch Loss:  0.7804639935493469   ACC:  78.125 %\n",
      "Epoch  5   1563  /  1060 batch Loss:  0.771061897277832   ACC:  71.875 %\n",
      "Epoch  5   1563  /  1080 batch Loss:  1.006864309310913   ACC:  65.625 %\n",
      "Epoch  5   1563  /  1100 batch Loss:  0.8226776123046875   ACC:  68.75 %\n",
      "Epoch  5   1563  /  1120 batch Loss:  0.6799100637435913   ACC:  78.125 %\n",
      "Epoch  5   1563  /  1140 batch Loss:  1.0144150257110596   ACC:  56.25 %\n",
      "Epoch  5   1563  /  1160 batch Loss:  0.7503551244735718   ACC:  78.125 %\n",
      "Epoch  5   1563  /  1180 batch Loss:  0.7850204706192017   ACC:  65.625 %\n",
      "Epoch  5   1563  /  1200 batch Loss:  0.833687961101532   ACC:  68.75 %\n",
      "Epoch  5   1563  /  1220 batch Loss:  1.084064245223999   ACC:  56.25 %\n",
      "Epoch  5   1563  /  1240 batch Loss:  0.6106240749359131   ACC:  87.5 %\n",
      "Epoch  5   1563  /  1260 batch Loss:  1.0427048206329346   ACC:  59.375 %\n",
      "Epoch  5   1563  /  1280 batch Loss:  0.9803521633148193   ACC:  59.375 %\n",
      "Epoch  5   1563  /  1300 batch Loss:  0.406656950712204   ACC:  87.5 %\n",
      "Epoch  5   1563  /  1320 batch Loss:  0.7901708483695984   ACC:  71.875 %\n",
      "Epoch  5   1563  /  1340 batch Loss:  1.019201636314392   ACC:  65.625 %\n",
      "Epoch  5   1563  /  1360 batch Loss:  1.0102906227111816   ACC:  65.625 %\n",
      "Epoch  5   1563  /  1380 batch Loss:  0.8233391046524048   ACC:  68.75 %\n",
      "Epoch  5   1563  /  1400 batch Loss:  0.9820132255554199   ACC:  65.625 %\n",
      "Epoch  5   1563  /  1420 batch Loss:  0.8602138161659241   ACC:  65.625 %\n",
      "Epoch  5   1563  /  1440 batch Loss:  0.9796190857887268   ACC:  65.625 %\n",
      "Epoch  5   1563  /  1460 batch Loss:  0.7310875058174133   ACC:  75.0 %\n",
      "Epoch  5   1563  /  1480 batch Loss:  0.6434352397918701   ACC:  68.75 %\n",
      "Epoch  5   1563  /  1500 batch Loss:  0.8918167352676392   ACC:  71.875 %\n",
      "Epoch  5   1563  /  1520 batch Loss:  0.950854480266571   ACC:  71.875 %\n",
      "Epoch  5   1563  /  1540 batch Loss:  0.9919621348381042   ACC:  62.5 %\n",
      "Epoch  5   1563  /  1560 batch Loss:  0.7819757461547852   ACC:  75.0 %\n",
      "Test acc: 70.64 \n",
      "\n",
      "Epoch  6   1563  /  20 batch Loss:  0.8155821561813354   ACC:  81.25 %\n",
      "Epoch  6   1563  /  40 batch Loss:  1.1945117712020874   ACC:  53.125 %\n",
      "Epoch  6   1563  /  60 batch Loss:  0.8523284792900085   ACC:  71.875 %\n",
      "Epoch  6   1563  /  80 batch Loss:  0.6715632081031799   ACC:  75.0 %\n",
      "Epoch  6   1563  /  100 batch Loss:  0.6341372728347778   ACC:  75.0 %\n",
      "Epoch  6   1563  /  120 batch Loss:  1.0676079988479614   ACC:  62.5 %\n",
      "Epoch  6   1563  /  140 batch Loss:  1.0593318939208984   ACC:  59.375 %\n",
      "Epoch  6   1563  /  160 batch Loss:  0.7211974859237671   ACC:  75.0 %\n",
      "Epoch  6   1563  /  180 batch Loss:  0.8726420998573303   ACC:  71.875 %\n",
      "Epoch  6   1563  /  200 batch Loss:  0.731949508190155   ACC:  75.0 %\n",
      "Epoch  6   1563  /  220 batch Loss:  0.8992109298706055   ACC:  62.5 %\n",
      "Epoch  6   1563  /  240 batch Loss:  0.5162251591682434   ACC:  87.5 %\n",
      "Epoch  6   1563  /  260 batch Loss:  0.5701109766960144   ACC:  81.25 %\n",
      "Epoch  6   1563  /  280 batch Loss:  0.9175381660461426   ACC:  59.375 %\n",
      "Epoch  6   1563  /  300 batch Loss:  0.7813210487365723   ACC:  78.125 %\n",
      "Epoch  6   1563  /  320 batch Loss:  0.5013083219528198   ACC:  75.0 %\n",
      "Epoch  6   1563  /  340 batch Loss:  0.8839192390441895   ACC:  68.75 %\n",
      "Epoch  6   1563  /  360 batch Loss:  0.5682071447372437   ACC:  84.375 %\n",
      "Epoch  6   1563  /  380 batch Loss:  0.9565481543540955   ACC:  59.375 %\n",
      "Epoch  6   1563  /  400 batch Loss:  0.4342501163482666   ACC:  87.5 %\n",
      "Epoch  6   1563  /  420 batch Loss:  0.7698314189910889   ACC:  78.125 %\n",
      "Epoch  6   1563  /  440 batch Loss:  0.8188573718070984   ACC:  75.0 %\n",
      "Epoch  6   1563  /  460 batch Loss:  0.6733139157295227   ACC:  87.5 %\n",
      "Epoch  6   1563  /  480 batch Loss:  0.6674280166625977   ACC:  75.0 %\n",
      "Epoch  6   1563  /  500 batch Loss:  0.8889055848121643   ACC:  62.5 %\n",
      "Epoch  6   1563  /  520 batch Loss:  0.7249571681022644   ACC:  75.0 %\n",
      "Epoch  6   1563  /  540 batch Loss:  0.5713554620742798   ACC:  81.25 %\n",
      "Epoch  6   1563  /  560 batch Loss:  1.167607307434082   ACC:  62.5 %\n",
      "Epoch  6   1563  /  580 batch Loss:  0.5890127420425415   ACC:  81.25 %\n",
      "Epoch  6   1563  /  600 batch Loss:  0.6850907206535339   ACC:  78.125 %\n",
      "Epoch  6   1563  /  620 batch Loss:  1.0238227844238281   ACC:  65.625 %\n",
      "Epoch  6   1563  /  640 batch Loss:  1.0153015851974487   ACC:  62.5 %\n",
      "Epoch  6   1563  /  660 batch Loss:  0.891769528388977   ACC:  71.875 %\n",
      "Epoch  6   1563  /  680 batch Loss:  1.0182008743286133   ACC:  65.625 %\n",
      "Epoch  6   1563  /  700 batch Loss:  1.1359916925430298   ACC:  71.875 %\n",
      "Epoch  6   1563  /  720 batch Loss:  0.7377591133117676   ACC:  71.875 %\n",
      "Epoch  6   1563  /  740 batch Loss:  0.7048748731613159   ACC:  78.125 %\n",
      "Epoch  6   1563  /  760 batch Loss:  0.7749688029289246   ACC:  71.875 %\n",
      "Epoch  6   1563  /  780 batch Loss:  0.9369679689407349   ACC:  71.875 %\n",
      "Epoch  6   1563  /  800 batch Loss:  0.9768096208572388   ACC:  68.75 %\n",
      "Epoch  6   1563  /  820 batch Loss:  0.5812211036682129   ACC:  81.25 %\n",
      "Epoch  6   1563  /  840 batch Loss:  0.6833736896514893   ACC:  78.125 %\n",
      "Epoch  6   1563  /  860 batch Loss:  0.757483959197998   ACC:  78.125 %\n",
      "Epoch  6   1563  /  880 batch Loss:  0.9615845084190369   ACC:  65.625 %\n",
      "Epoch  6   1563  /  900 batch Loss:  0.7344860434532166   ACC:  78.125 %\n",
      "Epoch  6   1563  /  920 batch Loss:  0.6557298302650452   ACC:  78.125 %\n",
      "Epoch  6   1563  /  940 batch Loss:  0.8803269267082214   ACC:  65.625 %\n",
      "Epoch  6   1563  /  960 batch Loss:  0.6980423331260681   ACC:  71.875 %\n",
      "Epoch  6   1563  /  980 batch Loss:  0.8119786977767944   ACC:  68.75 %\n",
      "Epoch  6   1563  /  1000 batch Loss:  0.9016667008399963   ACC:  71.875 %\n",
      "Epoch  6   1563  /  1020 batch Loss:  0.6470987796783447   ACC:  78.125 %\n",
      "Epoch  6   1563  /  1040 batch Loss:  1.0453709363937378   ACC:  71.875 %\n",
      "Epoch  6   1563  /  1060 batch Loss:  0.5340160131454468   ACC:  78.125 %\n",
      "Epoch  6   1563  /  1080 batch Loss:  1.266686201095581   ACC:  53.125 %\n",
      "Epoch  6   1563  /  1100 batch Loss:  0.5499992966651917   ACC:  71.875 %\n",
      "Epoch  6   1563  /  1120 batch Loss:  1.2282811403274536   ACC:  53.125 %\n",
      "Epoch  6   1563  /  1140 batch Loss:  1.108768343925476   ACC:  56.25 %\n",
      "Epoch  6   1563  /  1160 batch Loss:  0.8679083585739136   ACC:  71.875 %\n",
      "Epoch  6   1563  /  1180 batch Loss:  1.0177254676818848   ACC:  62.5 %\n",
      "Epoch  6   1563  /  1200 batch Loss:  0.44777363538742065   ACC:  84.375 %\n",
      "Epoch  6   1563  /  1220 batch Loss:  1.0943381786346436   ACC:  56.25 %\n",
      "Epoch  6   1563  /  1240 batch Loss:  1.1505118608474731   ACC:  68.75 %\n",
      "Epoch  6   1563  /  1260 batch Loss:  0.924435555934906   ACC:  65.625 %\n",
      "Epoch  6   1563  /  1280 batch Loss:  0.8629164099693298   ACC:  68.75 %\n",
      "Epoch  6   1563  /  1300 batch Loss:  0.7240421175956726   ACC:  75.0 %\n",
      "Epoch  6   1563  /  1320 batch Loss:  0.5690442323684692   ACC:  75.0 %\n",
      "Epoch  6   1563  /  1340 batch Loss:  0.9355350136756897   ACC:  65.625 %\n",
      "Epoch  6   1563  /  1360 batch Loss:  1.463659644126892   ACC:  56.25 %\n",
      "Epoch  6   1563  /  1380 batch Loss:  1.238490343093872   ACC:  53.125 %\n",
      "Epoch  6   1563  /  1400 batch Loss:  0.8558337688446045   ACC:  59.375 %\n",
      "Epoch  6   1563  /  1420 batch Loss:  0.8092000484466553   ACC:  68.75 %\n",
      "Epoch  6   1563  /  1440 batch Loss:  1.0033884048461914   ACC:  68.75 %\n",
      "Epoch  6   1563  /  1460 batch Loss:  0.6915349960327148   ACC:  75.0 %\n",
      "Epoch  6   1563  /  1480 batch Loss:  0.6811663508415222   ACC:  75.0 %\n",
      "Epoch  6   1563  /  1500 batch Loss:  0.7349057793617249   ACC:  71.875 %\n",
      "Epoch  6   1563  /  1520 batch Loss:  1.2061034440994263   ACC:  56.25 %\n",
      "Epoch  6   1563  /  1540 batch Loss:  0.6254115104675293   ACC:  84.375 %\n",
      "Epoch  6   1563  /  1560 batch Loss:  0.8769713044166565   ACC:  62.5 %\n",
      "Test acc: 71.03 \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13576/3721088854.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/mrjaehong/handwriting_gen/pytorch-Stand-Alone-Self-Attention/env/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/mrjaehong/handwriting_gen/pytorch-Stand-Alone-Self-Attention/env/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/mrjaehong/handwriting_gen/pytorch-Stand-Alone-Self-Attention/env/lib/python3.7/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    150\u001b[0m                   \u001b[0mdampening\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdampening\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                   \u001b[0mnesterov\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnesterov\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m                   maximize=maximize,)\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0;31m# update momentum_buffers in state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/mrjaehong/handwriting_gen/pytorch-Stand-Alone-Self-Attention/env/lib/python3.7/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36msgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, weight_decay, momentum, lr, dampening, nesterov, maximize)\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0mmomentum_buffer_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m                 \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdampening\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnesterov\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "print_interval = 20\n",
    "\n",
    "## 30에폭 마다 러닝레이트 줄임\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    decayed_lr = lr * (0.1 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = decayed_lr\n",
    "\n",
    "for epoch in range(1,epochs+1):\n",
    "    model.train()\n",
    "    train_acc = 0.0\n",
    "    step = 0\n",
    "    for data, target in train_loader:\n",
    "        adjust_learning_rate(optimizer, epoch)\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        ## onnx 저장\n",
    "\n",
    "\n",
    "        y_pred = output.data.max(1)[1]\n",
    "        \n",
    "        acc = float(y_pred.eq(target.data).sum()) / len(data) * 100.\n",
    "        train_acc += acc\n",
    "        step += 1\n",
    "        if step % print_interval == 0:\n",
    "            print(\"Epoch \",epoch,\" \",len(train_loader),\" / \",step,\"batch Loss: \",loss.item(),\"  ACC: \",acc,\"%\")\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            output = model(data)\n",
    "            prediction = output.data.max(1)[1]\n",
    "            correct += prediction.eq(target.data).sum()\n",
    "\n",
    "    acc = 100. * float(correct) / len(test_loader.dataset)\n",
    "    print('Test acc: {0:.2f} \\n'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## onnx 저장\n",
    "## operator_export_type=torch.onnx.OperatorExportTypes.ONNX_ATEN_FALLBACK 꼭 해줘야함\n",
    "## unfold 연산을 \n",
    "# https://github.com/pytorch/pytorch/issues/14395#issuecomment-444697403\n",
    "\n",
    "\n",
    "# import torch.onnx\n",
    "# torch.onnx.export(model.cpu(),data.cpu(),'./stand_alone_self.onnx',export_params=False,opset_version=12,\n",
    "#                   operator_export_type=torch.onnx.OperatorExportTypes.ONNX_ATEN_FALLBACK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "07b1216af5147bda247a947cdd561e2b589c1fe1b25ba98cf598d38fde5bfe85"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
