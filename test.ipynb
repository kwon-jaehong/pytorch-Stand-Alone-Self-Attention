{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from model import ResNet50, ResNet38, ResNet26\n",
    "\n",
    "# 논문 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## 데이터 셋은 CIFAR10으로\n",
    "num_classes = 10\n",
    "batch_size = 32\n",
    "num_workers = 4\n",
    "\n",
    "## 데이터셋 로드\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=(0.4914, 0.4822, 0.4465),\n",
    "        std=(0.2470, 0.2435, 0.2616)\n",
    "    )\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=(0.4914, 0.4822, 0.4465),\n",
    "        std=(0.2470, 0.2435, 0.2616)\n",
    "    )\n",
    "])\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('data', train=True, download=True, transform=transform_train),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('data', train=False, transform=transform_test),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem = False\n",
    "model = ResNet26(num_classes=num_classes, stem=stem)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 1\n",
    "best_acc = 0.0\n",
    "\n",
    "lr = 0.01\n",
    "momentum =0.9\n",
    "weight_decay = 0.0001\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1   1563  /  20 batch Loss:  2.245594024658203   ACC:  31.25 %\n",
      "Epoch  1   1563  /  40 batch Loss:  2.626516103744507   ACC:  18.75 %\n",
      "Epoch  1   1563  /  60 batch Loss:  2.2531518936157227   ACC:  15.625 %\n",
      "Epoch  1   1563  /  80 batch Loss:  2.061490297317505   ACC:  31.25 %\n",
      "Epoch  1   1563  /  100 batch Loss:  2.0909833908081055   ACC:  31.25 %\n",
      "Epoch  1   1563  /  120 batch Loss:  2.115436553955078   ACC:  28.125 %\n",
      "Epoch  1   1563  /  140 batch Loss:  2.0981976985931396   ACC:  12.5 %\n",
      "Epoch  1   1563  /  160 batch Loss:  2.080796957015991   ACC:  12.5 %\n",
      "Epoch  1   1563  /  180 batch Loss:  1.9663963317871094   ACC:  31.25 %\n",
      "Epoch  1   1563  /  200 batch Loss:  1.8342472314834595   ACC:  37.5 %\n",
      "Epoch  1   1563  /  220 batch Loss:  1.7848618030548096   ACC:  40.625 %\n",
      "Epoch  1   1563  /  240 batch Loss:  1.78364896774292   ACC:  28.125 %\n",
      "Epoch  1   1563  /  260 batch Loss:  2.096372604370117   ACC:  28.125 %\n",
      "Epoch  1   1563  /  280 batch Loss:  2.094644784927368   ACC:  28.125 %\n",
      "Epoch  1   1563  /  300 batch Loss:  1.7257730960845947   ACC:  43.75 %\n",
      "Epoch  1   1563  /  320 batch Loss:  1.738049864768982   ACC:  28.125 %\n",
      "Epoch  1   1563  /  340 batch Loss:  1.8121726512908936   ACC:  37.5 %\n",
      "Epoch  1   1563  /  360 batch Loss:  1.9274401664733887   ACC:  37.5 %\n",
      "Epoch  1   1563  /  380 batch Loss:  1.4733002185821533   ACC:  34.375 %\n",
      "Epoch  1   1563  /  400 batch Loss:  1.8192684650421143   ACC:  31.25 %\n",
      "Epoch  1   1563  /  420 batch Loss:  1.9127092361450195   ACC:  31.25 %\n",
      "Epoch  1   1563  /  440 batch Loss:  1.6137208938598633   ACC:  37.5 %\n",
      "Epoch  1   1563  /  460 batch Loss:  1.5875364542007446   ACC:  34.375 %\n",
      "Epoch  1   1563  /  480 batch Loss:  1.6657017469406128   ACC:  34.375 %\n",
      "Epoch  1   1563  /  500 batch Loss:  1.3564516305923462   ACC:  50.0 %\n",
      "Epoch  1   1563  /  520 batch Loss:  1.5073715448379517   ACC:  34.375 %\n",
      "Epoch  1   1563  /  540 batch Loss:  1.6833534240722656   ACC:  50.0 %\n",
      "Epoch  1   1563  /  560 batch Loss:  1.966321587562561   ACC:  28.125 %\n",
      "Epoch  1   1563  /  580 batch Loss:  1.5679620504379272   ACC:  43.75 %\n",
      "Epoch  1   1563  /  600 batch Loss:  1.5728806257247925   ACC:  50.0 %\n",
      "Epoch  1   1563  /  620 batch Loss:  1.6821526288986206   ACC:  46.875 %\n",
      "Epoch  1   1563  /  640 batch Loss:  1.573630928993225   ACC:  43.75 %\n",
      "Epoch  1   1563  /  660 batch Loss:  1.7749683856964111   ACC:  31.25 %\n",
      "Epoch  1   1563  /  680 batch Loss:  1.516723394393921   ACC:  34.375 %\n",
      "Epoch  1   1563  /  700 batch Loss:  1.4285086393356323   ACC:  50.0 %\n",
      "Epoch  1   1563  /  720 batch Loss:  1.52248215675354   ACC:  46.875 %\n",
      "Epoch  1   1563  /  740 batch Loss:  1.771918773651123   ACC:  31.25 %\n",
      "Epoch  1   1563  /  760 batch Loss:  1.886074185371399   ACC:  34.375 %\n",
      "Epoch  1   1563  /  780 batch Loss:  1.9156655073165894   ACC:  37.5 %\n",
      "Epoch  1   1563  /  800 batch Loss:  1.6078031063079834   ACC:  34.375 %\n",
      "Epoch  1   1563  /  820 batch Loss:  1.7381222248077393   ACC:  28.125 %\n",
      "Epoch  1   1563  /  840 batch Loss:  1.3304609060287476   ACC:  46.875 %\n",
      "Epoch  1   1563  /  860 batch Loss:  1.6730949878692627   ACC:  43.75 %\n",
      "Epoch  1   1563  /  880 batch Loss:  1.9101728200912476   ACC:  34.375 %\n",
      "Epoch  1   1563  /  900 batch Loss:  1.9127117395401   ACC:  37.5 %\n",
      "Epoch  1   1563  /  920 batch Loss:  1.537558674812317   ACC:  37.5 %\n",
      "Epoch  1   1563  /  940 batch Loss:  1.7593272924423218   ACC:  28.125 %\n",
      "Epoch  1   1563  /  960 batch Loss:  1.2569208145141602   ACC:  53.125 %\n",
      "Epoch  1   1563  /  980 batch Loss:  1.4775445461273193   ACC:  43.75 %\n",
      "Epoch  1   1563  /  1000 batch Loss:  1.6096612215042114   ACC:  46.875 %\n",
      "Epoch  1   1563  /  1020 batch Loss:  1.594733715057373   ACC:  37.5 %\n",
      "Epoch  1   1563  /  1040 batch Loss:  1.272774338722229   ACC:  53.125 %\n",
      "Epoch  1   1563  /  1060 batch Loss:  1.7426177263259888   ACC:  34.375 %\n",
      "Epoch  1   1563  /  1080 batch Loss:  1.0092631578445435   ACC:  62.5 %\n",
      "Epoch  1   1563  /  1100 batch Loss:  1.5831241607666016   ACC:  37.5 %\n",
      "Epoch  1   1563  /  1120 batch Loss:  1.4349342584609985   ACC:  50.0 %\n",
      "Epoch  1   1563  /  1140 batch Loss:  1.665116786956787   ACC:  43.75 %\n",
      "Epoch  1   1563  /  1160 batch Loss:  1.5039985179901123   ACC:  50.0 %\n",
      "Epoch  1   1563  /  1180 batch Loss:  1.3680022954940796   ACC:  46.875 %\n",
      "Epoch  1   1563  /  1200 batch Loss:  1.3442280292510986   ACC:  43.75 %\n",
      "Epoch  1   1563  /  1220 batch Loss:  1.3922556638717651   ACC:  53.125 %\n",
      "Epoch  1   1563  /  1240 batch Loss:  1.209309697151184   ACC:  46.875 %\n",
      "Epoch  1   1563  /  1260 batch Loss:  1.3198117017745972   ACC:  59.375 %\n",
      "Epoch  1   1563  /  1280 batch Loss:  1.6772652864456177   ACC:  40.625 %\n",
      "Epoch  1   1563  /  1300 batch Loss:  1.4856200218200684   ACC:  46.875 %\n",
      "Epoch  1   1563  /  1320 batch Loss:  1.3727362155914307   ACC:  53.125 %\n",
      "Epoch  1   1563  /  1340 batch Loss:  2.209468126296997   ACC:  28.125 %\n",
      "Epoch  1   1563  /  1360 batch Loss:  1.2940655946731567   ACC:  50.0 %\n",
      "Epoch  1   1563  /  1380 batch Loss:  1.6280474662780762   ACC:  46.875 %\n",
      "Epoch  1   1563  /  1400 batch Loss:  1.467904806137085   ACC:  53.125 %\n",
      "Epoch  1   1563  /  1420 batch Loss:  1.068625569343567   ACC:  53.125 %\n",
      "Epoch  1   1563  /  1440 batch Loss:  1.0762686729431152   ACC:  75.0 %\n",
      "Epoch  1   1563  /  1460 batch Loss:  1.3660770654678345   ACC:  40.625 %\n",
      "Epoch  1   1563  /  1480 batch Loss:  1.4336512088775635   ACC:  43.75 %\n",
      "Epoch  1   1563  /  1500 batch Loss:  1.2540266513824463   ACC:  59.375 %\n",
      "Epoch  1   1563  /  1520 batch Loss:  1.252503514289856   ACC:  59.375 %\n",
      "Epoch  1   1563  /  1540 batch Loss:  1.3782806396484375   ACC:  37.5 %\n",
      "Epoch  1   1563  /  1560 batch Loss:  1.1604855060577393   ACC:  50.0 %\n",
      "Test acc: 53.43 \n",
      "\n",
      "Epoch  2   1563  /  20 batch Loss:  0.9916476011276245   ACC:  62.5 %\n",
      "Epoch  2   1563  /  40 batch Loss:  1.4356272220611572   ACC:  46.875 %\n",
      "Epoch  2   1563  /  60 batch Loss:  1.2351047992706299   ACC:  50.0 %\n",
      "Epoch  2   1563  /  80 batch Loss:  1.161759853363037   ACC:  62.5 %\n",
      "Epoch  2   1563  /  100 batch Loss:  1.5255852937698364   ACC:  43.75 %\n",
      "Epoch  2   1563  /  120 batch Loss:  1.0313348770141602   ACC:  62.5 %\n",
      "Epoch  2   1563  /  140 batch Loss:  1.2879283428192139   ACC:  50.0 %\n",
      "Epoch  2   1563  /  160 batch Loss:  1.1277786493301392   ACC:  59.375 %\n",
      "Epoch  2   1563  /  180 batch Loss:  1.5205355882644653   ACC:  50.0 %\n",
      "Epoch  2   1563  /  200 batch Loss:  1.1441121101379395   ACC:  59.375 %\n",
      "Epoch  2   1563  /  220 batch Loss:  1.5806820392608643   ACC:  40.625 %\n",
      "Epoch  2   1563  /  240 batch Loss:  1.5900940895080566   ACC:  43.75 %\n",
      "Epoch  2   1563  /  260 batch Loss:  1.3300353288650513   ACC:  46.875 %\n",
      "Epoch  2   1563  /  280 batch Loss:  1.5187335014343262   ACC:  43.75 %\n",
      "Epoch  2   1563  /  300 batch Loss:  1.3355435132980347   ACC:  43.75 %\n",
      "Epoch  2   1563  /  320 batch Loss:  1.1329563856124878   ACC:  53.125 %\n",
      "Epoch  2   1563  /  340 batch Loss:  1.0654250383377075   ACC:  53.125 %\n",
      "Epoch  2   1563  /  360 batch Loss:  1.3404804468154907   ACC:  59.375 %\n",
      "Epoch  2   1563  /  380 batch Loss:  1.2306019067764282   ACC:  50.0 %\n",
      "Epoch  2   1563  /  400 batch Loss:  1.1623741388320923   ACC:  59.375 %\n",
      "Epoch  2   1563  /  420 batch Loss:  0.9358593821525574   ACC:  59.375 %\n",
      "Epoch  2   1563  /  440 batch Loss:  1.0378347635269165   ACC:  71.875 %\n",
      "Epoch  2   1563  /  460 batch Loss:  1.1208446025848389   ACC:  65.625 %\n",
      "Epoch  2   1563  /  480 batch Loss:  1.2520527839660645   ACC:  46.875 %\n",
      "Epoch  2   1563  /  500 batch Loss:  1.3968474864959717   ACC:  50.0 %\n",
      "Epoch  2   1563  /  520 batch Loss:  1.6244285106658936   ACC:  37.5 %\n",
      "Epoch  2   1563  /  540 batch Loss:  1.1981639862060547   ACC:  62.5 %\n",
      "Epoch  2   1563  /  560 batch Loss:  0.7908029556274414   ACC:  68.75 %\n",
      "Epoch  2   1563  /  580 batch Loss:  1.0385477542877197   ACC:  56.25 %\n",
      "Epoch  2   1563  /  600 batch Loss:  1.1922451257705688   ACC:  56.25 %\n",
      "Epoch  2   1563  /  620 batch Loss:  0.972407877445221   ACC:  75.0 %\n",
      "Epoch  2   1563  /  640 batch Loss:  1.2850258350372314   ACC:  40.625 %\n",
      "Epoch  2   1563  /  660 batch Loss:  1.0050746202468872   ACC:  68.75 %\n",
      "Epoch  2   1563  /  680 batch Loss:  1.261864423751831   ACC:  46.875 %\n",
      "Epoch  2   1563  /  700 batch Loss:  1.1414284706115723   ACC:  56.25 %\n",
      "Epoch  2   1563  /  720 batch Loss:  1.1264402866363525   ACC:  53.125 %\n",
      "Epoch  2   1563  /  740 batch Loss:  0.9011650681495667   ACC:  65.625 %\n",
      "Epoch  2   1563  /  760 batch Loss:  0.798109769821167   ACC:  71.875 %\n",
      "Epoch  2   1563  /  780 batch Loss:  1.33869469165802   ACC:  46.875 %\n",
      "Epoch  2   1563  /  800 batch Loss:  1.1878036260604858   ACC:  46.875 %\n",
      "Epoch  2   1563  /  820 batch Loss:  1.4724043607711792   ACC:  50.0 %\n",
      "Epoch  2   1563  /  840 batch Loss:  0.7916499376296997   ACC:  78.125 %\n",
      "Epoch  2   1563  /  860 batch Loss:  0.9981931447982788   ACC:  65.625 %\n",
      "Epoch  2   1563  /  880 batch Loss:  1.1394188404083252   ACC:  53.125 %\n",
      "Epoch  2   1563  /  900 batch Loss:  1.173072338104248   ACC:  56.25 %\n",
      "Epoch  2   1563  /  920 batch Loss:  1.1528953313827515   ACC:  65.625 %\n",
      "Epoch  2   1563  /  940 batch Loss:  1.449457049369812   ACC:  46.875 %\n",
      "Epoch  2   1563  /  960 batch Loss:  1.2602285146713257   ACC:  53.125 %\n",
      "Epoch  2   1563  /  980 batch Loss:  0.8950158357620239   ACC:  59.375 %\n",
      "Epoch  2   1563  /  1000 batch Loss:  1.0320611000061035   ACC:  59.375 %\n",
      "Epoch  2   1563  /  1020 batch Loss:  2.0281383991241455   ACC:  34.375 %\n",
      "Epoch  2   1563  /  1040 batch Loss:  1.2693049907684326   ACC:  53.125 %\n",
      "Epoch  2   1563  /  1060 batch Loss:  1.2617372274398804   ACC:  59.375 %\n",
      "Epoch  2   1563  /  1080 batch Loss:  0.9502413272857666   ACC:  56.25 %\n",
      "Epoch  2   1563  /  1100 batch Loss:  1.1395463943481445   ACC:  62.5 %\n",
      "Epoch  2   1563  /  1120 batch Loss:  1.0219351053237915   ACC:  56.25 %\n",
      "Epoch  2   1563  /  1140 batch Loss:  1.1622775793075562   ACC:  46.875 %\n",
      "Epoch  2   1563  /  1160 batch Loss:  1.0711637735366821   ACC:  68.75 %\n",
      "Epoch  2   1563  /  1180 batch Loss:  0.911751925945282   ACC:  65.625 %\n",
      "Epoch  2   1563  /  1200 batch Loss:  1.0635839700698853   ACC:  59.375 %\n",
      "Epoch  2   1563  /  1220 batch Loss:  1.121047854423523   ACC:  53.125 %\n",
      "Epoch  2   1563  /  1240 batch Loss:  1.0702875852584839   ACC:  53.125 %\n",
      "Epoch  2   1563  /  1260 batch Loss:  1.3655096292495728   ACC:  50.0 %\n",
      "Epoch  2   1563  /  1280 batch Loss:  1.4361310005187988   ACC:  56.25 %\n",
      "Epoch  2   1563  /  1300 batch Loss:  1.1479562520980835   ACC:  62.5 %\n",
      "Epoch  2   1563  /  1320 batch Loss:  1.293892502784729   ACC:  50.0 %\n",
      "Epoch  2   1563  /  1340 batch Loss:  0.9561068415641785   ACC:  65.625 %\n",
      "Epoch  2   1563  /  1360 batch Loss:  1.0343141555786133   ACC:  65.625 %\n",
      "Epoch  2   1563  /  1380 batch Loss:  1.1580188274383545   ACC:  53.125 %\n",
      "Epoch  2   1563  /  1400 batch Loss:  1.137932538986206   ACC:  56.25 %\n",
      "Epoch  2   1563  /  1420 batch Loss:  0.9781461954116821   ACC:  65.625 %\n",
      "Epoch  2   1563  /  1440 batch Loss:  1.2101157903671265   ACC:  56.25 %\n",
      "Epoch  2   1563  /  1460 batch Loss:  0.9706237316131592   ACC:  75.0 %\n",
      "Epoch  2   1563  /  1480 batch Loss:  1.1210747957229614   ACC:  59.375 %\n",
      "Epoch  2   1563  /  1500 batch Loss:  0.799058735370636   ACC:  62.5 %\n",
      "Epoch  2   1563  /  1520 batch Loss:  0.8973824977874756   ACC:  65.625 %\n",
      "Epoch  2   1563  /  1540 batch Loss:  1.004367470741272   ACC:  56.25 %\n",
      "Epoch  2   1563  /  1560 batch Loss:  1.277235746383667   ACC:  62.5 %\n",
      "Test acc: 60.07 \n",
      "\n",
      "Epoch  3   1563  /  20 batch Loss:  1.1871240139007568   ACC:  46.875 %\n",
      "Epoch  3   1563  /  40 batch Loss:  0.8820303082466125   ACC:  65.625 %\n",
      "Epoch  3   1563  /  60 batch Loss:  0.9728798866271973   ACC:  56.25 %\n",
      "Epoch  3   1563  /  80 batch Loss:  1.2191370725631714   ACC:  56.25 %\n",
      "Epoch  3   1563  /  100 batch Loss:  1.0082412958145142   ACC:  62.5 %\n",
      "Epoch  3   1563  /  120 batch Loss:  1.0244190692901611   ACC:  65.625 %\n",
      "Epoch  3   1563  /  140 batch Loss:  0.6445838212966919   ACC:  75.0 %\n",
      "Epoch  3   1563  /  160 batch Loss:  1.0301162004470825   ACC:  65.625 %\n",
      "Epoch  3   1563  /  180 batch Loss:  0.8271781802177429   ACC:  71.875 %\n",
      "Epoch  3   1563  /  200 batch Loss:  1.0753966569900513   ACC:  53.125 %\n",
      "Epoch  3   1563  /  220 batch Loss:  0.8485177755355835   ACC:  78.125 %\n",
      "Epoch  3   1563  /  240 batch Loss:  0.9966272711753845   ACC:  65.625 %\n",
      "Epoch  3   1563  /  260 batch Loss:  0.9692769050598145   ACC:  65.625 %\n",
      "Epoch  3   1563  /  280 batch Loss:  1.0570447444915771   ACC:  62.5 %\n",
      "Epoch  3   1563  /  300 batch Loss:  1.2719954252243042   ACC:  53.125 %\n",
      "Epoch  3   1563  /  320 batch Loss:  0.9100503325462341   ACC:  65.625 %\n",
      "Epoch  3   1563  /  340 batch Loss:  0.7878189086914062   ACC:  81.25 %\n",
      "Epoch  3   1563  /  360 batch Loss:  1.1686393022537231   ACC:  62.5 %\n",
      "Epoch  3   1563  /  380 batch Loss:  1.0212998390197754   ACC:  71.875 %\n",
      "Epoch  3   1563  /  400 batch Loss:  1.0334888696670532   ACC:  62.5 %\n",
      "Epoch  3   1563  /  420 batch Loss:  1.0648857355117798   ACC:  78.125 %\n",
      "Epoch  3   1563  /  440 batch Loss:  1.044716238975525   ACC:  68.75 %\n",
      "Epoch  3   1563  /  460 batch Loss:  0.5187618136405945   ACC:  78.125 %\n",
      "Epoch  3   1563  /  480 batch Loss:  0.7492833137512207   ACC:  78.125 %\n",
      "Epoch  3   1563  /  500 batch Loss:  1.028507113456726   ACC:  65.625 %\n",
      "Epoch  3   1563  /  520 batch Loss:  0.8555371165275574   ACC:  68.75 %\n",
      "Epoch  3   1563  /  540 batch Loss:  0.9039189219474792   ACC:  59.375 %\n",
      "Epoch  3   1563  /  560 batch Loss:  0.9611172676086426   ACC:  71.875 %\n",
      "Epoch  3   1563  /  580 batch Loss:  1.0220468044281006   ACC:  62.5 %\n",
      "Epoch  3   1563  /  600 batch Loss:  0.6511268019676208   ACC:  81.25 %\n",
      "Epoch  3   1563  /  620 batch Loss:  1.1882734298706055   ACC:  59.375 %\n",
      "Epoch  3   1563  /  640 batch Loss:  0.8104979991912842   ACC:  68.75 %\n",
      "Epoch  3   1563  /  660 batch Loss:  1.2817820310592651   ACC:  59.375 %\n",
      "Epoch  3   1563  /  680 batch Loss:  1.0418663024902344   ACC:  65.625 %\n",
      "Epoch  3   1563  /  700 batch Loss:  1.0426028966903687   ACC:  71.875 %\n",
      "Epoch  3   1563  /  720 batch Loss:  0.9909579157829285   ACC:  62.5 %\n",
      "Epoch  3   1563  /  740 batch Loss:  0.8324389457702637   ACC:  71.875 %\n",
      "Epoch  3   1563  /  760 batch Loss:  0.9397541284561157   ACC:  62.5 %\n",
      "Epoch  3   1563  /  780 batch Loss:  1.0336008071899414   ACC:  62.5 %\n",
      "Epoch  3   1563  /  800 batch Loss:  0.7965352535247803   ACC:  65.625 %\n",
      "Epoch  3   1563  /  820 batch Loss:  0.8092401027679443   ACC:  71.875 %\n",
      "Epoch  3   1563  /  840 batch Loss:  1.1153870820999146   ACC:  59.375 %\n",
      "Epoch  3   1563  /  860 batch Loss:  1.0926194190979004   ACC:  65.625 %\n",
      "Epoch  3   1563  /  880 batch Loss:  0.9487006664276123   ACC:  71.875 %\n",
      "Epoch  3   1563  /  900 batch Loss:  0.965691864490509   ACC:  65.625 %\n",
      "Epoch  3   1563  /  920 batch Loss:  0.804663896560669   ACC:  71.875 %\n",
      "Epoch  3   1563  /  940 batch Loss:  0.8804322481155396   ACC:  65.625 %\n",
      "Epoch  3   1563  /  960 batch Loss:  1.1881327629089355   ACC:  65.625 %\n",
      "Epoch  3   1563  /  980 batch Loss:  0.8575291633605957   ACC:  68.75 %\n",
      "Epoch  3   1563  /  1000 batch Loss:  0.5926639437675476   ACC:  78.125 %\n",
      "Epoch  3   1563  /  1020 batch Loss:  1.007857322692871   ACC:  68.75 %\n",
      "Epoch  3   1563  /  1040 batch Loss:  0.8677464723587036   ACC:  71.875 %\n",
      "Epoch  3   1563  /  1060 batch Loss:  0.604273796081543   ACC:  81.25 %\n",
      "Epoch  3   1563  /  1080 batch Loss:  0.7506449818611145   ACC:  78.125 %\n",
      "Epoch  3   1563  /  1100 batch Loss:  1.0755133628845215   ACC:  65.625 %\n",
      "Epoch  3   1563  /  1120 batch Loss:  1.0523619651794434   ACC:  65.625 %\n",
      "Epoch  3   1563  /  1140 batch Loss:  1.349479079246521   ACC:  59.375 %\n",
      "Epoch  3   1563  /  1160 batch Loss:  1.11360502243042   ACC:  53.125 %\n",
      "Epoch  3   1563  /  1180 batch Loss:  0.9074853658676147   ACC:  62.5 %\n",
      "Epoch  3   1563  /  1200 batch Loss:  0.7302351593971252   ACC:  68.75 %\n",
      "Epoch  3   1563  /  1220 batch Loss:  0.5686538219451904   ACC:  75.0 %\n",
      "Epoch  3   1563  /  1240 batch Loss:  0.770026445388794   ACC:  62.5 %\n",
      "Epoch  3   1563  /  1260 batch Loss:  0.9687044620513916   ACC:  56.25 %\n",
      "Epoch  3   1563  /  1280 batch Loss:  0.8523951768875122   ACC:  62.5 %\n",
      "Epoch  3   1563  /  1300 batch Loss:  0.89632248878479   ACC:  71.875 %\n",
      "Epoch  3   1563  /  1320 batch Loss:  0.959026575088501   ACC:  78.125 %\n",
      "Epoch  3   1563  /  1340 batch Loss:  1.0053374767303467   ACC:  71.875 %\n",
      "Epoch  3   1563  /  1360 batch Loss:  0.8540825247764587   ACC:  71.875 %\n",
      "Epoch  3   1563  /  1380 batch Loss:  0.9290481209754944   ACC:  62.5 %\n",
      "Epoch  3   1563  /  1400 batch Loss:  0.9274236559867859   ACC:  68.75 %\n",
      "Epoch  3   1563  /  1420 batch Loss:  0.6986595392227173   ACC:  78.125 %\n",
      "Epoch  3   1563  /  1440 batch Loss:  0.8146132230758667   ACC:  71.875 %\n",
      "Epoch  3   1563  /  1460 batch Loss:  1.1048964262008667   ACC:  56.25 %\n",
      "Epoch  3   1563  /  1480 batch Loss:  1.066367745399475   ACC:  68.75 %\n",
      "Epoch  3   1563  /  1500 batch Loss:  0.7989780306816101   ACC:  75.0 %\n",
      "Epoch  3   1563  /  1520 batch Loss:  0.9787630438804626   ACC:  71.875 %\n",
      "Epoch  3   1563  /  1540 batch Loss:  0.7309980392456055   ACC:  75.0 %\n",
      "Epoch  3   1563  /  1560 batch Loss:  0.7293664216995239   ACC:  71.875 %\n",
      "Test acc: 66.54 \n",
      "\n",
      "Epoch  4   1563  /  20 batch Loss:  0.9547725915908813   ACC:  71.875 %\n",
      "Epoch  4   1563  /  40 batch Loss:  0.4731169044971466   ACC:  81.25 %\n",
      "Epoch  4   1563  /  60 batch Loss:  0.8028273582458496   ACC:  68.75 %\n",
      "Epoch  4   1563  /  80 batch Loss:  0.7827895879745483   ACC:  71.875 %\n",
      "Epoch  4   1563  /  100 batch Loss:  0.9163558483123779   ACC:  59.375 %\n",
      "Epoch  4   1563  /  120 batch Loss:  0.7839843034744263   ACC:  65.625 %\n",
      "Epoch  4   1563  /  140 batch Loss:  0.9386953711509705   ACC:  75.0 %\n",
      "Epoch  4   1563  /  160 batch Loss:  1.0372114181518555   ACC:  65.625 %\n",
      "Epoch  4   1563  /  180 batch Loss:  0.669736385345459   ACC:  78.125 %\n",
      "Epoch  4   1563  /  200 batch Loss:  1.204445481300354   ACC:  59.375 %\n",
      "Epoch  4   1563  /  220 batch Loss:  0.9405720829963684   ACC:  65.625 %\n",
      "Epoch  4   1563  /  240 batch Loss:  0.905423104763031   ACC:  65.625 %\n",
      "Epoch  4   1563  /  260 batch Loss:  0.8662642240524292   ACC:  65.625 %\n",
      "Epoch  4   1563  /  280 batch Loss:  1.0899922847747803   ACC:  68.75 %\n",
      "Epoch  4   1563  /  300 batch Loss:  0.8918938636779785   ACC:  71.875 %\n",
      "Epoch  4   1563  /  320 batch Loss:  0.8035328984260559   ACC:  68.75 %\n",
      "Epoch  4   1563  /  340 batch Loss:  0.8807536363601685   ACC:  65.625 %\n",
      "Epoch  4   1563  /  360 batch Loss:  0.954950213432312   ACC:  71.875 %\n",
      "Epoch  4   1563  /  380 batch Loss:  0.5898408889770508   ACC:  71.875 %\n",
      "Epoch  4   1563  /  400 batch Loss:  0.7294708490371704   ACC:  78.125 %\n",
      "Epoch  4   1563  /  420 batch Loss:  0.8010929226875305   ACC:  81.25 %\n",
      "Epoch  4   1563  /  440 batch Loss:  0.8515439033508301   ACC:  71.875 %\n",
      "Epoch  4   1563  /  460 batch Loss:  1.0390814542770386   ACC:  75.0 %\n",
      "Epoch  4   1563  /  480 batch Loss:  0.6713482141494751   ACC:  71.875 %\n",
      "Epoch  4   1563  /  500 batch Loss:  0.7324423789978027   ACC:  68.75 %\n",
      "Epoch  4   1563  /  520 batch Loss:  0.5306988954544067   ACC:  78.125 %\n",
      "Epoch  4   1563  /  540 batch Loss:  1.1155292987823486   ACC:  59.375 %\n",
      "Epoch  4   1563  /  560 batch Loss:  0.6604393720626831   ACC:  81.25 %\n",
      "Epoch  4   1563  /  580 batch Loss:  0.601507306098938   ACC:  75.0 %\n",
      "Epoch  4   1563  /  600 batch Loss:  0.9349740743637085   ACC:  71.875 %\n",
      "Epoch  4   1563  /  620 batch Loss:  0.5821988582611084   ACC:  68.75 %\n",
      "Epoch  4   1563  /  640 batch Loss:  0.7513728737831116   ACC:  84.375 %\n",
      "Epoch  4   1563  /  660 batch Loss:  0.6787170767784119   ACC:  71.875 %\n",
      "Epoch  4   1563  /  680 batch Loss:  0.4727652072906494   ACC:  84.375 %\n",
      "Epoch  4   1563  /  700 batch Loss:  0.8577607274055481   ACC:  59.375 %\n",
      "Epoch  4   1563  /  720 batch Loss:  0.7814483642578125   ACC:  65.625 %\n",
      "Epoch  4   1563  /  740 batch Loss:  0.9500677585601807   ACC:  65.625 %\n",
      "Epoch  4   1563  /  760 batch Loss:  0.6889126896858215   ACC:  75.0 %\n",
      "Epoch  4   1563  /  780 batch Loss:  0.9678769707679749   ACC:  65.625 %\n",
      "Epoch  4   1563  /  800 batch Loss:  0.5441534519195557   ACC:  78.125 %\n",
      "Epoch  4   1563  /  820 batch Loss:  0.6562857627868652   ACC:  75.0 %\n",
      "Epoch  4   1563  /  840 batch Loss:  0.5945098996162415   ACC:  81.25 %\n",
      "Epoch  4   1563  /  860 batch Loss:  0.7502102255821228   ACC:  78.125 %\n",
      "Epoch  4   1563  /  880 batch Loss:  0.600950300693512   ACC:  78.125 %\n",
      "Epoch  4   1563  /  900 batch Loss:  0.7120616436004639   ACC:  84.375 %\n",
      "Epoch  4   1563  /  920 batch Loss:  0.49458688497543335   ACC:  78.125 %\n",
      "Epoch  4   1563  /  940 batch Loss:  0.6999444365501404   ACC:  75.0 %\n",
      "Epoch  4   1563  /  960 batch Loss:  0.8928899765014648   ACC:  71.875 %\n",
      "Epoch  4   1563  /  980 batch Loss:  0.8898980617523193   ACC:  75.0 %\n",
      "Epoch  4   1563  /  1000 batch Loss:  0.7804311513900757   ACC:  75.0 %\n",
      "Epoch  4   1563  /  1020 batch Loss:  0.49711471796035767   ACC:  81.25 %\n",
      "Epoch  4   1563  /  1040 batch Loss:  0.6861065030097961   ACC:  78.125 %\n",
      "Epoch  4   1563  /  1060 batch Loss:  0.8662269711494446   ACC:  68.75 %\n",
      "Epoch  4   1563  /  1080 batch Loss:  0.7640409469604492   ACC:  68.75 %\n",
      "Epoch  4   1563  /  1100 batch Loss:  0.8405234813690186   ACC:  75.0 %\n",
      "Epoch  4   1563  /  1120 batch Loss:  0.6945948600769043   ACC:  78.125 %\n",
      "Epoch  4   1563  /  1140 batch Loss:  0.8140588402748108   ACC:  68.75 %\n",
      "Epoch  4   1563  /  1160 batch Loss:  0.8580712080001831   ACC:  71.875 %\n",
      "Epoch  4   1563  /  1180 batch Loss:  0.8431560397148132   ACC:  71.875 %\n",
      "Epoch  4   1563  /  1200 batch Loss:  0.9538964629173279   ACC:  65.625 %\n",
      "Epoch  4   1563  /  1220 batch Loss:  0.6730167269706726   ACC:  78.125 %\n",
      "Epoch  4   1563  /  1240 batch Loss:  0.7144466042518616   ACC:  68.75 %\n",
      "Epoch  4   1563  /  1260 batch Loss:  1.0098518133163452   ACC:  71.875 %\n",
      "Epoch  4   1563  /  1280 batch Loss:  0.8996907472610474   ACC:  71.875 %\n",
      "Epoch  4   1563  /  1300 batch Loss:  0.669568657875061   ACC:  78.125 %\n",
      "Epoch  4   1563  /  1320 batch Loss:  0.6766574382781982   ACC:  75.0 %\n",
      "Epoch  4   1563  /  1340 batch Loss:  0.792594313621521   ACC:  71.875 %\n",
      "Epoch  4   1563  /  1360 batch Loss:  0.647863507270813   ACC:  68.75 %\n",
      "Epoch  4   1563  /  1380 batch Loss:  1.0115529298782349   ACC:  65.625 %\n",
      "Epoch  4   1563  /  1400 batch Loss:  0.743887186050415   ACC:  84.375 %\n",
      "Epoch  4   1563  /  1420 batch Loss:  0.43520984053611755   ACC:  81.25 %\n",
      "Epoch  4   1563  /  1440 batch Loss:  0.9067695736885071   ACC:  71.875 %\n",
      "Epoch  4   1563  /  1460 batch Loss:  0.6043985486030579   ACC:  75.0 %\n",
      "Epoch  4   1563  /  1480 batch Loss:  1.0782243013381958   ACC:  65.625 %\n",
      "Epoch  4   1563  /  1500 batch Loss:  0.9181197881698608   ACC:  65.625 %\n",
      "Epoch  4   1563  /  1520 batch Loss:  0.5210845470428467   ACC:  81.25 %\n",
      "Epoch  4   1563  /  1540 batch Loss:  0.8155232667922974   ACC:  78.125 %\n",
      "Epoch  4   1563  /  1560 batch Loss:  0.6906054019927979   ACC:  62.5 %\n",
      "Test acc: 73.06 \n",
      "\n",
      "Epoch  5   1563  /  20 batch Loss:  0.6450963020324707   ACC:  78.125 %\n",
      "Epoch  5   1563  /  40 batch Loss:  0.7413457632064819   ACC:  75.0 %\n",
      "Epoch  5   1563  /  60 batch Loss:  0.7610428929328918   ACC:  59.375 %\n",
      "Epoch  5   1563  /  80 batch Loss:  0.7658612728118896   ACC:  71.875 %\n",
      "Epoch  5   1563  /  100 batch Loss:  0.8043318390846252   ACC:  65.625 %\n",
      "Epoch  5   1563  /  120 batch Loss:  1.0120570659637451   ACC:  68.75 %\n",
      "Epoch  5   1563  /  140 batch Loss:  0.7822941541671753   ACC:  78.125 %\n",
      "Epoch  5   1563  /  160 batch Loss:  0.6870231628417969   ACC:  71.875 %\n",
      "Epoch  5   1563  /  180 batch Loss:  0.5725482702255249   ACC:  78.125 %\n",
      "Epoch  5   1563  /  200 batch Loss:  0.6112133860588074   ACC:  78.125 %\n",
      "Epoch  5   1563  /  220 batch Loss:  0.6833412647247314   ACC:  71.875 %\n",
      "Epoch  5   1563  /  240 batch Loss:  0.9837934970855713   ACC:  62.5 %\n",
      "Epoch  5   1563  /  260 batch Loss:  0.7397460341453552   ACC:  75.0 %\n",
      "Epoch  5   1563  /  280 batch Loss:  0.7864215970039368   ACC:  71.875 %\n",
      "Epoch  5   1563  /  300 batch Loss:  0.8952447175979614   ACC:  65.625 %\n",
      "Epoch  5   1563  /  320 batch Loss:  0.5706294775009155   ACC:  78.125 %\n",
      "Epoch  5   1563  /  340 batch Loss:  0.5806639194488525   ACC:  81.25 %\n",
      "Epoch  5   1563  /  360 batch Loss:  0.7395941615104675   ACC:  75.0 %\n",
      "Epoch  5   1563  /  380 batch Loss:  0.6143261790275574   ACC:  75.0 %\n",
      "Epoch  5   1563  /  400 batch Loss:  0.733334481716156   ACC:  65.625 %\n",
      "Epoch  5   1563  /  420 batch Loss:  1.016544222831726   ACC:  65.625 %\n",
      "Epoch  5   1563  /  440 batch Loss:  0.3970699608325958   ACC:  84.375 %\n",
      "Epoch  5   1563  /  460 batch Loss:  0.5877801179885864   ACC:  78.125 %\n",
      "Epoch  5   1563  /  480 batch Loss:  0.9467037916183472   ACC:  62.5 %\n",
      "Epoch  5   1563  /  500 batch Loss:  0.7425605654716492   ACC:  75.0 %\n",
      "Epoch  5   1563  /  520 batch Loss:  0.7079504132270813   ACC:  81.25 %\n",
      "Epoch  5   1563  /  540 batch Loss:  0.7544789910316467   ACC:  81.25 %\n",
      "Epoch  5   1563  /  560 batch Loss:  0.3794902265071869   ACC:  90.625 %\n",
      "Epoch  5   1563  /  580 batch Loss:  0.676105797290802   ACC:  78.125 %\n",
      "Epoch  5   1563  /  600 batch Loss:  0.9325008988380432   ACC:  65.625 %\n",
      "Epoch  5   1563  /  620 batch Loss:  0.457958459854126   ACC:  75.0 %\n",
      "Epoch  5   1563  /  640 batch Loss:  0.7852573990821838   ACC:  78.125 %\n",
      "Epoch  5   1563  /  660 batch Loss:  0.9177893400192261   ACC:  65.625 %\n",
      "Epoch  5   1563  /  680 batch Loss:  0.695376455783844   ACC:  71.875 %\n",
      "Epoch  5   1563  /  700 batch Loss:  1.1003074645996094   ACC:  59.375 %\n",
      "Epoch  5   1563  /  720 batch Loss:  0.6037529706954956   ACC:  84.375 %\n",
      "Epoch  5   1563  /  740 batch Loss:  0.456242173910141   ACC:  87.5 %\n",
      "Epoch  5   1563  /  760 batch Loss:  0.7090554237365723   ACC:  68.75 %\n",
      "Epoch  5   1563  /  780 batch Loss:  0.6874082684516907   ACC:  81.25 %\n",
      "Epoch  5   1563  /  800 batch Loss:  0.7799022197723389   ACC:  78.125 %\n",
      "Epoch  5   1563  /  820 batch Loss:  0.9548068642616272   ACC:  78.125 %\n",
      "Epoch  5   1563  /  840 batch Loss:  0.7197439670562744   ACC:  71.875 %\n",
      "Epoch  5   1563  /  860 batch Loss:  0.43015116453170776   ACC:  90.625 %\n",
      "Epoch  5   1563  /  880 batch Loss:  0.5930722951889038   ACC:  81.25 %\n",
      "Epoch  5   1563  /  900 batch Loss:  0.622427225112915   ACC:  71.875 %\n",
      "Epoch  5   1563  /  920 batch Loss:  0.4784082770347595   ACC:  84.375 %\n",
      "Epoch  5   1563  /  940 batch Loss:  0.5382773280143738   ACC:  81.25 %\n",
      "Epoch  5   1563  /  960 batch Loss:  0.7448757290840149   ACC:  68.75 %\n",
      "Epoch  5   1563  /  980 batch Loss:  0.853645384311676   ACC:  62.5 %\n",
      "Epoch  5   1563  /  1000 batch Loss:  0.5904073119163513   ACC:  75.0 %\n",
      "Epoch  5   1563  /  1020 batch Loss:  0.43384850025177   ACC:  87.5 %\n",
      "Epoch  5   1563  /  1040 batch Loss:  0.33730828762054443   ACC:  90.625 %\n",
      "Epoch  5   1563  /  1060 batch Loss:  1.078481912612915   ACC:  65.625 %\n",
      "Epoch  5   1563  /  1080 batch Loss:  0.5557674169540405   ACC:  78.125 %\n",
      "Epoch  5   1563  /  1100 batch Loss:  0.5584524869918823   ACC:  78.125 %\n",
      "Epoch  5   1563  /  1120 batch Loss:  0.944460391998291   ACC:  68.75 %\n",
      "Epoch  5   1563  /  1140 batch Loss:  0.5065860152244568   ACC:  78.125 %\n",
      "Epoch  5   1563  /  1160 batch Loss:  0.9633130431175232   ACC:  62.5 %\n",
      "Epoch  5   1563  /  1180 batch Loss:  0.7690325975418091   ACC:  75.0 %\n",
      "Epoch  5   1563  /  1200 batch Loss:  0.7503007054328918   ACC:  71.875 %\n",
      "Epoch  5   1563  /  1220 batch Loss:  0.6647281646728516   ACC:  75.0 %\n",
      "Epoch  5   1563  /  1240 batch Loss:  0.6677634119987488   ACC:  81.25 %\n",
      "Epoch  5   1563  /  1260 batch Loss:  0.8526830673217773   ACC:  59.375 %\n",
      "Epoch  5   1563  /  1280 batch Loss:  0.7050464749336243   ACC:  75.0 %\n",
      "Epoch  5   1563  /  1300 batch Loss:  0.36748260259628296   ACC:  87.5 %\n",
      "Epoch  5   1563  /  1320 batch Loss:  0.5948817729949951   ACC:  84.375 %\n",
      "Epoch  5   1563  /  1340 batch Loss:  0.38849952816963196   ACC:  90.625 %\n",
      "Epoch  5   1563  /  1360 batch Loss:  0.7220580577850342   ACC:  81.25 %\n",
      "Epoch  5   1563  /  1380 batch Loss:  0.6400320529937744   ACC:  71.875 %\n",
      "Epoch  5   1563  /  1400 batch Loss:  0.866698145866394   ACC:  68.75 %\n",
      "Epoch  5   1563  /  1420 batch Loss:  0.8292809724807739   ACC:  75.0 %\n",
      "Epoch  5   1563  /  1440 batch Loss:  0.4873616099357605   ACC:  71.875 %\n",
      "Epoch  5   1563  /  1460 batch Loss:  0.7768974304199219   ACC:  68.75 %\n",
      "Epoch  5   1563  /  1480 batch Loss:  0.553115725517273   ACC:  81.25 %\n",
      "Epoch  5   1563  /  1500 batch Loss:  0.7524459362030029   ACC:  65.625 %\n",
      "Epoch  5   1563  /  1520 batch Loss:  0.6268203854560852   ACC:  71.875 %\n",
      "Epoch  5   1563  /  1540 batch Loss:  0.7574882507324219   ACC:  68.75 %\n",
      "Epoch  5   1563  /  1560 batch Loss:  0.5454056262969971   ACC:  78.125 %\n",
      "Test acc: 74.46 \n",
      "\n",
      "Epoch  6   1563  /  20 batch Loss:  0.5018020272254944   ACC:  78.125 %\n",
      "Epoch  6   1563  /  40 batch Loss:  0.5567467212677002   ACC:  78.125 %\n",
      "Epoch  6   1563  /  60 batch Loss:  0.7359460592269897   ACC:  81.25 %\n",
      "Epoch  6   1563  /  80 batch Loss:  0.7256358861923218   ACC:  71.875 %\n",
      "Epoch  6   1563  /  100 batch Loss:  0.6319055557250977   ACC:  68.75 %\n",
      "Epoch  6   1563  /  120 batch Loss:  0.4306734502315521   ACC:  87.5 %\n",
      "Epoch  6   1563  /  140 batch Loss:  0.39267095923423767   ACC:  81.25 %\n",
      "Epoch  6   1563  /  160 batch Loss:  0.9256924390792847   ACC:  62.5 %\n",
      "Epoch  6   1563  /  180 batch Loss:  0.4949914216995239   ACC:  81.25 %\n",
      "Epoch  6   1563  /  200 batch Loss:  0.5688112378120422   ACC:  78.125 %\n",
      "Epoch  6   1563  /  220 batch Loss:  0.7404271960258484   ACC:  78.125 %\n",
      "Epoch  6   1563  /  240 batch Loss:  0.804839015007019   ACC:  78.125 %\n",
      "Epoch  6   1563  /  260 batch Loss:  0.8860704302787781   ACC:  78.125 %\n",
      "Epoch  6   1563  /  280 batch Loss:  0.4365708827972412   ACC:  90.625 %\n",
      "Epoch  6   1563  /  300 batch Loss:  0.7771102786064148   ACC:  68.75 %\n",
      "Epoch  6   1563  /  320 batch Loss:  0.8221153020858765   ACC:  68.75 %\n",
      "Epoch  6   1563  /  340 batch Loss:  0.5744093656539917   ACC:  84.375 %\n",
      "Epoch  6   1563  /  360 batch Loss:  0.5337528586387634   ACC:  75.0 %\n",
      "Epoch  6   1563  /  380 batch Loss:  0.5669636726379395   ACC:  81.25 %\n",
      "Epoch  6   1563  /  400 batch Loss:  0.8282700777053833   ACC:  68.75 %\n",
      "Epoch  6   1563  /  420 batch Loss:  0.6840944290161133   ACC:  71.875 %\n",
      "Epoch  6   1563  /  440 batch Loss:  0.6804832220077515   ACC:  78.125 %\n",
      "Epoch  6   1563  /  460 batch Loss:  0.575133740901947   ACC:  81.25 %\n",
      "Epoch  6   1563  /  480 batch Loss:  0.47361430525779724   ACC:  84.375 %\n",
      "Epoch  6   1563  /  500 batch Loss:  0.6948833465576172   ACC:  78.125 %\n",
      "Epoch  6   1563  /  520 batch Loss:  0.45401912927627563   ACC:  87.5 %\n",
      "Epoch  6   1563  /  540 batch Loss:  0.9118775129318237   ACC:  62.5 %\n",
      "Epoch  6   1563  /  560 batch Loss:  0.7066722512245178   ACC:  71.875 %\n",
      "Epoch  6   1563  /  580 batch Loss:  0.5677523016929626   ACC:  71.875 %\n",
      "Epoch  6   1563  /  600 batch Loss:  0.4891447424888611   ACC:  84.375 %\n",
      "Epoch  6   1563  /  620 batch Loss:  0.24435849487781525   ACC:  93.75 %\n",
      "Epoch  6   1563  /  640 batch Loss:  0.7693259716033936   ACC:  68.75 %\n",
      "Epoch  6   1563  /  660 batch Loss:  0.4690130949020386   ACC:  81.25 %\n",
      "Epoch  6   1563  /  680 batch Loss:  0.9360902309417725   ACC:  68.75 %\n",
      "Epoch  6   1563  /  700 batch Loss:  0.4492839276790619   ACC:  87.5 %\n",
      "Epoch  6   1563  /  720 batch Loss:  0.9209463596343994   ACC:  68.75 %\n",
      "Epoch  6   1563  /  740 batch Loss:  0.5843416452407837   ACC:  81.25 %\n",
      "Epoch  6   1563  /  760 batch Loss:  0.41886162757873535   ACC:  84.375 %\n",
      "Epoch  6   1563  /  780 batch Loss:  0.7684268951416016   ACC:  71.875 %\n",
      "Epoch  6   1563  /  800 batch Loss:  0.5029785633087158   ACC:  84.375 %\n",
      "Epoch  6   1563  /  820 batch Loss:  0.5947986245155334   ACC:  78.125 %\n",
      "Epoch  6   1563  /  840 batch Loss:  0.5366614460945129   ACC:  81.25 %\n",
      "Epoch  6   1563  /  860 batch Loss:  0.8251776099205017   ACC:  65.625 %\n",
      "Epoch  6   1563  /  880 batch Loss:  0.32294872403144836   ACC:  84.375 %\n",
      "Epoch  6   1563  /  900 batch Loss:  0.3273414671421051   ACC:  87.5 %\n",
      "Epoch  6   1563  /  920 batch Loss:  0.3733660578727722   ACC:  87.5 %\n",
      "Epoch  6   1563  /  940 batch Loss:  0.459984689950943   ACC:  84.375 %\n",
      "Epoch  6   1563  /  960 batch Loss:  0.968453049659729   ACC:  65.625 %\n",
      "Epoch  6   1563  /  980 batch Loss:  0.8825078010559082   ACC:  65.625 %\n",
      "Epoch  6   1563  /  1000 batch Loss:  0.5463090538978577   ACC:  81.25 %\n",
      "Epoch  6   1563  /  1020 batch Loss:  0.8518611192703247   ACC:  71.875 %\n",
      "Epoch  6   1563  /  1040 batch Loss:  0.6453692317008972   ACC:  75.0 %\n",
      "Epoch  6   1563  /  1060 batch Loss:  0.44430282711982727   ACC:  84.375 %\n",
      "Epoch  6   1563  /  1080 batch Loss:  0.43456703424453735   ACC:  84.375 %\n",
      "Epoch  6   1563  /  1100 batch Loss:  0.7166898846626282   ACC:  75.0 %\n",
      "Epoch  6   1563  /  1120 batch Loss:  0.5738844871520996   ACC:  75.0 %\n",
      "Epoch  6   1563  /  1140 batch Loss:  0.726607084274292   ACC:  75.0 %\n",
      "Epoch  6   1563  /  1160 batch Loss:  0.47054779529571533   ACC:  87.5 %\n",
      "Epoch  6   1563  /  1180 batch Loss:  0.5460056066513062   ACC:  75.0 %\n",
      "Epoch  6   1563  /  1200 batch Loss:  1.139885425567627   ACC:  75.0 %\n",
      "Epoch  6   1563  /  1220 batch Loss:  0.9264808297157288   ACC:  59.375 %\n",
      "Epoch  6   1563  /  1240 batch Loss:  0.6719561219215393   ACC:  81.25 %\n",
      "Epoch  6   1563  /  1260 batch Loss:  0.35624101758003235   ACC:  84.375 %\n",
      "Epoch  6   1563  /  1280 batch Loss:  0.46914464235305786   ACC:  81.25 %\n",
      "Epoch  6   1563  /  1300 batch Loss:  0.6271063685417175   ACC:  75.0 %\n",
      "Epoch  6   1563  /  1320 batch Loss:  0.8190694451332092   ACC:  75.0 %\n",
      "Epoch  6   1563  /  1340 batch Loss:  0.7921618223190308   ACC:  71.875 %\n",
      "Epoch  6   1563  /  1360 batch Loss:  0.4476501941680908   ACC:  84.375 %\n",
      "Epoch  6   1563  /  1380 batch Loss:  0.8416593074798584   ACC:  78.125 %\n",
      "Epoch  6   1563  /  1400 batch Loss:  0.43343421816825867   ACC:  84.375 %\n",
      "Epoch  6   1563  /  1420 batch Loss:  0.5675484538078308   ACC:  81.25 %\n",
      "Epoch  6   1563  /  1440 batch Loss:  0.5582505464553833   ACC:  71.875 %\n",
      "Epoch  6   1563  /  1460 batch Loss:  0.43941521644592285   ACC:  87.5 %\n",
      "Epoch  6   1563  /  1480 batch Loss:  0.41283804178237915   ACC:  81.25 %\n",
      "Epoch  6   1563  /  1500 batch Loss:  0.630769670009613   ACC:  81.25 %\n",
      "Epoch  6   1563  /  1520 batch Loss:  0.349715918302536   ACC:  93.75 %\n",
      "Epoch  6   1563  /  1540 batch Loss:  0.3696853816509247   ACC:  90.625 %\n",
      "Epoch  6   1563  /  1560 batch Loss:  0.49317118525505066   ACC:  75.0 %\n",
      "Test acc: 75.98 \n",
      "\n",
      "Epoch  7   1563  /  20 batch Loss:  0.4466570019721985   ACC:  78.125 %\n",
      "Epoch  7   1563  /  40 batch Loss:  0.5326820611953735   ACC:  84.375 %\n",
      "Epoch  7   1563  /  60 batch Loss:  0.5320126414299011   ACC:  87.5 %\n",
      "Epoch  7   1563  /  80 batch Loss:  0.3939245045185089   ACC:  93.75 %\n",
      "Epoch  7   1563  /  100 batch Loss:  0.29878467321395874   ACC:  87.5 %\n",
      "Epoch  7   1563  /  120 batch Loss:  0.5726274251937866   ACC:  81.25 %\n",
      "Epoch  7   1563  /  140 batch Loss:  0.6270782947540283   ACC:  78.125 %\n",
      "Epoch  7   1563  /  160 batch Loss:  0.44609805941581726   ACC:  84.375 %\n",
      "Epoch  7   1563  /  180 batch Loss:  0.411478728055954   ACC:  87.5 %\n",
      "Epoch  7   1563  /  200 batch Loss:  0.6020773649215698   ACC:  78.125 %\n",
      "Epoch  7   1563  /  220 batch Loss:  0.7912602424621582   ACC:  65.625 %\n",
      "Epoch  7   1563  /  240 batch Loss:  0.5865516066551208   ACC:  78.125 %\n",
      "Epoch  7   1563  /  260 batch Loss:  0.5240522027015686   ACC:  84.375 %\n",
      "Epoch  7   1563  /  280 batch Loss:  0.44648972153663635   ACC:  81.25 %\n",
      "Epoch  7   1563  /  300 batch Loss:  0.8233595490455627   ACC:  62.5 %\n",
      "Epoch  7   1563  /  320 batch Loss:  0.49831661581993103   ACC:  81.25 %\n",
      "Epoch  7   1563  /  340 batch Loss:  0.30239152908325195   ACC:  90.625 %\n",
      "Epoch  7   1563  /  360 batch Loss:  0.8079404234886169   ACC:  71.875 %\n",
      "Epoch  7   1563  /  380 batch Loss:  0.8483201265335083   ACC:  75.0 %\n",
      "Epoch  7   1563  /  400 batch Loss:  0.43582668900489807   ACC:  84.375 %\n",
      "Epoch  7   1563  /  420 batch Loss:  0.1810852438211441   ACC:  93.75 %\n",
      "Epoch  7   1563  /  440 batch Loss:  0.44334620237350464   ACC:  84.375 %\n",
      "Epoch  7   1563  /  460 batch Loss:  0.35701245069503784   ACC:  87.5 %\n",
      "Epoch  7   1563  /  480 batch Loss:  0.3456738293170929   ACC:  87.5 %\n",
      "Epoch  7   1563  /  500 batch Loss:  0.5367332696914673   ACC:  84.375 %\n",
      "Epoch  7   1563  /  520 batch Loss:  0.6203768849372864   ACC:  81.25 %\n",
      "Epoch  7   1563  /  540 batch Loss:  0.5952510833740234   ACC:  78.125 %\n",
      "Epoch  7   1563  /  560 batch Loss:  0.35324326157569885   ACC:  87.5 %\n",
      "Epoch  7   1563  /  580 batch Loss:  0.762202262878418   ACC:  75.0 %\n",
      "Epoch  7   1563  /  600 batch Loss:  0.5204401016235352   ACC:  78.125 %\n",
      "Epoch  7   1563  /  620 batch Loss:  0.37215107679367065   ACC:  87.5 %\n",
      "Epoch  7   1563  /  640 batch Loss:  0.6694916486740112   ACC:  71.875 %\n",
      "Epoch  7   1563  /  660 batch Loss:  0.4953002631664276   ACC:  81.25 %\n",
      "Epoch  7   1563  /  680 batch Loss:  0.7331894040107727   ACC:  75.0 %\n",
      "Epoch  7   1563  /  700 batch Loss:  0.6310023665428162   ACC:  78.125 %\n",
      "Epoch  7   1563  /  720 batch Loss:  0.28562936186790466   ACC:  90.625 %\n",
      "Epoch  7   1563  /  740 batch Loss:  0.49036815762519836   ACC:  84.375 %\n",
      "Epoch  7   1563  /  760 batch Loss:  0.5671801567077637   ACC:  78.125 %\n",
      "Epoch  7   1563  /  780 batch Loss:  0.7266785502433777   ACC:  75.0 %\n",
      "Epoch  7   1563  /  800 batch Loss:  0.4921574592590332   ACC:  90.625 %\n",
      "Epoch  7   1563  /  820 batch Loss:  0.32433196902275085   ACC:  90.625 %\n",
      "Epoch  7   1563  /  840 batch Loss:  0.5172531008720398   ACC:  75.0 %\n",
      "Epoch  7   1563  /  860 batch Loss:  0.42487841844558716   ACC:  84.375 %\n",
      "Epoch  7   1563  /  880 batch Loss:  0.4427170753479004   ACC:  87.5 %\n",
      "Epoch  7   1563  /  900 batch Loss:  0.20551010966300964   ACC:  93.75 %\n",
      "Epoch  7   1563  /  920 batch Loss:  0.7668778300285339   ACC:  78.125 %\n",
      "Epoch  7   1563  /  940 batch Loss:  0.5776223540306091   ACC:  75.0 %\n",
      "Epoch  7   1563  /  960 batch Loss:  0.6673498749732971   ACC:  78.125 %\n",
      "Epoch  7   1563  /  980 batch Loss:  0.5812280178070068   ACC:  75.0 %\n",
      "Epoch  7   1563  /  1000 batch Loss:  0.3286505341529846   ACC:  87.5 %\n",
      "Epoch  7   1563  /  1020 batch Loss:  0.5187169909477234   ACC:  81.25 %\n",
      "Epoch  7   1563  /  1040 batch Loss:  0.29802405834198   ACC:  87.5 %\n",
      "Epoch  7   1563  /  1060 batch Loss:  0.5861058235168457   ACC:  75.0 %\n",
      "Epoch  7   1563  /  1080 batch Loss:  0.7169926762580872   ACC:  78.125 %\n",
      "Epoch  7   1563  /  1100 batch Loss:  0.6136388182640076   ACC:  71.875 %\n",
      "Epoch  7   1563  /  1120 batch Loss:  0.5443118810653687   ACC:  84.375 %\n",
      "Epoch  7   1563  /  1140 batch Loss:  0.7962623238563538   ACC:  78.125 %\n",
      "Epoch  7   1563  /  1160 batch Loss:  0.4895602762699127   ACC:  84.375 %\n",
      "Epoch  7   1563  /  1180 batch Loss:  0.5508870482444763   ACC:  75.0 %\n",
      "Epoch  7   1563  /  1200 batch Loss:  0.5441198945045471   ACC:  81.25 %\n",
      "Epoch  7   1563  /  1220 batch Loss:  0.6273150444030762   ACC:  81.25 %\n",
      "Epoch  7   1563  /  1240 batch Loss:  0.28044670820236206   ACC:  90.625 %\n",
      "Epoch  7   1563  /  1260 batch Loss:  0.5566035509109497   ACC:  75.0 %\n",
      "Epoch  7   1563  /  1280 batch Loss:  0.2653186023235321   ACC:  87.5 %\n",
      "Epoch  7   1563  /  1300 batch Loss:  0.39566218852996826   ACC:  90.625 %\n",
      "Epoch  7   1563  /  1320 batch Loss:  0.8709924221038818   ACC:  75.0 %\n",
      "Epoch  7   1563  /  1340 batch Loss:  0.45025449991226196   ACC:  87.5 %\n",
      "Epoch  7   1563  /  1360 batch Loss:  0.643984317779541   ACC:  68.75 %\n",
      "Epoch  7   1563  /  1380 batch Loss:  0.48476582765579224   ACC:  78.125 %\n",
      "Epoch  7   1563  /  1400 batch Loss:  0.649297297000885   ACC:  71.875 %\n",
      "Epoch  7   1563  /  1420 batch Loss:  0.6544880270957947   ACC:  84.375 %\n",
      "Epoch  7   1563  /  1440 batch Loss:  0.5588783621788025   ACC:  84.375 %\n",
      "Epoch  7   1563  /  1460 batch Loss:  0.5565553307533264   ACC:  78.125 %\n",
      "Epoch  7   1563  /  1480 batch Loss:  0.7846918106079102   ACC:  78.125 %\n",
      "Epoch  7   1563  /  1500 batch Loss:  0.6252486109733582   ACC:  75.0 %\n",
      "Epoch  7   1563  /  1520 batch Loss:  0.4005175828933716   ACC:  78.125 %\n",
      "Epoch  7   1563  /  1540 batch Loss:  0.49413788318634033   ACC:  84.375 %\n",
      "Epoch  7   1563  /  1560 batch Loss:  0.47109609842300415   ACC:  84.375 %\n",
      "Test acc: 77.46 \n",
      "\n",
      "Epoch  8   1563  /  20 batch Loss:  0.2699738144874573   ACC:  96.875 %\n",
      "Epoch  8   1563  /  40 batch Loss:  0.42418092489242554   ACC:  87.5 %\n",
      "Epoch  8   1563  /  60 batch Loss:  0.5854555368423462   ACC:  84.375 %\n",
      "Epoch  8   1563  /  80 batch Loss:  0.6720765233039856   ACC:  75.0 %\n",
      "Epoch  8   1563  /  100 batch Loss:  0.3050324320793152   ACC:  90.625 %\n",
      "Epoch  8   1563  /  120 batch Loss:  0.2551218271255493   ACC:  90.625 %\n",
      "Epoch  8   1563  /  140 batch Loss:  0.7589749693870544   ACC:  68.75 %\n",
      "Epoch  8   1563  /  160 batch Loss:  0.4738169014453888   ACC:  84.375 %\n",
      "Epoch  8   1563  /  180 batch Loss:  0.3161679804325104   ACC:  90.625 %\n",
      "Epoch  8   1563  /  200 batch Loss:  0.4445721507072449   ACC:  84.375 %\n",
      "Epoch  8   1563  /  220 batch Loss:  0.22335661947727203   ACC:  93.75 %\n",
      "Epoch  8   1563  /  240 batch Loss:  0.5183582901954651   ACC:  78.125 %\n",
      "Epoch  8   1563  /  260 batch Loss:  0.487714022397995   ACC:  87.5 %\n",
      "Epoch  8   1563  /  280 batch Loss:  0.6272129416465759   ACC:  87.5 %\n",
      "Epoch  8   1563  /  300 batch Loss:  0.512611448764801   ACC:  81.25 %\n",
      "Epoch  8   1563  /  320 batch Loss:  0.38603976368904114   ACC:  87.5 %\n",
      "Epoch  8   1563  /  340 batch Loss:  0.6325133442878723   ACC:  78.125 %\n",
      "Epoch  8   1563  /  360 batch Loss:  0.629745364189148   ACC:  81.25 %\n",
      "Epoch  8   1563  /  380 batch Loss:  0.19834764301776886   ACC:  93.75 %\n",
      "Epoch  8   1563  /  400 batch Loss:  0.6372805833816528   ACC:  81.25 %\n",
      "Epoch  8   1563  /  420 batch Loss:  0.6325867176055908   ACC:  81.25 %\n",
      "Epoch  8   1563  /  440 batch Loss:  0.6042973399162292   ACC:  84.375 %\n",
      "Epoch  8   1563  /  460 batch Loss:  0.6013288497924805   ACC:  75.0 %\n",
      "Epoch  8   1563  /  480 batch Loss:  0.43218451738357544   ACC:  81.25 %\n",
      "Epoch  8   1563  /  500 batch Loss:  0.5951718091964722   ACC:  75.0 %\n",
      "Epoch  8   1563  /  520 batch Loss:  0.4430472254753113   ACC:  81.25 %\n",
      "Epoch  8   1563  /  540 batch Loss:  0.34470808506011963   ACC:  93.75 %\n",
      "Epoch  8   1563  /  560 batch Loss:  0.44909965991973877   ACC:  87.5 %\n",
      "Epoch  8   1563  /  580 batch Loss:  0.2838529944419861   ACC:  90.625 %\n",
      "Epoch  8   1563  /  600 batch Loss:  0.3424219489097595   ACC:  87.5 %\n",
      "Epoch  8   1563  /  620 batch Loss:  0.5079460740089417   ACC:  84.375 %\n",
      "Epoch  8   1563  /  640 batch Loss:  0.19794906675815582   ACC:  93.75 %\n",
      "Epoch  8   1563  /  660 batch Loss:  0.4149174988269806   ACC:  84.375 %\n",
      "Epoch  8   1563  /  680 batch Loss:  0.28305739164352417   ACC:  96.875 %\n",
      "Epoch  8   1563  /  700 batch Loss:  0.6601050496101379   ACC:  81.25 %\n",
      "Epoch  8   1563  /  720 batch Loss:  0.5440264940261841   ACC:  75.0 %\n",
      "Epoch  8   1563  /  740 batch Loss:  0.3662280738353729   ACC:  87.5 %\n",
      "Epoch  8   1563  /  760 batch Loss:  0.5020887851715088   ACC:  87.5 %\n",
      "Epoch  8   1563  /  780 batch Loss:  0.9811128377914429   ACC:  71.875 %\n",
      "Epoch  8   1563  /  800 batch Loss:  0.38256126642227173   ACC:  87.5 %\n",
      "Epoch  8   1563  /  820 batch Loss:  0.31208565831184387   ACC:  90.625 %\n",
      "Epoch  8   1563  /  840 batch Loss:  0.3704686462879181   ACC:  87.5 %\n",
      "Epoch  8   1563  /  860 batch Loss:  0.7173438668251038   ACC:  75.0 %\n",
      "Epoch  8   1563  /  880 batch Loss:  0.23363324999809265   ACC:  90.625 %\n",
      "Epoch  8   1563  /  900 batch Loss:  0.681648313999176   ACC:  78.125 %\n",
      "Epoch  8   1563  /  920 batch Loss:  0.5414532423019409   ACC:  84.375 %\n",
      "Epoch  8   1563  /  940 batch Loss:  0.5539311766624451   ACC:  87.5 %\n",
      "Epoch  8   1563  /  960 batch Loss:  0.3364560306072235   ACC:  90.625 %\n",
      "Epoch  8   1563  /  980 batch Loss:  0.7091581225395203   ACC:  71.875 %\n",
      "Epoch  8   1563  /  1000 batch Loss:  0.5400010943412781   ACC:  84.375 %\n",
      "Epoch  8   1563  /  1020 batch Loss:  0.7754968404769897   ACC:  75.0 %\n",
      "Epoch  8   1563  /  1040 batch Loss:  0.2297103852033615   ACC:  96.875 %\n",
      "Epoch  8   1563  /  1060 batch Loss:  1.0359143018722534   ACC:  68.75 %\n",
      "Epoch  8   1563  /  1080 batch Loss:  0.43685382604599   ACC:  87.5 %\n",
      "Epoch  8   1563  /  1100 batch Loss:  0.22904455661773682   ACC:  93.75 %\n",
      "Epoch  8   1563  /  1120 batch Loss:  0.7767528891563416   ACC:  68.75 %\n",
      "Epoch  8   1563  /  1140 batch Loss:  0.4930901527404785   ACC:  87.5 %\n",
      "Epoch  8   1563  /  1160 batch Loss:  0.36901038885116577   ACC:  87.5 %\n",
      "Epoch  8   1563  /  1180 batch Loss:  0.5548349022865295   ACC:  78.125 %\n",
      "Epoch  8   1563  /  1200 batch Loss:  0.6314653158187866   ACC:  78.125 %\n",
      "Epoch  8   1563  /  1220 batch Loss:  0.39519983530044556   ACC:  87.5 %\n",
      "Epoch  8   1563  /  1240 batch Loss:  0.4025132358074188   ACC:  90.625 %\n",
      "Epoch  8   1563  /  1260 batch Loss:  0.5925999283790588   ACC:  87.5 %\n",
      "Epoch  8   1563  /  1280 batch Loss:  0.8540546894073486   ACC:  68.75 %\n",
      "Epoch  8   1563  /  1300 batch Loss:  0.35364919900894165   ACC:  87.5 %\n",
      "Epoch  8   1563  /  1320 batch Loss:  0.3216862678527832   ACC:  87.5 %\n",
      "Epoch  8   1563  /  1340 batch Loss:  0.5353319048881531   ACC:  78.125 %\n",
      "Epoch  8   1563  /  1360 batch Loss:  0.4895579218864441   ACC:  81.25 %\n",
      "Epoch  8   1563  /  1380 batch Loss:  0.39496251940727234   ACC:  87.5 %\n",
      "Epoch  8   1563  /  1400 batch Loss:  0.37975627183914185   ACC:  84.375 %\n",
      "Epoch  8   1563  /  1420 batch Loss:  0.4269828796386719   ACC:  84.375 %\n",
      "Epoch  8   1563  /  1440 batch Loss:  0.22217969596385956   ACC:  96.875 %\n",
      "Epoch  8   1563  /  1460 batch Loss:  0.44227084517478943   ACC:  84.375 %\n",
      "Epoch  8   1563  /  1480 batch Loss:  0.3422386646270752   ACC:  87.5 %\n",
      "Epoch  8   1563  /  1500 batch Loss:  0.46107378602027893   ACC:  90.625 %\n",
      "Epoch  8   1563  /  1520 batch Loss:  0.29842859506607056   ACC:  93.75 %\n",
      "Epoch  8   1563  /  1540 batch Loss:  0.7238088846206665   ACC:  71.875 %\n",
      "Epoch  8   1563  /  1560 batch Loss:  0.6674222946166992   ACC:  81.25 %\n",
      "Test acc: 79.82 \n",
      "\n",
      "Epoch  9   1563  /  20 batch Loss:  0.4993482530117035   ACC:  84.375 %\n",
      "Epoch  9   1563  /  40 batch Loss:  0.3696952164173126   ACC:  84.375 %\n",
      "Epoch  9   1563  /  60 batch Loss:  0.34499385952949524   ACC:  84.375 %\n",
      "Epoch  9   1563  /  80 batch Loss:  0.30259326100349426   ACC:  90.625 %\n",
      "Epoch  9   1563  /  100 batch Loss:  0.5172663331031799   ACC:  84.375 %\n",
      "Epoch  9   1563  /  120 batch Loss:  0.47727692127227783   ACC:  84.375 %\n",
      "Epoch  9   1563  /  140 batch Loss:  0.3906334936618805   ACC:  90.625 %\n",
      "Epoch  9   1563  /  160 batch Loss:  0.3660695254802704   ACC:  81.25 %\n",
      "Epoch  9   1563  /  180 batch Loss:  0.4202422797679901   ACC:  87.5 %\n",
      "Epoch  9   1563  /  200 batch Loss:  0.5693722367286682   ACC:  78.125 %\n",
      "Epoch  9   1563  /  220 batch Loss:  0.20313404500484467   ACC:  93.75 %\n",
      "Epoch  9   1563  /  240 batch Loss:  0.18238729238510132   ACC:  93.75 %\n",
      "Epoch  9   1563  /  260 batch Loss:  0.37271183729171753   ACC:  87.5 %\n",
      "Epoch  9   1563  /  280 batch Loss:  0.7221784591674805   ACC:  75.0 %\n",
      "Epoch  9   1563  /  300 batch Loss:  0.7800279855728149   ACC:  71.875 %\n",
      "Epoch  9   1563  /  320 batch Loss:  0.4283801019191742   ACC:  84.375 %\n",
      "Epoch  9   1563  /  340 batch Loss:  0.5546550750732422   ACC:  78.125 %\n",
      "Epoch  9   1563  /  360 batch Loss:  0.39746376872062683   ACC:  87.5 %\n",
      "Epoch  9   1563  /  380 batch Loss:  0.28919628262519836   ACC:  90.625 %\n",
      "Epoch  9   1563  /  400 batch Loss:  0.6193029880523682   ACC:  75.0 %\n",
      "Epoch  9   1563  /  420 batch Loss:  0.5134595036506653   ACC:  84.375 %\n",
      "Epoch  9   1563  /  440 batch Loss:  0.5714465379714966   ACC:  78.125 %\n",
      "Epoch  9   1563  /  460 batch Loss:  0.5870469212532043   ACC:  84.375 %\n",
      "Epoch  9   1563  /  480 batch Loss:  0.15762272477149963   ACC:  96.875 %\n",
      "Epoch  9   1563  /  500 batch Loss:  0.23112238943576813   ACC:  90.625 %\n",
      "Epoch  9   1563  /  520 batch Loss:  0.3023150861263275   ACC:  87.5 %\n",
      "Epoch  9   1563  /  540 batch Loss:  0.49474018812179565   ACC:  87.5 %\n",
      "Epoch  9   1563  /  560 batch Loss:  0.29502925276756287   ACC:  90.625 %\n",
      "Epoch  9   1563  /  580 batch Loss:  0.6528910398483276   ACC:  71.875 %\n",
      "Epoch  9   1563  /  600 batch Loss:  0.5256404876708984   ACC:  75.0 %\n",
      "Epoch  9   1563  /  620 batch Loss:  0.5812234282493591   ACC:  75.0 %\n",
      "Epoch  9   1563  /  640 batch Loss:  0.596367359161377   ACC:  75.0 %\n",
      "Epoch  9   1563  /  660 batch Loss:  0.417984277009964   ACC:  81.25 %\n",
      "Epoch  9   1563  /  680 batch Loss:  0.6216311454772949   ACC:  81.25 %\n",
      "Epoch  9   1563  /  700 batch Loss:  0.3862971067428589   ACC:  87.5 %\n",
      "Epoch  9   1563  /  720 batch Loss:  0.29620739817619324   ACC:  93.75 %\n",
      "Epoch  9   1563  /  740 batch Loss:  0.40602216124534607   ACC:  84.375 %\n",
      "Epoch  9   1563  /  760 batch Loss:  0.6211572289466858   ACC:  84.375 %\n",
      "Epoch  9   1563  /  780 batch Loss:  0.15641334652900696   ACC:  93.75 %\n",
      "Epoch  9   1563  /  800 batch Loss:  0.529110848903656   ACC:  81.25 %\n",
      "Epoch  9   1563  /  820 batch Loss:  0.5157649517059326   ACC:  84.375 %\n",
      "Epoch  9   1563  /  840 batch Loss:  0.312506765127182   ACC:  84.375 %\n",
      "Epoch  9   1563  /  860 batch Loss:  0.3194577097892761   ACC:  87.5 %\n",
      "Epoch  9   1563  /  880 batch Loss:  0.24411915242671967   ACC:  90.625 %\n",
      "Epoch  9   1563  /  900 batch Loss:  0.2850562632083893   ACC:  90.625 %\n",
      "Epoch  9   1563  /  920 batch Loss:  0.36359715461730957   ACC:  84.375 %\n",
      "Epoch  9   1563  /  940 batch Loss:  0.6899420619010925   ACC:  75.0 %\n",
      "Epoch  9   1563  /  960 batch Loss:  0.6060194373130798   ACC:  84.375 %\n",
      "Epoch  9   1563  /  980 batch Loss:  0.6025072336196899   ACC:  84.375 %\n",
      "Epoch  9   1563  /  1000 batch Loss:  0.36106637120246887   ACC:  87.5 %\n",
      "Epoch  9   1563  /  1020 batch Loss:  0.3302801549434662   ACC:  90.625 %\n",
      "Epoch  9   1563  /  1040 batch Loss:  0.9785915613174438   ACC:  75.0 %\n",
      "Epoch  9   1563  /  1060 batch Loss:  0.31971684098243713   ACC:  87.5 %\n",
      "Epoch  9   1563  /  1080 batch Loss:  0.3107519745826721   ACC:  90.625 %\n",
      "Epoch  9   1563  /  1100 batch Loss:  0.4164043366909027   ACC:  87.5 %\n",
      "Epoch  9   1563  /  1120 batch Loss:  0.4573723375797272   ACC:  84.375 %\n",
      "Epoch  9   1563  /  1140 batch Loss:  0.3396396040916443   ACC:  93.75 %\n",
      "Epoch  9   1563  /  1160 batch Loss:  0.5870750546455383   ACC:  81.25 %\n",
      "Epoch  9   1563  /  1180 batch Loss:  0.5577031970024109   ACC:  75.0 %\n",
      "Epoch  9   1563  /  1200 batch Loss:  0.4689936637878418   ACC:  90.625 %\n",
      "Epoch  9   1563  /  1220 batch Loss:  0.6762006878852844   ACC:  78.125 %\n",
      "Epoch  9   1563  /  1240 batch Loss:  0.24139386415481567   ACC:  87.5 %\n",
      "Epoch  9   1563  /  1260 batch Loss:  0.6981280446052551   ACC:  75.0 %\n",
      "Epoch  9   1563  /  1280 batch Loss:  0.21499863266944885   ACC:  93.75 %\n",
      "Epoch  9   1563  /  1300 batch Loss:  0.4217272996902466   ACC:  84.375 %\n",
      "Epoch  9   1563  /  1320 batch Loss:  0.31915125250816345   ACC:  87.5 %\n",
      "Epoch  9   1563  /  1340 batch Loss:  0.3715112507343292   ACC:  87.5 %\n",
      "Epoch  9   1563  /  1360 batch Loss:  0.5693649649620056   ACC:  71.875 %\n",
      "Epoch  9   1563  /  1380 batch Loss:  0.6566681861877441   ACC:  71.875 %\n",
      "Epoch  9   1563  /  1400 batch Loss:  0.5346399545669556   ACC:  87.5 %\n",
      "Epoch  9   1563  /  1420 batch Loss:  0.15786007046699524   ACC:  96.875 %\n",
      "Epoch  9   1563  /  1440 batch Loss:  0.18837569653987885   ACC:  93.75 %\n",
      "Epoch  9   1563  /  1460 batch Loss:  0.5347028970718384   ACC:  81.25 %\n",
      "Epoch  9   1563  /  1480 batch Loss:  0.27285853028297424   ACC:  93.75 %\n",
      "Epoch  9   1563  /  1500 batch Loss:  0.5145152807235718   ACC:  87.5 %\n",
      "Epoch  9   1563  /  1520 batch Loss:  0.2762221395969391   ACC:  90.625 %\n",
      "Epoch  9   1563  /  1540 batch Loss:  0.11249787360429764   ACC:  96.875 %\n",
      "Epoch  9   1563  /  1560 batch Loss:  0.5695960521697998   ACC:  84.375 %\n",
      "Test acc: 79.08 \n",
      "\n",
      "Epoch  10   1563  /  20 batch Loss:  0.5290012359619141   ACC:  90.625 %\n",
      "Epoch  10   1563  /  40 batch Loss:  0.2664127051830292   ACC:  90.625 %\n",
      "Epoch  10   1563  /  60 batch Loss:  0.3409946858882904   ACC:  87.5 %\n",
      "Epoch  10   1563  /  80 batch Loss:  0.5620236396789551   ACC:  78.125 %\n",
      "Epoch  10   1563  /  100 batch Loss:  0.15502451360225677   ACC:  93.75 %\n",
      "Epoch  10   1563  /  120 batch Loss:  0.2862773835659027   ACC:  84.375 %\n",
      "Epoch  10   1563  /  140 batch Loss:  0.25917237997055054   ACC:  93.75 %\n",
      "Epoch  10   1563  /  160 batch Loss:  0.35029447078704834   ACC:  87.5 %\n",
      "Epoch  10   1563  /  180 batch Loss:  0.3150768280029297   ACC:  87.5 %\n",
      "Epoch  10   1563  /  200 batch Loss:  0.3615695536136627   ACC:  87.5 %\n",
      "Epoch  10   1563  /  220 batch Loss:  0.26894837617874146   ACC:  90.625 %\n",
      "Epoch  10   1563  /  240 batch Loss:  0.5978641510009766   ACC:  71.875 %\n",
      "Epoch  10   1563  /  260 batch Loss:  0.4924269914627075   ACC:  81.25 %\n",
      "Epoch  10   1563  /  280 batch Loss:  0.7102426886558533   ACC:  81.25 %\n",
      "Epoch  10   1563  /  300 batch Loss:  0.36376404762268066   ACC:  84.375 %\n",
      "Epoch  10   1563  /  320 batch Loss:  0.4818249046802521   ACC:  81.25 %\n",
      "Epoch  10   1563  /  340 batch Loss:  0.21266582608222961   ACC:  93.75 %\n",
      "Epoch  10   1563  /  360 batch Loss:  0.3960370123386383   ACC:  87.5 %\n",
      "Epoch  10   1563  /  380 batch Loss:  0.3393988609313965   ACC:  93.75 %\n",
      "Epoch  10   1563  /  400 batch Loss:  0.4379822611808777   ACC:  84.375 %\n",
      "Epoch  10   1563  /  420 batch Loss:  0.5388633608818054   ACC:  81.25 %\n",
      "Epoch  10   1563  /  440 batch Loss:  0.5762311220169067   ACC:  81.25 %\n",
      "Epoch  10   1563  /  460 batch Loss:  0.3782818615436554   ACC:  84.375 %\n",
      "Epoch  10   1563  /  480 batch Loss:  0.2248009741306305   ACC:  96.875 %\n",
      "Epoch  10   1563  /  500 batch Loss:  0.2518508732318878   ACC:  90.625 %\n",
      "Epoch  10   1563  /  520 batch Loss:  0.24708715081214905   ACC:  90.625 %\n",
      "Epoch  10   1563  /  540 batch Loss:  0.5083619356155396   ACC:  81.25 %\n",
      "Epoch  10   1563  /  560 batch Loss:  0.23183539509773254   ACC:  87.5 %\n",
      "Epoch  10   1563  /  580 batch Loss:  0.2799558937549591   ACC:  90.625 %\n",
      "Epoch  10   1563  /  600 batch Loss:  0.25780460238456726   ACC:  90.625 %\n",
      "Epoch  10   1563  /  620 batch Loss:  0.4850660562515259   ACC:  81.25 %\n",
      "Epoch  10   1563  /  640 batch Loss:  0.46173328161239624   ACC:  84.375 %\n",
      "Epoch  10   1563  /  660 batch Loss:  0.29846009612083435   ACC:  90.625 %\n",
      "Epoch  10   1563  /  680 batch Loss:  0.5025486350059509   ACC:  78.125 %\n",
      "Epoch  10   1563  /  700 batch Loss:  0.2935323119163513   ACC:  90.625 %\n",
      "Epoch  10   1563  /  720 batch Loss:  0.3698304295539856   ACC:  81.25 %\n",
      "Epoch  10   1563  /  740 batch Loss:  0.4616616666316986   ACC:  81.25 %\n",
      "Epoch  10   1563  /  760 batch Loss:  0.26351168751716614   ACC:  90.625 %\n",
      "Epoch  10   1563  /  780 batch Loss:  0.3457360863685608   ACC:  87.5 %\n",
      "Epoch  10   1563  /  800 batch Loss:  0.23121333122253418   ACC:  93.75 %\n",
      "Epoch  10   1563  /  820 batch Loss:  0.45726874470710754   ACC:  84.375 %\n",
      "Epoch  10   1563  /  840 batch Loss:  0.16866283118724823   ACC:  93.75 %\n",
      "Epoch  10   1563  /  860 batch Loss:  0.5043720602989197   ACC:  81.25 %\n",
      "Epoch  10   1563  /  880 batch Loss:  0.30947771668434143   ACC:  87.5 %\n",
      "Epoch  10   1563  /  900 batch Loss:  0.18604014813899994   ACC:  96.875 %\n",
      "Epoch  10   1563  /  920 batch Loss:  0.14472393691539764   ACC:  96.875 %\n",
      "Epoch  10   1563  /  940 batch Loss:  0.560495138168335   ACC:  81.25 %\n",
      "Epoch  10   1563  /  960 batch Loss:  0.2760429382324219   ACC:  90.625 %\n",
      "Epoch  10   1563  /  980 batch Loss:  0.37911367416381836   ACC:  87.5 %\n",
      "Epoch  10   1563  /  1000 batch Loss:  0.5078092813491821   ACC:  84.375 %\n",
      "Epoch  10   1563  /  1020 batch Loss:  0.4113120138645172   ACC:  87.5 %\n",
      "Epoch  10   1563  /  1040 batch Loss:  0.3549194037914276   ACC:  84.375 %\n",
      "Epoch  10   1563  /  1060 batch Loss:  0.27639129757881165   ACC:  93.75 %\n",
      "Epoch  10   1563  /  1080 batch Loss:  0.9120610952377319   ACC:  81.25 %\n",
      "Epoch  10   1563  /  1100 batch Loss:  0.5371301770210266   ACC:  90.625 %\n",
      "Epoch  10   1563  /  1120 batch Loss:  0.2156858742237091   ACC:  87.5 %\n",
      "Epoch  10   1563  /  1140 batch Loss:  0.7513103485107422   ACC:  78.125 %\n",
      "Epoch  10   1563  /  1160 batch Loss:  0.37298479676246643   ACC:  87.5 %\n",
      "Epoch  10   1563  /  1180 batch Loss:  0.4284067153930664   ACC:  75.0 %\n",
      "Epoch  10   1563  /  1200 batch Loss:  0.3068804144859314   ACC:  90.625 %\n",
      "Epoch  10   1563  /  1220 batch Loss:  0.4418167471885681   ACC:  78.125 %\n",
      "Epoch  10   1563  /  1240 batch Loss:  0.3917941451072693   ACC:  84.375 %\n",
      "Epoch  10   1563  /  1260 batch Loss:  0.29833704233169556   ACC:  93.75 %\n",
      "Epoch  10   1563  /  1280 batch Loss:  0.5364103317260742   ACC:  75.0 %\n",
      "Epoch  10   1563  /  1300 batch Loss:  0.2159152328968048   ACC:  96.875 %\n",
      "Epoch  10   1563  /  1320 batch Loss:  0.23669062554836273   ACC:  93.75 %\n",
      "Epoch  10   1563  /  1340 batch Loss:  0.5535843968391418   ACC:  84.375 %\n",
      "Epoch  10   1563  /  1360 batch Loss:  0.2082672268152237   ACC:  96.875 %\n",
      "Epoch  10   1563  /  1380 batch Loss:  0.3018072843551636   ACC:  84.375 %\n",
      "Epoch  10   1563  /  1400 batch Loss:  0.2113906890153885   ACC:  96.875 %\n",
      "Epoch  10   1563  /  1420 batch Loss:  0.23503586649894714   ACC:  90.625 %\n",
      "Epoch  10   1563  /  1440 batch Loss:  0.4127757251262665   ACC:  84.375 %\n",
      "Epoch  10   1563  /  1460 batch Loss:  0.6723837852478027   ACC:  81.25 %\n",
      "Epoch  10   1563  /  1480 batch Loss:  0.44855955243110657   ACC:  78.125 %\n",
      "Epoch  10   1563  /  1500 batch Loss:  0.2718978524208069   ACC:  93.75 %\n",
      "Epoch  10   1563  /  1520 batch Loss:  0.2789158523082733   ACC:  96.875 %\n",
      "Epoch  10   1563  /  1540 batch Loss:  0.47398385405540466   ACC:  78.125 %\n",
      "Epoch  10   1563  /  1560 batch Loss:  0.24477717280387878   ACC:  90.625 %\n",
      "Test acc: 81.40 \n",
      "\n",
      "Epoch  11   1563  /  20 batch Loss:  0.32749220728874207   ACC:  87.5 %\n",
      "Epoch  11   1563  /  40 batch Loss:  0.24786829948425293   ACC:  93.75 %\n",
      "Epoch  11   1563  /  60 batch Loss:  0.6607888340950012   ACC:  81.25 %\n",
      "Epoch  11   1563  /  80 batch Loss:  0.24358363449573517   ACC:  90.625 %\n",
      "Epoch  11   1563  /  100 batch Loss:  0.7400454878807068   ACC:  84.375 %\n",
      "Epoch  11   1563  /  120 batch Loss:  0.35118481516838074   ACC:  87.5 %\n",
      "Epoch  11   1563  /  140 batch Loss:  0.3588024079799652   ACC:  84.375 %\n",
      "Epoch  11   1563  /  160 batch Loss:  0.13453443348407745   ACC:  100.0 %\n",
      "Epoch  11   1563  /  180 batch Loss:  0.2373959720134735   ACC:  93.75 %\n",
      "Epoch  11   1563  /  200 batch Loss:  0.28909653425216675   ACC:  93.75 %\n",
      "Epoch  11   1563  /  220 batch Loss:  0.34046947956085205   ACC:  84.375 %\n",
      "Epoch  11   1563  /  240 batch Loss:  0.28493884205818176   ACC:  90.625 %\n",
      "Epoch  11   1563  /  260 batch Loss:  0.4718029797077179   ACC:  87.5 %\n",
      "Epoch  11   1563  /  280 batch Loss:  0.7793903350830078   ACC:  84.375 %\n",
      "Epoch  11   1563  /  300 batch Loss:  0.3381594121456146   ACC:  90.625 %\n",
      "Epoch  11   1563  /  320 batch Loss:  0.3750747740268707   ACC:  87.5 %\n",
      "Epoch  11   1563  /  340 batch Loss:  0.44385969638824463   ACC:  81.25 %\n",
      "Epoch  11   1563  /  360 batch Loss:  0.14170682430267334   ACC:  96.875 %\n",
      "Epoch  11   1563  /  380 batch Loss:  0.2211105227470398   ACC:  93.75 %\n",
      "Epoch  11   1563  /  400 batch Loss:  0.2143852710723877   ACC:  93.75 %\n",
      "Epoch  11   1563  /  420 batch Loss:  0.32339799404144287   ACC:  81.25 %\n",
      "Epoch  11   1563  /  440 batch Loss:  0.28454330563545227   ACC:  96.875 %\n",
      "Epoch  11   1563  /  460 batch Loss:  0.2727111876010895   ACC:  87.5 %\n",
      "Epoch  11   1563  /  480 batch Loss:  0.49096882343292236   ACC:  81.25 %\n",
      "Epoch  11   1563  /  500 batch Loss:  0.7722376585006714   ACC:  75.0 %\n",
      "Epoch  11   1563  /  520 batch Loss:  0.6023423671722412   ACC:  78.125 %\n",
      "Epoch  11   1563  /  540 batch Loss:  0.5713738799095154   ACC:  78.125 %\n",
      "Epoch  11   1563  /  560 batch Loss:  0.3555133640766144   ACC:  90.625 %\n",
      "Epoch  11   1563  /  580 batch Loss:  0.23001399636268616   ACC:  93.75 %\n",
      "Epoch  11   1563  /  600 batch Loss:  0.23139503598213196   ACC:  90.625 %\n",
      "Epoch  11   1563  /  620 batch Loss:  0.37263014912605286   ACC:  87.5 %\n",
      "Epoch  11   1563  /  640 batch Loss:  0.30150413513183594   ACC:  90.625 %\n",
      "Epoch  11   1563  /  660 batch Loss:  0.2568875551223755   ACC:  87.5 %\n",
      "Epoch  11   1563  /  680 batch Loss:  0.8798055052757263   ACC:  71.875 %\n",
      "Epoch  11   1563  /  700 batch Loss:  0.293781578540802   ACC:  90.625 %\n",
      "Epoch  11   1563  /  720 batch Loss:  0.503170371055603   ACC:  78.125 %\n",
      "Epoch  11   1563  /  740 batch Loss:  0.5923315286636353   ACC:  84.375 %\n",
      "Epoch  11   1563  /  760 batch Loss:  0.18546035885810852   ACC:  90.625 %\n",
      "Epoch  11   1563  /  780 batch Loss:  0.41886797547340393   ACC:  84.375 %\n",
      "Epoch  11   1563  /  800 batch Loss:  0.42229124903678894   ACC:  84.375 %\n",
      "Epoch  11   1563  /  820 batch Loss:  0.8177305459976196   ACC:  71.875 %\n",
      "Epoch  11   1563  /  840 batch Loss:  0.32614848017692566   ACC:  87.5 %\n",
      "Epoch  11   1563  /  860 batch Loss:  0.3025277554988861   ACC:  90.625 %\n",
      "Epoch  11   1563  /  880 batch Loss:  0.4222780168056488   ACC:  87.5 %\n",
      "Epoch  11   1563  /  900 batch Loss:  0.25743600726127625   ACC:  90.625 %\n",
      "Epoch  11   1563  /  920 batch Loss:  0.19253861904144287   ACC:  93.75 %\n",
      "Epoch  11   1563  /  940 batch Loss:  0.41155996918678284   ACC:  90.625 %\n",
      "Epoch  11   1563  /  960 batch Loss:  0.26239392161369324   ACC:  90.625 %\n",
      "Epoch  11   1563  /  980 batch Loss:  0.558281421661377   ACC:  81.25 %\n",
      "Epoch  11   1563  /  1000 batch Loss:  0.5070968270301819   ACC:  78.125 %\n",
      "Epoch  11   1563  /  1020 batch Loss:  0.6868557929992676   ACC:  75.0 %\n",
      "Epoch  11   1563  /  1040 batch Loss:  0.06795551627874374   ACC:  100.0 %\n",
      "Epoch  11   1563  /  1060 batch Loss:  0.31839701533317566   ACC:  87.5 %\n",
      "Epoch  11   1563  /  1080 batch Loss:  0.29385995864868164   ACC:  87.5 %\n",
      "Epoch  11   1563  /  1100 batch Loss:  0.46705424785614014   ACC:  84.375 %\n",
      "Epoch  11   1563  /  1120 batch Loss:  0.4203537404537201   ACC:  84.375 %\n",
      "Epoch  11   1563  /  1140 batch Loss:  0.2582882344722748   ACC:  96.875 %\n",
      "Epoch  11   1563  /  1160 batch Loss:  0.6695014834403992   ACC:  81.25 %\n",
      "Epoch  11   1563  /  1180 batch Loss:  0.2640050947666168   ACC:  84.375 %\n",
      "Epoch  11   1563  /  1200 batch Loss:  0.3982226848602295   ACC:  81.25 %\n",
      "Epoch  11   1563  /  1220 batch Loss:  0.19049198925495148   ACC:  93.75 %\n",
      "Epoch  11   1563  /  1240 batch Loss:  0.3925468325614929   ACC:  84.375 %\n",
      "Epoch  11   1563  /  1260 batch Loss:  0.3439158797264099   ACC:  90.625 %\n",
      "Epoch  11   1563  /  1280 batch Loss:  0.23805445432662964   ACC:  93.75 %\n",
      "Epoch  11   1563  /  1300 batch Loss:  0.5148482918739319   ACC:  81.25 %\n",
      "Epoch  11   1563  /  1320 batch Loss:  0.4648570716381073   ACC:  84.375 %\n",
      "Epoch  11   1563  /  1340 batch Loss:  0.4825313091278076   ACC:  87.5 %\n",
      "Epoch  11   1563  /  1360 batch Loss:  0.15045717358589172   ACC:  93.75 %\n",
      "Epoch  11   1563  /  1380 batch Loss:  0.17690926790237427   ACC:  90.625 %\n",
      "Epoch  11   1563  /  1400 batch Loss:  0.1415909230709076   ACC:  96.875 %\n",
      "Epoch  11   1563  /  1420 batch Loss:  0.4574711322784424   ACC:  84.375 %\n",
      "Epoch  11   1563  /  1440 batch Loss:  0.32070034742355347   ACC:  90.625 %\n",
      "Epoch  11   1563  /  1460 batch Loss:  0.21166421473026276   ACC:  87.5 %\n",
      "Epoch  11   1563  /  1480 batch Loss:  0.2634003460407257   ACC:  90.625 %\n",
      "Epoch  11   1563  /  1500 batch Loss:  0.24944451451301575   ACC:  87.5 %\n",
      "Epoch  11   1563  /  1520 batch Loss:  0.5199956893920898   ACC:  81.25 %\n",
      "Epoch  11   1563  /  1540 batch Loss:  0.48468130826950073   ACC:  84.375 %\n",
      "Epoch  11   1563  /  1560 batch Loss:  0.4687950611114502   ACC:  78.125 %\n",
      "Test acc: 81.48 \n",
      "\n",
      "Epoch  12   1563  /  20 batch Loss:  0.26791271567344666   ACC:  90.625 %\n",
      "Epoch  12   1563  /  40 batch Loss:  0.3025083541870117   ACC:  87.5 %\n",
      "Epoch  12   1563  /  60 batch Loss:  0.1551515758037567   ACC:  96.875 %\n",
      "Epoch  12   1563  /  80 batch Loss:  0.19810299575328827   ACC:  90.625 %\n",
      "Epoch  12   1563  /  100 batch Loss:  0.49398598074913025   ACC:  78.125 %\n",
      "Epoch  12   1563  /  120 batch Loss:  0.2644142508506775   ACC:  90.625 %\n",
      "Epoch  12   1563  /  140 batch Loss:  0.4018508195877075   ACC:  87.5 %\n",
      "Epoch  12   1563  /  160 batch Loss:  0.3564353287220001   ACC:  84.375 %\n",
      "Epoch  12   1563  /  180 batch Loss:  0.21927447617053986   ACC:  87.5 %\n",
      "Epoch  12   1563  /  200 batch Loss:  0.4492713510990143   ACC:  90.625 %\n",
      "Epoch  12   1563  /  220 batch Loss:  0.4725346267223358   ACC:  84.375 %\n",
      "Epoch  12   1563  /  240 batch Loss:  0.463973730802536   ACC:  81.25 %\n",
      "Epoch  12   1563  /  260 batch Loss:  0.10231560468673706   ACC:  96.875 %\n",
      "Epoch  12   1563  /  280 batch Loss:  0.1804337352514267   ACC:  93.75 %\n",
      "Epoch  12   1563  /  300 batch Loss:  0.16531918942928314   ACC:  93.75 %\n",
      "Epoch  12   1563  /  320 batch Loss:  0.2137768715620041   ACC:  90.625 %\n",
      "Epoch  12   1563  /  340 batch Loss:  0.33051562309265137   ACC:  87.5 %\n",
      "Epoch  12   1563  /  360 batch Loss:  0.12674961984157562   ACC:  96.875 %\n",
      "Epoch  12   1563  /  380 batch Loss:  0.401445209980011   ACC:  84.375 %\n",
      "Epoch  12   1563  /  400 batch Loss:  0.3222097158432007   ACC:  87.5 %\n",
      "Epoch  12   1563  /  420 batch Loss:  0.357094407081604   ACC:  81.25 %\n",
      "Epoch  12   1563  /  440 batch Loss:  0.2778000235557556   ACC:  87.5 %\n",
      "Epoch  12   1563  /  460 batch Loss:  0.3674551844596863   ACC:  87.5 %\n",
      "Epoch  12   1563  /  480 batch Loss:  0.35840174555778503   ACC:  87.5 %\n",
      "Epoch  12   1563  /  500 batch Loss:  0.5005032420158386   ACC:  84.375 %\n",
      "Epoch  12   1563  /  520 batch Loss:  0.32494786381721497   ACC:  84.375 %\n",
      "Epoch  12   1563  /  540 batch Loss:  0.29021787643432617   ACC:  87.5 %\n",
      "Epoch  12   1563  /  560 batch Loss:  0.4477049708366394   ACC:  84.375 %\n",
      "Epoch  12   1563  /  580 batch Loss:  0.40520527958869934   ACC:  87.5 %\n",
      "Epoch  12   1563  /  600 batch Loss:  0.21283598244190216   ACC:  93.75 %\n",
      "Epoch  12   1563  /  620 batch Loss:  0.5990782380104065   ACC:  71.875 %\n",
      "Epoch  12   1563  /  640 batch Loss:  0.23994103074073792   ACC:  87.5 %\n",
      "Epoch  12   1563  /  660 batch Loss:  0.40957313776016235   ACC:  81.25 %\n",
      "Epoch  12   1563  /  680 batch Loss:  0.27295807003974915   ACC:  87.5 %\n",
      "Epoch  12   1563  /  700 batch Loss:  0.1262327879667282   ACC:  96.875 %\n",
      "Epoch  12   1563  /  720 batch Loss:  0.20786593854427338   ACC:  93.75 %\n",
      "Epoch  12   1563  /  740 batch Loss:  0.3639105558395386   ACC:  84.375 %\n",
      "Epoch  12   1563  /  760 batch Loss:  0.40948063135147095   ACC:  81.25 %\n",
      "Epoch  12   1563  /  780 batch Loss:  0.23980140686035156   ACC:  93.75 %\n",
      "Epoch  12   1563  /  800 batch Loss:  0.18317225575447083   ACC:  93.75 %\n",
      "Epoch  12   1563  /  820 batch Loss:  0.7705807089805603   ACC:  75.0 %\n",
      "Epoch  12   1563  /  840 batch Loss:  0.2754198908805847   ACC:  90.625 %\n",
      "Epoch  12   1563  /  860 batch Loss:  0.3801058232784271   ACC:  84.375 %\n",
      "Epoch  12   1563  /  880 batch Loss:  0.3071654736995697   ACC:  87.5 %\n",
      "Epoch  12   1563  /  900 batch Loss:  0.4555564224720001   ACC:  84.375 %\n",
      "Epoch  12   1563  /  920 batch Loss:  0.2550007104873657   ACC:  87.5 %\n",
      "Epoch  12   1563  /  940 batch Loss:  0.4749777615070343   ACC:  87.5 %\n",
      "Epoch  12   1563  /  960 batch Loss:  0.25169867277145386   ACC:  87.5 %\n",
      "Epoch  12   1563  /  980 batch Loss:  0.4777202010154724   ACC:  78.125 %\n",
      "Epoch  12   1563  /  1000 batch Loss:  0.255795955657959   ACC:  93.75 %\n",
      "Epoch  12   1563  /  1020 batch Loss:  0.5616326332092285   ACC:  84.375 %\n",
      "Epoch  12   1563  /  1040 batch Loss:  0.4057126045227051   ACC:  87.5 %\n",
      "Epoch  12   1563  /  1060 batch Loss:  0.44586214423179626   ACC:  87.5 %\n",
      "Epoch  12   1563  /  1080 batch Loss:  0.33897823095321655   ACC:  84.375 %\n",
      "Epoch  12   1563  /  1100 batch Loss:  0.2133883535861969   ACC:  90.625 %\n",
      "Epoch  12   1563  /  1120 batch Loss:  0.23955385386943817   ACC:  87.5 %\n",
      "Epoch  12   1563  /  1140 batch Loss:  0.23901128768920898   ACC:  87.5 %\n",
      "Epoch  12   1563  /  1160 batch Loss:  0.364245742559433   ACC:  90.625 %\n",
      "Epoch  12   1563  /  1180 batch Loss:  0.298557847738266   ACC:  93.75 %\n",
      "Epoch  12   1563  /  1200 batch Loss:  0.16168376803398132   ACC:  96.875 %\n",
      "Epoch  12   1563  /  1220 batch Loss:  0.21641767024993896   ACC:  93.75 %\n",
      "Epoch  12   1563  /  1240 batch Loss:  0.27114054560661316   ACC:  84.375 %\n",
      "Epoch  12   1563  /  1260 batch Loss:  0.4417801797389984   ACC:  87.5 %\n",
      "Epoch  12   1563  /  1280 batch Loss:  0.23863939940929413   ACC:  93.75 %\n",
      "Epoch  12   1563  /  1300 batch Loss:  0.5297983884811401   ACC:  75.0 %\n",
      "Epoch  12   1563  /  1320 batch Loss:  0.08259028196334839   ACC:  100.0 %\n",
      "Epoch  12   1563  /  1340 batch Loss:  0.2696945071220398   ACC:  87.5 %\n",
      "Epoch  12   1563  /  1360 batch Loss:  0.2259042114019394   ACC:  90.625 %\n",
      "Epoch  12   1563  /  1380 batch Loss:  0.07128506153821945   ACC:  96.875 %\n",
      "Epoch  12   1563  /  1400 batch Loss:  0.15861156582832336   ACC:  93.75 %\n",
      "Epoch  12   1563  /  1420 batch Loss:  0.16660082340240479   ACC:  93.75 %\n",
      "Epoch  12   1563  /  1440 batch Loss:  0.38786035776138306   ACC:  87.5 %\n",
      "Epoch  12   1563  /  1460 batch Loss:  0.2523599863052368   ACC:  90.625 %\n",
      "Epoch  12   1563  /  1480 batch Loss:  0.3117232620716095   ACC:  93.75 %\n",
      "Epoch  12   1563  /  1500 batch Loss:  0.5358933210372925   ACC:  87.5 %\n",
      "Epoch  12   1563  /  1520 batch Loss:  0.34549441933631897   ACC:  87.5 %\n",
      "Epoch  12   1563  /  1540 batch Loss:  0.330257385969162   ACC:  81.25 %\n",
      "Epoch  12   1563  /  1560 batch Loss:  0.6141689419746399   ACC:  81.25 %\n",
      "Test acc: 81.75 \n",
      "\n",
      "Epoch  13   1563  /  20 batch Loss:  0.2311672568321228   ACC:  90.625 %\n",
      "Epoch  13   1563  /  40 batch Loss:  0.32477402687072754   ACC:  90.625 %\n",
      "Epoch  13   1563  /  60 batch Loss:  0.2830435037612915   ACC:  90.625 %\n",
      "Epoch  13   1563  /  80 batch Loss:  0.16271643340587616   ACC:  96.875 %\n",
      "Epoch  13   1563  /  100 batch Loss:  0.3412359952926636   ACC:  81.25 %\n",
      "Epoch  13   1563  /  120 batch Loss:  0.34193044900894165   ACC:  90.625 %\n",
      "Epoch  13   1563  /  140 batch Loss:  0.21722795069217682   ACC:  93.75 %\n",
      "Epoch  13   1563  /  160 batch Loss:  0.14771398901939392   ACC:  90.625 %\n",
      "Epoch  13   1563  /  180 batch Loss:  0.32768869400024414   ACC:  93.75 %\n",
      "Epoch  13   1563  /  200 batch Loss:  0.3414459526538849   ACC:  90.625 %\n",
      "Epoch  13   1563  /  220 batch Loss:  0.44260644912719727   ACC:  78.125 %\n",
      "Epoch  13   1563  /  240 batch Loss:  0.50629061460495   ACC:  90.625 %\n",
      "Epoch  13   1563  /  260 batch Loss:  0.36933740973472595   ACC:  90.625 %\n",
      "Epoch  13   1563  /  280 batch Loss:  0.28757134079933167   ACC:  93.75 %\n",
      "Epoch  13   1563  /  300 batch Loss:  0.18016569316387177   ACC:  93.75 %\n",
      "Epoch  13   1563  /  320 batch Loss:  0.218612939119339   ACC:  93.75 %\n",
      "Epoch  13   1563  /  340 batch Loss:  0.42968297004699707   ACC:  90.625 %\n",
      "Epoch  13   1563  /  360 batch Loss:  0.4586501121520996   ACC:  84.375 %\n",
      "Epoch  13   1563  /  380 batch Loss:  0.3769738972187042   ACC:  87.5 %\n",
      "Epoch  13   1563  /  400 batch Loss:  0.2875805199146271   ACC:  93.75 %\n",
      "Epoch  13   1563  /  420 batch Loss:  0.2328362911939621   ACC:  90.625 %\n",
      "Epoch  13   1563  /  440 batch Loss:  0.3174791634082794   ACC:  90.625 %\n",
      "Epoch  13   1563  /  460 batch Loss:  0.2747321128845215   ACC:  90.625 %\n",
      "Epoch  13   1563  /  480 batch Loss:  0.26013290882110596   ACC:  93.75 %\n",
      "Epoch  13   1563  /  500 batch Loss:  0.17825625836849213   ACC:  96.875 %\n",
      "Epoch  13   1563  /  520 batch Loss:  0.2780108153820038   ACC:  87.5 %\n",
      "Epoch  13   1563  /  540 batch Loss:  0.35520949959754944   ACC:  84.375 %\n",
      "Epoch  13   1563  /  560 batch Loss:  0.4396936595439911   ACC:  81.25 %\n",
      "Epoch  13   1563  /  580 batch Loss:  0.1124606803059578   ACC:  96.875 %\n",
      "Epoch  13   1563  /  600 batch Loss:  0.33293184638023376   ACC:  90.625 %\n",
      "Epoch  13   1563  /  620 batch Loss:  0.21936370432376862   ACC:  93.75 %\n",
      "Epoch  13   1563  /  640 batch Loss:  0.3732524812221527   ACC:  84.375 %\n",
      "Epoch  13   1563  /  660 batch Loss:  0.4067511260509491   ACC:  84.375 %\n",
      "Epoch  13   1563  /  680 batch Loss:  0.2807807922363281   ACC:  90.625 %\n",
      "Epoch  13   1563  /  700 batch Loss:  0.36887961626052856   ACC:  90.625 %\n",
      "Epoch  13   1563  /  720 batch Loss:  0.2575882375240326   ACC:  87.5 %\n",
      "Epoch  13   1563  /  740 batch Loss:  0.23213762044906616   ACC:  87.5 %\n",
      "Epoch  13   1563  /  760 batch Loss:  0.1841440349817276   ACC:  96.875 %\n",
      "Epoch  13   1563  /  780 batch Loss:  0.15238887071609497   ACC:  93.75 %\n",
      "Epoch  13   1563  /  800 batch Loss:  0.23142866790294647   ACC:  90.625 %\n",
      "Epoch  13   1563  /  820 batch Loss:  0.39717942476272583   ACC:  87.5 %\n",
      "Epoch  13   1563  /  840 batch Loss:  0.1218184232711792   ACC:  96.875 %\n",
      "Epoch  13   1563  /  860 batch Loss:  0.16506314277648926   ACC:  96.875 %\n",
      "Epoch  13   1563  /  880 batch Loss:  0.12395286560058594   ACC:  96.875 %\n",
      "Epoch  13   1563  /  900 batch Loss:  0.6987871527671814   ACC:  81.25 %\n",
      "Epoch  13   1563  /  920 batch Loss:  0.2094694972038269   ACC:  90.625 %\n",
      "Epoch  13   1563  /  940 batch Loss:  0.3518655002117157   ACC:  90.625 %\n",
      "Epoch  13   1563  /  960 batch Loss:  0.3853108286857605   ACC:  81.25 %\n",
      "Epoch  13   1563  /  980 batch Loss:  0.15861910581588745   ACC:  93.75 %\n",
      "Epoch  13   1563  /  1000 batch Loss:  0.166981041431427   ACC:  93.75 %\n",
      "Epoch  13   1563  /  1020 batch Loss:  0.2619912624359131   ACC:  90.625 %\n",
      "Epoch  13   1563  /  1040 batch Loss:  0.259063184261322   ACC:  90.625 %\n",
      "Epoch  13   1563  /  1060 batch Loss:  0.3976864516735077   ACC:  87.5 %\n",
      "Epoch  13   1563  /  1080 batch Loss:  0.06784455478191376   ACC:  100.0 %\n",
      "Epoch  13   1563  /  1100 batch Loss:  0.26396653056144714   ACC:  93.75 %\n",
      "Epoch  13   1563  /  1120 batch Loss:  0.23874130845069885   ACC:  90.625 %\n",
      "Epoch  13   1563  /  1140 batch Loss:  0.319221556186676   ACC:  84.375 %\n",
      "Epoch  13   1563  /  1160 batch Loss:  0.3307681083679199   ACC:  78.125 %\n",
      "Epoch  13   1563  /  1180 batch Loss:  0.10413113981485367   ACC:  100.0 %\n",
      "Epoch  13   1563  /  1200 batch Loss:  0.2200082242488861   ACC:  93.75 %\n",
      "Epoch  13   1563  /  1220 batch Loss:  0.518973708152771   ACC:  75.0 %\n",
      "Epoch  13   1563  /  1240 batch Loss:  0.5307273864746094   ACC:  84.375 %\n",
      "Epoch  13   1563  /  1260 batch Loss:  0.49093469977378845   ACC:  78.125 %\n",
      "Epoch  13   1563  /  1280 batch Loss:  0.21220512688159943   ACC:  96.875 %\n",
      "Epoch  13   1563  /  1300 batch Loss:  0.6675763130187988   ACC:  71.875 %\n",
      "Epoch  13   1563  /  1320 batch Loss:  0.3818627893924713   ACC:  81.25 %\n",
      "Epoch  13   1563  /  1340 batch Loss:  0.5359442830085754   ACC:  78.125 %\n",
      "Epoch  13   1563  /  1360 batch Loss:  0.5708329677581787   ACC:  81.25 %\n",
      "Epoch  13   1563  /  1380 batch Loss:  0.1573381870985031   ACC:  90.625 %\n",
      "Epoch  13   1563  /  1400 batch Loss:  0.6775189638137817   ACC:  78.125 %\n",
      "Epoch  13   1563  /  1420 batch Loss:  0.4185047745704651   ACC:  84.375 %\n",
      "Epoch  13   1563  /  1440 batch Loss:  0.17385916411876678   ACC:  90.625 %\n",
      "Epoch  13   1563  /  1460 batch Loss:  0.30632802844047546   ACC:  81.25 %\n",
      "Epoch  13   1563  /  1480 batch Loss:  0.26354578137397766   ACC:  90.625 %\n",
      "Epoch  13   1563  /  1500 batch Loss:  0.2644590735435486   ACC:  90.625 %\n",
      "Epoch  13   1563  /  1520 batch Loss:  0.1290995180606842   ACC:  90.625 %\n",
      "Epoch  13   1563  /  1540 batch Loss:  0.6782974004745483   ACC:  78.125 %\n",
      "Epoch  13   1563  /  1560 batch Loss:  0.36664631962776184   ACC:  84.375 %\n",
      "Test acc: 80.57 \n",
      "\n",
      "Epoch  14   1563  /  20 batch Loss:  0.4561392068862915   ACC:  84.375 %\n",
      "Epoch  14   1563  /  40 batch Loss:  0.18188242614269257   ACC:  93.75 %\n",
      "Epoch  14   1563  /  60 batch Loss:  0.08779659122228622   ACC:  100.0 %\n",
      "Epoch  14   1563  /  80 batch Loss:  0.2000720053911209   ACC:  90.625 %\n",
      "Epoch  14   1563  /  100 batch Loss:  0.20907069742679596   ACC:  90.625 %\n",
      "Epoch  14   1563  /  120 batch Loss:  0.19426962733268738   ACC:  93.75 %\n",
      "Epoch  14   1563  /  140 batch Loss:  0.20898132026195526   ACC:  90.625 %\n",
      "Epoch  14   1563  /  160 batch Loss:  0.31413957476615906   ACC:  90.625 %\n",
      "Epoch  14   1563  /  180 batch Loss:  0.6313202977180481   ACC:  78.125 %\n",
      "Epoch  14   1563  /  200 batch Loss:  0.15438634157180786   ACC:  93.75 %\n",
      "Epoch  14   1563  /  220 batch Loss:  0.19083046913146973   ACC:  87.5 %\n",
      "Epoch  14   1563  /  240 batch Loss:  0.1445447951555252   ACC:  96.875 %\n",
      "Epoch  14   1563  /  260 batch Loss:  0.3091892600059509   ACC:  93.75 %\n",
      "Epoch  14   1563  /  280 batch Loss:  0.18836839497089386   ACC:  93.75 %\n",
      "Epoch  14   1563  /  300 batch Loss:  0.30000755190849304   ACC:  90.625 %\n",
      "Epoch  14   1563  /  320 batch Loss:  0.24854901432991028   ACC:  90.625 %\n",
      "Epoch  14   1563  /  340 batch Loss:  0.12648603320121765   ACC:  93.75 %\n",
      "Epoch  14   1563  /  360 batch Loss:  0.31016382575035095   ACC:  90.625 %\n",
      "Epoch  14   1563  /  380 batch Loss:  0.34230250120162964   ACC:  84.375 %\n",
      "Epoch  14   1563  /  400 batch Loss:  0.21448232233524323   ACC:  90.625 %\n",
      "Epoch  14   1563  /  420 batch Loss:  0.23431575298309326   ACC:  90.625 %\n",
      "Epoch  14   1563  /  440 batch Loss:  0.15642285346984863   ACC:  96.875 %\n",
      "Epoch  14   1563  /  460 batch Loss:  0.3265846073627472   ACC:  90.625 %\n",
      "Epoch  14   1563  /  480 batch Loss:  0.37186822295188904   ACC:  93.75 %\n",
      "Epoch  14   1563  /  500 batch Loss:  0.3736385107040405   ACC:  90.625 %\n",
      "Epoch  14   1563  /  520 batch Loss:  0.24174864590168   ACC:  90.625 %\n",
      "Epoch  14   1563  /  540 batch Loss:  0.1102064996957779   ACC:  96.875 %\n",
      "Epoch  14   1563  /  560 batch Loss:  0.2680691182613373   ACC:  87.5 %\n",
      "Epoch  14   1563  /  580 batch Loss:  0.16107173264026642   ACC:  93.75 %\n",
      "Epoch  14   1563  /  600 batch Loss:  0.1946517378091812   ACC:  96.875 %\n",
      "Epoch  14   1563  /  620 batch Loss:  0.1463816910982132   ACC:  96.875 %\n",
      "Epoch  14   1563  /  640 batch Loss:  0.2070012390613556   ACC:  93.75 %\n",
      "Epoch  14   1563  /  660 batch Loss:  0.3282431662082672   ACC:  93.75 %\n",
      "Epoch  14   1563  /  680 batch Loss:  0.24449987709522247   ACC:  90.625 %\n",
      "Epoch  14   1563  /  700 batch Loss:  0.595369815826416   ACC:  84.375 %\n",
      "Epoch  14   1563  /  720 batch Loss:  0.3473479449748993   ACC:  84.375 %\n",
      "Epoch  14   1563  /  740 batch Loss:  0.2740519642829895   ACC:  84.375 %\n",
      "Epoch  14   1563  /  760 batch Loss:  0.13481153547763824   ACC:  93.75 %\n",
      "Epoch  14   1563  /  780 batch Loss:  0.47019127011299133   ACC:  87.5 %\n",
      "Epoch  14   1563  /  800 batch Loss:  0.44901788234710693   ACC:  78.125 %\n",
      "Epoch  14   1563  /  820 batch Loss:  0.19902026653289795   ACC:  93.75 %\n",
      "Epoch  14   1563  /  840 batch Loss:  0.15928320586681366   ACC:  93.75 %\n",
      "Epoch  14   1563  /  860 batch Loss:  0.2953718602657318   ACC:  84.375 %\n",
      "Epoch  14   1563  /  880 batch Loss:  0.3406253755092621   ACC:  87.5 %\n",
      "Epoch  14   1563  /  900 batch Loss:  0.1258103996515274   ACC:  93.75 %\n",
      "Epoch  14   1563  /  920 batch Loss:  0.2848929464817047   ACC:  90.625 %\n",
      "Epoch  14   1563  /  940 batch Loss:  0.3261130750179291   ACC:  87.5 %\n",
      "Epoch  14   1563  /  960 batch Loss:  0.27648523449897766   ACC:  87.5 %\n",
      "Epoch  14   1563  /  980 batch Loss:  0.252938449382782   ACC:  90.625 %\n",
      "Epoch  14   1563  /  1000 batch Loss:  0.19694259762763977   ACC:  93.75 %\n",
      "Epoch  14   1563  /  1020 batch Loss:  0.2853628695011139   ACC:  87.5 %\n",
      "Epoch  14   1563  /  1040 batch Loss:  0.2680099904537201   ACC:  93.75 %\n",
      "Epoch  14   1563  /  1060 batch Loss:  0.22083555161952972   ACC:  84.375 %\n",
      "Epoch  14   1563  /  1080 batch Loss:  0.5760310292243958   ACC:  90.625 %\n",
      "Epoch  14   1563  /  1100 batch Loss:  0.2945455312728882   ACC:  93.75 %\n",
      "Epoch  14   1563  /  1120 batch Loss:  0.43516993522644043   ACC:  84.375 %\n",
      "Epoch  14   1563  /  1140 batch Loss:  0.21610206365585327   ACC:  87.5 %\n",
      "Epoch  14   1563  /  1160 batch Loss:  0.18962088227272034   ACC:  93.75 %\n",
      "Epoch  14   1563  /  1180 batch Loss:  0.119371697306633   ACC:  96.875 %\n",
      "Epoch  14   1563  /  1200 batch Loss:  0.25282707810401917   ACC:  90.625 %\n",
      "Epoch  14   1563  /  1220 batch Loss:  0.29500627517700195   ACC:  90.625 %\n",
      "Epoch  14   1563  /  1240 batch Loss:  0.10318373888731003   ACC:  96.875 %\n",
      "Epoch  14   1563  /  1260 batch Loss:  0.2942802309989929   ACC:  93.75 %\n",
      "Epoch  14   1563  /  1280 batch Loss:  0.18174608051776886   ACC:  93.75 %\n",
      "Epoch  14   1563  /  1300 batch Loss:  0.22033905982971191   ACC:  90.625 %\n",
      "Epoch  14   1563  /  1320 batch Loss:  0.3424462080001831   ACC:  87.5 %\n",
      "Epoch  14   1563  /  1340 batch Loss:  0.3366089463233948   ACC:  87.5 %\n",
      "Epoch  14   1563  /  1360 batch Loss:  0.294846773147583   ACC:  96.875 %\n",
      "Epoch  14   1563  /  1380 batch Loss:  0.39114996790885925   ACC:  84.375 %\n",
      "Epoch  14   1563  /  1400 batch Loss:  0.25136682391166687   ACC:  87.5 %\n",
      "Epoch  14   1563  /  1420 batch Loss:  0.2956393361091614   ACC:  90.625 %\n",
      "Epoch  14   1563  /  1440 batch Loss:  0.15476736426353455   ACC:  96.875 %\n",
      "Epoch  14   1563  /  1460 batch Loss:  0.2716709077358246   ACC:  90.625 %\n",
      "Epoch  14   1563  /  1480 batch Loss:  0.38417139649391174   ACC:  84.375 %\n",
      "Epoch  14   1563  /  1500 batch Loss:  0.2743673026561737   ACC:  87.5 %\n",
      "Epoch  14   1563  /  1520 batch Loss:  0.31293627619743347   ACC:  90.625 %\n",
      "Epoch  14   1563  /  1540 batch Loss:  0.3944585919380188   ACC:  84.375 %\n",
      "Epoch  14   1563  /  1560 batch Loss:  0.11249636858701706   ACC:  100.0 %\n",
      "Test acc: 81.97 \n",
      "\n",
      "Epoch  15   1563  /  20 batch Loss:  0.1991138756275177   ACC:  93.75 %\n",
      "Epoch  15   1563  /  40 batch Loss:  0.2447906732559204   ACC:  87.5 %\n",
      "Epoch  15   1563  /  60 batch Loss:  0.22756898403167725   ACC:  90.625 %\n",
      "Epoch  15   1563  /  80 batch Loss:  0.18249163031578064   ACC:  96.875 %\n",
      "Epoch  15   1563  /  100 batch Loss:  0.23096492886543274   ACC:  93.75 %\n",
      "Epoch  15   1563  /  120 batch Loss:  0.2691107988357544   ACC:  87.5 %\n",
      "Epoch  15   1563  /  140 batch Loss:  0.2179577499628067   ACC:  93.75 %\n",
      "Epoch  15   1563  /  160 batch Loss:  0.10713387280702591   ACC:  96.875 %\n",
      "Epoch  15   1563  /  180 batch Loss:  0.32259058952331543   ACC:  90.625 %\n",
      "Epoch  15   1563  /  200 batch Loss:  0.23285730183124542   ACC:  93.75 %\n",
      "Epoch  15   1563  /  220 batch Loss:  0.395095556974411   ACC:  81.25 %\n",
      "Epoch  15   1563  /  240 batch Loss:  0.14138592779636383   ACC:  96.875 %\n",
      "Epoch  15   1563  /  260 batch Loss:  0.10630852729082108   ACC:  100.0 %\n",
      "Epoch  15   1563  /  280 batch Loss:  0.33172598481178284   ACC:  84.375 %\n",
      "Epoch  15   1563  /  300 batch Loss:  0.21723514795303345   ACC:  93.75 %\n",
      "Epoch  15   1563  /  320 batch Loss:  0.3763524293899536   ACC:  81.25 %\n",
      "Epoch  15   1563  /  340 batch Loss:  0.08522889763116837   ACC:  96.875 %\n",
      "Epoch  15   1563  /  360 batch Loss:  0.15056033432483673   ACC:  90.625 %\n",
      "Epoch  15   1563  /  380 batch Loss:  0.3109574317932129   ACC:  90.625 %\n",
      "Epoch  15   1563  /  400 batch Loss:  0.38040974736213684   ACC:  87.5 %\n",
      "Epoch  15   1563  /  420 batch Loss:  0.22331763803958893   ACC:  96.875 %\n",
      "Epoch  15   1563  /  440 batch Loss:  0.2409050166606903   ACC:  93.75 %\n",
      "Epoch  15   1563  /  460 batch Loss:  0.3044862449169159   ACC:  84.375 %\n",
      "Epoch  15   1563  /  480 batch Loss:  0.28180819749832153   ACC:  90.625 %\n",
      "Epoch  15   1563  /  500 batch Loss:  0.45857736468315125   ACC:  87.5 %\n",
      "Epoch  15   1563  /  520 batch Loss:  0.2231779396533966   ACC:  90.625 %\n",
      "Epoch  15   1563  /  540 batch Loss:  0.2294556051492691   ACC:  93.75 %\n",
      "Epoch  15   1563  /  560 batch Loss:  0.17181697487831116   ACC:  93.75 %\n",
      "Epoch  15   1563  /  580 batch Loss:  0.2494029551744461   ACC:  90.625 %\n",
      "Epoch  15   1563  /  600 batch Loss:  0.366639643907547   ACC:  90.625 %\n",
      "Epoch  15   1563  /  620 batch Loss:  0.3212500214576721   ACC:  90.625 %\n",
      "Epoch  15   1563  /  640 batch Loss:  0.18661025166511536   ACC:  93.75 %\n",
      "Epoch  15   1563  /  660 batch Loss:  0.13120412826538086   ACC:  93.75 %\n",
      "Epoch  15   1563  /  680 batch Loss:  0.273510217666626   ACC:  90.625 %\n",
      "Epoch  15   1563  /  700 batch Loss:  0.2709064781665802   ACC:  90.625 %\n",
      "Epoch  15   1563  /  720 batch Loss:  0.16524526476860046   ACC:  96.875 %\n",
      "Epoch  15   1563  /  740 batch Loss:  0.18699638545513153   ACC:  90.625 %\n",
      "Epoch  15   1563  /  760 batch Loss:  0.1792324185371399   ACC:  93.75 %\n",
      "Epoch  15   1563  /  780 batch Loss:  0.2700185477733612   ACC:  84.375 %\n",
      "Epoch  15   1563  /  800 batch Loss:  0.41050592064857483   ACC:  84.375 %\n",
      "Epoch  15   1563  /  820 batch Loss:  0.20962399244308472   ACC:  93.75 %\n",
      "Epoch  15   1563  /  840 batch Loss:  0.19729778170585632   ACC:  90.625 %\n",
      "Epoch  15   1563  /  860 batch Loss:  0.06793254613876343   ACC:  96.875 %\n",
      "Epoch  15   1563  /  880 batch Loss:  0.38304582238197327   ACC:  87.5 %\n",
      "Epoch  15   1563  /  900 batch Loss:  0.1706155687570572   ACC:  96.875 %\n",
      "Epoch  15   1563  /  920 batch Loss:  0.37552639842033386   ACC:  93.75 %\n",
      "Epoch  15   1563  /  940 batch Loss:  0.25191187858581543   ACC:  90.625 %\n",
      "Epoch  15   1563  /  960 batch Loss:  0.1189287081360817   ACC:  93.75 %\n",
      "Epoch  15   1563  /  980 batch Loss:  0.11192546039819717   ACC:  100.0 %\n",
      "Epoch  15   1563  /  1000 batch Loss:  0.20223328471183777   ACC:  90.625 %\n",
      "Epoch  15   1563  /  1020 batch Loss:  0.15538708865642548   ACC:  96.875 %\n",
      "Epoch  15   1563  /  1040 batch Loss:  0.14739613234996796   ACC:  93.75 %\n",
      "Epoch  15   1563  /  1060 batch Loss:  0.10166411846876144   ACC:  96.875 %\n",
      "Epoch  15   1563  /  1080 batch Loss:  0.08827344328165054   ACC:  96.875 %\n",
      "Epoch  15   1563  /  1100 batch Loss:  0.1773528903722763   ACC:  90.625 %\n",
      "Epoch  15   1563  /  1120 batch Loss:  0.26398298144340515   ACC:  87.5 %\n",
      "Epoch  15   1563  /  1140 batch Loss:  0.2673690617084503   ACC:  90.625 %\n",
      "Epoch  15   1563  /  1160 batch Loss:  0.39811617136001587   ACC:  87.5 %\n",
      "Epoch  15   1563  /  1180 batch Loss:  0.0717252641916275   ACC:  96.875 %\n",
      "Epoch  15   1563  /  1200 batch Loss:  0.32850179076194763   ACC:  84.375 %\n",
      "Epoch  15   1563  /  1220 batch Loss:  0.5025956630706787   ACC:  78.125 %\n",
      "Epoch  15   1563  /  1240 batch Loss:  0.23211540281772614   ACC:  87.5 %\n",
      "Epoch  15   1563  /  1260 batch Loss:  0.4106632471084595   ACC:  93.75 %\n",
      "Epoch  15   1563  /  1280 batch Loss:  0.04516883194446564   ACC:  100.0 %\n",
      "Epoch  15   1563  /  1300 batch Loss:  0.22536706924438477   ACC:  90.625 %\n",
      "Epoch  15   1563  /  1320 batch Loss:  0.4721560776233673   ACC:  84.375 %\n",
      "Epoch  15   1563  /  1340 batch Loss:  0.20424550771713257   ACC:  90.625 %\n",
      "Epoch  15   1563  /  1360 batch Loss:  0.22273924946784973   ACC:  87.5 %\n",
      "Epoch  15   1563  /  1380 batch Loss:  0.08309520035982132   ACC:  96.875 %\n",
      "Epoch  15   1563  /  1400 batch Loss:  0.10994657874107361   ACC:  96.875 %\n",
      "Epoch  15   1563  /  1420 batch Loss:  0.3204408884048462   ACC:  93.75 %\n",
      "Epoch  15   1563  /  1440 batch Loss:  0.23211558163166046   ACC:  93.75 %\n",
      "Epoch  15   1563  /  1460 batch Loss:  0.28756389021873474   ACC:  90.625 %\n",
      "Epoch  15   1563  /  1480 batch Loss:  0.3405889570713043   ACC:  87.5 %\n",
      "Epoch  15   1563  /  1500 batch Loss:  0.16360072791576385   ACC:  90.625 %\n",
      "Epoch  15   1563  /  1520 batch Loss:  0.4168396592140198   ACC:  84.375 %\n",
      "Epoch  15   1563  /  1540 batch Loss:  0.21367229521274567   ACC:  93.75 %\n",
      "Epoch  15   1563  /  1560 batch Loss:  0.3802253007888794   ACC:  93.75 %\n",
      "Test acc: 82.06 \n",
      "\n",
      "Epoch  16   1563  /  20 batch Loss:  0.16404305398464203   ACC:  93.75 %\n",
      "Epoch  16   1563  /  40 batch Loss:  0.33881422877311707   ACC:  87.5 %\n",
      "Epoch  16   1563  /  60 batch Loss:  0.22161194682121277   ACC:  90.625 %\n",
      "Epoch  16   1563  /  80 batch Loss:  0.2233964204788208   ACC:  90.625 %\n",
      "Epoch  16   1563  /  100 batch Loss:  0.35790857672691345   ACC:  84.375 %\n",
      "Epoch  16   1563  /  120 batch Loss:  0.144142284989357   ACC:  96.875 %\n",
      "Epoch  16   1563  /  140 batch Loss:  0.2356518656015396   ACC:  90.625 %\n",
      "Epoch  16   1563  /  160 batch Loss:  0.2907971739768982   ACC:  90.625 %\n",
      "Epoch  16   1563  /  180 batch Loss:  0.19040542840957642   ACC:  93.75 %\n",
      "Epoch  16   1563  /  200 batch Loss:  0.26986923813819885   ACC:  87.5 %\n",
      "Epoch  16   1563  /  220 batch Loss:  0.20237664878368378   ACC:  93.75 %\n",
      "Epoch  16   1563  /  240 batch Loss:  0.22664247453212738   ACC:  93.75 %\n",
      "Epoch  16   1563  /  260 batch Loss:  0.3645579516887665   ACC:  87.5 %\n",
      "Epoch  16   1563  /  280 batch Loss:  0.17070907354354858   ACC:  93.75 %\n",
      "Epoch  16   1563  /  300 batch Loss:  0.21817201375961304   ACC:  90.625 %\n",
      "Epoch  16   1563  /  320 batch Loss:  0.06523855775594711   ACC:  96.875 %\n",
      "Epoch  16   1563  /  340 batch Loss:  0.2637718915939331   ACC:  90.625 %\n",
      "Epoch  16   1563  /  360 batch Loss:  0.23705056309700012   ACC:  90.625 %\n",
      "Epoch  16   1563  /  380 batch Loss:  0.1960318386554718   ACC:  90.625 %\n",
      "Epoch  16   1563  /  400 batch Loss:  0.30591127276420593   ACC:  90.625 %\n",
      "Epoch  16   1563  /  420 batch Loss:  0.1180817037820816   ACC:  96.875 %\n",
      "Epoch  16   1563  /  440 batch Loss:  0.1774779111146927   ACC:  96.875 %\n",
      "Epoch  16   1563  /  460 batch Loss:  0.09615548700094223   ACC:  96.875 %\n",
      "Epoch  16   1563  /  480 batch Loss:  0.2167913019657135   ACC:  90.625 %\n",
      "Epoch  16   1563  /  500 batch Loss:  0.2782950699329376   ACC:  90.625 %\n",
      "Epoch  16   1563  /  520 batch Loss:  0.09343993663787842   ACC:  96.875 %\n",
      "Epoch  16   1563  /  540 batch Loss:  0.12420844286680222   ACC:  96.875 %\n",
      "Epoch  16   1563  /  560 batch Loss:  0.26815059781074524   ACC:  90.625 %\n",
      "Epoch  16   1563  /  580 batch Loss:  0.417524516582489   ACC:  90.625 %\n",
      "Epoch  16   1563  /  600 batch Loss:  0.24785400927066803   ACC:  90.625 %\n",
      "Epoch  16   1563  /  620 batch Loss:  0.32879066467285156   ACC:  87.5 %\n",
      "Epoch  16   1563  /  640 batch Loss:  0.1725768893957138   ACC:  93.75 %\n",
      "Epoch  16   1563  /  660 batch Loss:  0.23039653897285461   ACC:  90.625 %\n",
      "Epoch  16   1563  /  680 batch Loss:  0.3532828390598297   ACC:  90.625 %\n",
      "Epoch  16   1563  /  700 batch Loss:  0.27817487716674805   ACC:  84.375 %\n",
      "Epoch  16   1563  /  720 batch Loss:  0.21243856847286224   ACC:  93.75 %\n",
      "Epoch  16   1563  /  740 batch Loss:  0.38633015751838684   ACC:  90.625 %\n",
      "Epoch  16   1563  /  760 batch Loss:  0.2870088219642639   ACC:  87.5 %\n",
      "Epoch  16   1563  /  780 batch Loss:  0.2066630870103836   ACC:  90.625 %\n",
      "Epoch  16   1563  /  800 batch Loss:  0.3746572434902191   ACC:  84.375 %\n",
      "Epoch  16   1563  /  820 batch Loss:  0.4379905164241791   ACC:  87.5 %\n",
      "Epoch  16   1563  /  840 batch Loss:  0.10739569365978241   ACC:  93.75 %\n",
      "Epoch  16   1563  /  860 batch Loss:  0.12189668416976929   ACC:  96.875 %\n",
      "Epoch  16   1563  /  880 batch Loss:  0.20833635330200195   ACC:  90.625 %\n",
      "Epoch  16   1563  /  900 batch Loss:  0.1700226068496704   ACC:  93.75 %\n",
      "Epoch  16   1563  /  920 batch Loss:  0.1274789422750473   ACC:  93.75 %\n",
      "Epoch  16   1563  /  940 batch Loss:  0.23427805304527283   ACC:  87.5 %\n",
      "Epoch  16   1563  /  960 batch Loss:  0.1526208072900772   ACC:  96.875 %\n",
      "Epoch  16   1563  /  980 batch Loss:  0.2677105963230133   ACC:  90.625 %\n",
      "Epoch  16   1563  /  1000 batch Loss:  0.285137802362442   ACC:  87.5 %\n",
      "Epoch  16   1563  /  1020 batch Loss:  0.3282783627510071   ACC:  90.625 %\n",
      "Epoch  16   1563  /  1040 batch Loss:  0.1840563714504242   ACC:  93.75 %\n",
      "Epoch  16   1563  /  1060 batch Loss:  0.12711811065673828   ACC:  93.75 %\n",
      "Epoch  16   1563  /  1080 batch Loss:  0.32389867305755615   ACC:  93.75 %\n",
      "Epoch  16   1563  /  1100 batch Loss:  0.25087881088256836   ACC:  90.625 %\n",
      "Epoch  16   1563  /  1120 batch Loss:  0.16338269412517548   ACC:  90.625 %\n",
      "Epoch  16   1563  /  1140 batch Loss:  0.3934157192707062   ACC:  87.5 %\n",
      "Epoch  16   1563  /  1160 batch Loss:  0.22198981046676636   ACC:  90.625 %\n",
      "Epoch  16   1563  /  1180 batch Loss:  0.055499378591775894   ACC:  100.0 %\n",
      "Epoch  16   1563  /  1200 batch Loss:  0.31219637393951416   ACC:  93.75 %\n",
      "Epoch  16   1563  /  1220 batch Loss:  0.11674188077449799   ACC:  96.875 %\n",
      "Epoch  16   1563  /  1240 batch Loss:  0.132823184132576   ACC:  93.75 %\n",
      "Epoch  16   1563  /  1260 batch Loss:  0.12294549494981766   ACC:  96.875 %\n",
      "Epoch  16   1563  /  1280 batch Loss:  0.23821477591991425   ACC:  90.625 %\n",
      "Epoch  16   1563  /  1300 batch Loss:  0.29887405037879944   ACC:  90.625 %\n",
      "Epoch  16   1563  /  1320 batch Loss:  0.26043248176574707   ACC:  87.5 %\n",
      "Epoch  16   1563  /  1340 batch Loss:  0.2555124759674072   ACC:  84.375 %\n",
      "Epoch  16   1563  /  1360 batch Loss:  0.1385975480079651   ACC:  96.875 %\n",
      "Epoch  16   1563  /  1380 batch Loss:  0.23589253425598145   ACC:  96.875 %\n",
      "Epoch  16   1563  /  1400 batch Loss:  0.3587777018547058   ACC:  93.75 %\n",
      "Epoch  16   1563  /  1420 batch Loss:  0.1407770961523056   ACC:  96.875 %\n",
      "Epoch  16   1563  /  1440 batch Loss:  0.36540594696998596   ACC:  84.375 %\n",
      "Epoch  16   1563  /  1460 batch Loss:  0.13966508209705353   ACC:  96.875 %\n",
      "Epoch  16   1563  /  1480 batch Loss:  0.13691163063049316   ACC:  93.75 %\n",
      "Epoch  16   1563  /  1500 batch Loss:  0.2145024836063385   ACC:  90.625 %\n",
      "Epoch  16   1563  /  1520 batch Loss:  0.41933920979499817   ACC:  87.5 %\n",
      "Epoch  16   1563  /  1540 batch Loss:  0.12809200584888458   ACC:  96.875 %\n",
      "Epoch  16   1563  /  1560 batch Loss:  0.17332614958286285   ACC:  90.625 %\n",
      "Test acc: 83.11 \n",
      "\n",
      "Epoch  17   1563  /  20 batch Loss:  0.24691647291183472   ACC:  90.625 %\n",
      "Epoch  17   1563  /  40 batch Loss:  0.1497146338224411   ACC:  93.75 %\n",
      "Epoch  17   1563  /  60 batch Loss:  0.04917483031749725   ACC:  100.0 %\n",
      "Epoch  17   1563  /  80 batch Loss:  0.21162588894367218   ACC:  93.75 %\n",
      "Epoch  17   1563  /  100 batch Loss:  0.18011625111103058   ACC:  93.75 %\n",
      "Epoch  17   1563  /  120 batch Loss:  0.1859099119901657   ACC:  87.5 %\n",
      "Epoch  17   1563  /  140 batch Loss:  0.1729818731546402   ACC:  93.75 %\n",
      "Epoch  17   1563  /  160 batch Loss:  0.1611652821302414   ACC:  93.75 %\n",
      "Epoch  17   1563  /  180 batch Loss:  0.31917688250541687   ACC:  87.5 %\n",
      "Epoch  17   1563  /  200 batch Loss:  0.22531574964523315   ACC:  93.75 %\n",
      "Epoch  17   1563  /  220 batch Loss:  0.12539374828338623   ACC:  93.75 %\n",
      "Epoch  17   1563  /  240 batch Loss:  0.462620347738266   ACC:  93.75 %\n",
      "Epoch  17   1563  /  260 batch Loss:  0.1820426732301712   ACC:  96.875 %\n",
      "Epoch  17   1563  /  280 batch Loss:  0.35074421763420105   ACC:  87.5 %\n",
      "Epoch  17   1563  /  300 batch Loss:  0.25397196412086487   ACC:  93.75 %\n",
      "Epoch  17   1563  /  320 batch Loss:  0.16534779965877533   ACC:  93.75 %\n",
      "Epoch  17   1563  /  340 batch Loss:  0.189081072807312   ACC:  93.75 %\n",
      "Epoch  17   1563  /  360 batch Loss:  0.3739681541919708   ACC:  90.625 %\n",
      "Epoch  17   1563  /  380 batch Loss:  0.20261870324611664   ACC:  87.5 %\n",
      "Epoch  17   1563  /  400 batch Loss:  0.16777661442756653   ACC:  93.75 %\n",
      "Epoch  17   1563  /  420 batch Loss:  0.03999745473265648   ACC:  100.0 %\n",
      "Epoch  17   1563  /  440 batch Loss:  0.10522661358118057   ACC:  96.875 %\n",
      "Epoch  17   1563  /  460 batch Loss:  0.09955264627933502   ACC:  96.875 %\n",
      "Epoch  17   1563  /  480 batch Loss:  0.09185784310102463   ACC:  100.0 %\n",
      "Epoch  17   1563  /  500 batch Loss:  0.21319197118282318   ACC:  87.5 %\n",
      "Epoch  17   1563  /  520 batch Loss:  0.2677776515483856   ACC:  90.625 %\n",
      "Epoch  17   1563  /  540 batch Loss:  0.3200562298297882   ACC:  90.625 %\n",
      "Epoch  17   1563  /  560 batch Loss:  0.4330785274505615   ACC:  84.375 %\n",
      "Epoch  17   1563  /  580 batch Loss:  0.14315690100193024   ACC:  93.75 %\n",
      "Epoch  17   1563  /  600 batch Loss:  0.10277806222438812   ACC:  96.875 %\n",
      "Epoch  17   1563  /  620 batch Loss:  0.21961098909378052   ACC:  87.5 %\n",
      "Epoch  17   1563  /  640 batch Loss:  0.19768212735652924   ACC:  90.625 %\n",
      "Epoch  17   1563  /  660 batch Loss:  0.13948281109333038   ACC:  96.875 %\n",
      "Epoch  17   1563  /  680 batch Loss:  0.1520942747592926   ACC:  96.875 %\n",
      "Epoch  17   1563  /  700 batch Loss:  0.17527922987937927   ACC:  90.625 %\n",
      "Epoch  17   1563  /  720 batch Loss:  0.13128331303596497   ACC:  96.875 %\n",
      "Epoch  17   1563  /  740 batch Loss:  0.1183035597205162   ACC:  100.0 %\n",
      "Epoch  17   1563  /  760 batch Loss:  0.15889842808246613   ACC:  96.875 %\n",
      "Epoch  17   1563  /  780 batch Loss:  0.31143850088119507   ACC:  90.625 %\n",
      "Epoch  17   1563  /  800 batch Loss:  0.02724653296172619   ACC:  100.0 %\n",
      "Epoch  17   1563  /  820 batch Loss:  0.12043172866106033   ACC:  93.75 %\n",
      "Epoch  17   1563  /  840 batch Loss:  0.17933890223503113   ACC:  93.75 %\n",
      "Epoch  17   1563  /  860 batch Loss:  0.1312171220779419   ACC:  90.625 %\n",
      "Epoch  17   1563  /  880 batch Loss:  0.21814511716365814   ACC:  93.75 %\n",
      "Epoch  17   1563  /  900 batch Loss:  0.18762648105621338   ACC:  96.875 %\n",
      "Epoch  17   1563  /  920 batch Loss:  0.1086883544921875   ACC:  96.875 %\n",
      "Epoch  17   1563  /  940 batch Loss:  0.16900113224983215   ACC:  93.75 %\n",
      "Epoch  17   1563  /  960 batch Loss:  0.15264281630516052   ACC:  93.75 %\n",
      "Epoch  17   1563  /  980 batch Loss:  0.18433216214179993   ACC:  87.5 %\n",
      "Epoch  17   1563  /  1000 batch Loss:  0.159549742937088   ACC:  93.75 %\n",
      "Epoch  17   1563  /  1020 batch Loss:  0.1050044596195221   ACC:  96.875 %\n",
      "Epoch  17   1563  /  1040 batch Loss:  0.11538426578044891   ACC:  96.875 %\n",
      "Epoch  17   1563  /  1060 batch Loss:  0.13294531404972076   ACC:  93.75 %\n",
      "Epoch  17   1563  /  1080 batch Loss:  0.2542901039123535   ACC:  90.625 %\n",
      "Epoch  17   1563  /  1100 batch Loss:  0.25811898708343506   ACC:  93.75 %\n",
      "Epoch  17   1563  /  1120 batch Loss:  0.22605447471141815   ACC:  93.75 %\n",
      "Epoch  17   1563  /  1140 batch Loss:  0.09560275077819824   ACC:  96.875 %\n",
      "Epoch  17   1563  /  1160 batch Loss:  0.07117638736963272   ACC:  100.0 %\n",
      "Epoch  17   1563  /  1180 batch Loss:  0.22683683037757874   ACC:  93.75 %\n",
      "Epoch  17   1563  /  1200 batch Loss:  0.41674432158470154   ACC:  87.5 %\n",
      "Epoch  17   1563  /  1220 batch Loss:  0.10800416767597198   ACC:  93.75 %\n",
      "Epoch  17   1563  /  1240 batch Loss:  0.2672169506549835   ACC:  93.75 %\n",
      "Epoch  17   1563  /  1260 batch Loss:  0.25586867332458496   ACC:  84.375 %\n",
      "Epoch  17   1563  /  1280 batch Loss:  0.275168776512146   ACC:  90.625 %\n",
      "Epoch  17   1563  /  1300 batch Loss:  0.09120185673236847   ACC:  96.875 %\n",
      "Epoch  17   1563  /  1320 batch Loss:  0.3600509762763977   ACC:  90.625 %\n",
      "Epoch  17   1563  /  1340 batch Loss:  0.06948031485080719   ACC:  100.0 %\n",
      "Epoch  17   1563  /  1360 batch Loss:  0.342721164226532   ACC:  87.5 %\n",
      "Epoch  17   1563  /  1380 batch Loss:  0.09588167071342468   ACC:  96.875 %\n",
      "Epoch  17   1563  /  1400 batch Loss:  0.7057657241821289   ACC:  78.125 %\n",
      "Epoch  17   1563  /  1420 batch Loss:  0.19403505325317383   ACC:  93.75 %\n",
      "Epoch  17   1563  /  1440 batch Loss:  0.21630680561065674   ACC:  93.75 %\n",
      "Epoch  17   1563  /  1460 batch Loss:  0.32493168115615845   ACC:  90.625 %\n",
      "Epoch  17   1563  /  1480 batch Loss:  0.12573082745075226   ACC:  93.75 %\n",
      "Epoch  17   1563  /  1500 batch Loss:  0.18353968858718872   ACC:  93.75 %\n",
      "Epoch  17   1563  /  1520 batch Loss:  0.29182305932044983   ACC:  90.625 %\n",
      "Epoch  17   1563  /  1540 batch Loss:  0.20986539125442505   ACC:  90.625 %\n",
      "Epoch  17   1563  /  1560 batch Loss:  0.23683340847492218   ACC:  90.625 %\n",
      "Test acc: 83.36 \n",
      "\n",
      "Epoch  18   1563  /  20 batch Loss:  0.1433003842830658   ACC:  93.75 %\n",
      "Epoch  18   1563  /  40 batch Loss:  0.09903182834386826   ACC:  96.875 %\n",
      "Epoch  18   1563  /  60 batch Loss:  0.06898121535778046   ACC:  96.875 %\n",
      "Epoch  18   1563  /  80 batch Loss:  0.10136884450912476   ACC:  96.875 %\n",
      "Epoch  18   1563  /  100 batch Loss:  0.14130061864852905   ACC:  93.75 %\n",
      "Epoch  18   1563  /  120 batch Loss:  0.10968878120183945   ACC:  90.625 %\n",
      "Epoch  18   1563  /  140 batch Loss:  0.05058968439698219   ACC:  96.875 %\n",
      "Epoch  18   1563  /  160 batch Loss:  0.14547501504421234   ACC:  93.75 %\n",
      "Epoch  18   1563  /  180 batch Loss:  0.08185253292322159   ACC:  96.875 %\n",
      "Epoch  18   1563  /  200 batch Loss:  0.057065945118665695   ACC:  100.0 %\n",
      "Epoch  18   1563  /  220 batch Loss:  0.1718558967113495   ACC:  90.625 %\n",
      "Epoch  18   1563  /  240 batch Loss:  0.2382599115371704   ACC:  87.5 %\n",
      "Epoch  18   1563  /  260 batch Loss:  0.07477838546037674   ACC:  96.875 %\n",
      "Epoch  18   1563  /  280 batch Loss:  0.22135929763317108   ACC:  93.75 %\n",
      "Epoch  18   1563  /  300 batch Loss:  0.16280493140220642   ACC:  93.75 %\n",
      "Epoch  18   1563  /  320 batch Loss:  0.22847266495227814   ACC:  90.625 %\n",
      "Epoch  18   1563  /  340 batch Loss:  0.358402281999588   ACC:  90.625 %\n",
      "Epoch  18   1563  /  360 batch Loss:  0.11101476103067398   ACC:  96.875 %\n",
      "Epoch  18   1563  /  380 batch Loss:  0.0680072233080864   ACC:  100.0 %\n",
      "Epoch  18   1563  /  400 batch Loss:  0.036297764629125595   ACC:  100.0 %\n",
      "Epoch  18   1563  /  420 batch Loss:  0.2301218956708908   ACC:  90.625 %\n",
      "Epoch  18   1563  /  440 batch Loss:  0.10475769639015198   ACC:  100.0 %\n",
      "Epoch  18   1563  /  460 batch Loss:  0.3257952034473419   ACC:  81.25 %\n",
      "Epoch  18   1563  /  480 batch Loss:  0.11937063932418823   ACC:  96.875 %\n",
      "Epoch  18   1563  /  500 batch Loss:  0.24861958622932434   ACC:  90.625 %\n",
      "Epoch  18   1563  /  520 batch Loss:  0.08234276622533798   ACC:  100.0 %\n",
      "Epoch  18   1563  /  540 batch Loss:  0.2281620353460312   ACC:  93.75 %\n",
      "Epoch  18   1563  /  560 batch Loss:  0.24773511290550232   ACC:  90.625 %\n",
      "Epoch  18   1563  /  580 batch Loss:  0.128274604678154   ACC:  93.75 %\n",
      "Epoch  18   1563  /  600 batch Loss:  0.15358977019786835   ACC:  93.75 %\n",
      "Epoch  18   1563  /  620 batch Loss:  0.17630423605442047   ACC:  90.625 %\n",
      "Epoch  18   1563  /  640 batch Loss:  0.1888345181941986   ACC:  90.625 %\n",
      "Epoch  18   1563  /  660 batch Loss:  0.39047423005104065   ACC:  87.5 %\n",
      "Epoch  18   1563  /  680 batch Loss:  0.17453202605247498   ACC:  93.75 %\n",
      "Epoch  18   1563  /  700 batch Loss:  0.0842883437871933   ACC:  96.875 %\n",
      "Epoch  18   1563  /  720 batch Loss:  0.20207734405994415   ACC:  93.75 %\n",
      "Epoch  18   1563  /  740 batch Loss:  0.24555449187755585   ACC:  90.625 %\n",
      "Epoch  18   1563  /  760 batch Loss:  0.0915146991610527   ACC:  93.75 %\n",
      "Epoch  18   1563  /  780 batch Loss:  0.08836039155721664   ACC:  96.875 %\n",
      "Epoch  18   1563  /  800 batch Loss:  0.07668866962194443   ACC:  96.875 %\n",
      "Epoch  18   1563  /  820 batch Loss:  0.16858971118927002   ACC:  93.75 %\n",
      "Epoch  18   1563  /  840 batch Loss:  0.29802435636520386   ACC:  87.5 %\n",
      "Epoch  18   1563  /  860 batch Loss:  0.29115426540374756   ACC:  87.5 %\n",
      "Epoch  18   1563  /  880 batch Loss:  0.13202841579914093   ACC:  96.875 %\n",
      "Epoch  18   1563  /  900 batch Loss:  0.23978036642074585   ACC:  90.625 %\n",
      "Epoch  18   1563  /  920 batch Loss:  0.16985970735549927   ACC:  87.5 %\n",
      "Epoch  18   1563  /  940 batch Loss:  0.27196717262268066   ACC:  90.625 %\n",
      "Epoch  18   1563  /  960 batch Loss:  0.24633240699768066   ACC:  93.75 %\n",
      "Epoch  18   1563  /  980 batch Loss:  0.48558562994003296   ACC:  84.375 %\n",
      "Epoch  18   1563  /  1000 batch Loss:  0.09260127693414688   ACC:  96.875 %\n",
      "Epoch  18   1563  /  1020 batch Loss:  0.1179673969745636   ACC:  100.0 %\n",
      "Epoch  18   1563  /  1040 batch Loss:  0.1513485312461853   ACC:  93.75 %\n",
      "Epoch  18   1563  /  1060 batch Loss:  0.08560427278280258   ACC:  96.875 %\n",
      "Epoch  18   1563  /  1080 batch Loss:  0.2084164321422577   ACC:  93.75 %\n",
      "Epoch  18   1563  /  1100 batch Loss:  0.10518671572208405   ACC:  96.875 %\n",
      "Epoch  18   1563  /  1120 batch Loss:  0.16064654290676117   ACC:  93.75 %\n",
      "Epoch  18   1563  /  1140 batch Loss:  0.09512197971343994   ACC:  96.875 %\n",
      "Epoch  18   1563  /  1160 batch Loss:  0.14270003139972687   ACC:  93.75 %\n",
      "Epoch  18   1563  /  1180 batch Loss:  0.04954485595226288   ACC:  100.0 %\n",
      "Epoch  18   1563  /  1200 batch Loss:  0.16425439715385437   ACC:  93.75 %\n",
      "Epoch  18   1563  /  1220 batch Loss:  0.3788444399833679   ACC:  87.5 %\n",
      "Epoch  18   1563  /  1240 batch Loss:  0.06939686089754105   ACC:  100.0 %\n",
      "Epoch  18   1563  /  1260 batch Loss:  0.37587642669677734   ACC:  78.125 %\n",
      "Epoch  18   1563  /  1280 batch Loss:  0.08872590959072113   ACC:  96.875 %\n",
      "Epoch  18   1563  /  1300 batch Loss:  0.21685992181301117   ACC:  90.625 %\n",
      "Epoch  18   1563  /  1320 batch Loss:  0.3266080915927887   ACC:  87.5 %\n",
      "Epoch  18   1563  /  1340 batch Loss:  0.11799892038106918   ACC:  96.875 %\n",
      "Epoch  18   1563  /  1360 batch Loss:  0.4351077079772949   ACC:  84.375 %\n",
      "Epoch  18   1563  /  1380 batch Loss:  0.1482703983783722   ACC:  93.75 %\n",
      "Epoch  18   1563  /  1400 batch Loss:  0.1270933598279953   ACC:  96.875 %\n",
      "Epoch  18   1563  /  1420 batch Loss:  0.30294162034988403   ACC:  90.625 %\n",
      "Epoch  18   1563  /  1440 batch Loss:  0.19413262605667114   ACC:  87.5 %\n",
      "Epoch  18   1563  /  1460 batch Loss:  0.360475093126297   ACC:  84.375 %\n",
      "Epoch  18   1563  /  1480 batch Loss:  0.2718152701854706   ACC:  93.75 %\n",
      "Epoch  18   1563  /  1500 batch Loss:  0.27806562185287476   ACC:  93.75 %\n",
      "Epoch  18   1563  /  1520 batch Loss:  0.11628903448581696   ACC:  96.875 %\n",
      "Epoch  18   1563  /  1540 batch Loss:  0.08344106376171112   ACC:  96.875 %\n",
      "Epoch  18   1563  /  1560 batch Loss:  0.2658447325229645   ACC:  90.625 %\n",
      "Test acc: 82.67 \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2197/3368660309.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/mrjaehong/handwriting_gen/pytorch-Stand-Alone-Self-Attention/env/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/mrjaehong/handwriting_gen/pytorch-Stand-Alone-Self-Attention/env/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "print_interval = 20\n",
    "\n",
    "## 30에폭 마다 러닝레이트 줄임\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    decayed_lr = lr * (0.1 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = decayed_lr\n",
    "\n",
    "for epoch in range(1,epochs+1):\n",
    "    model.train()\n",
    "    train_acc = 0.0\n",
    "    step = 0\n",
    "    for data, target in train_loader:\n",
    "        adjust_learning_rate(optimizer, epoch)\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "    \n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        ## onnx 저장\n",
    "\n",
    "\n",
    "        y_pred = output.data.max(1)[1]\n",
    "        \n",
    "        acc = float(y_pred.eq(target.data).sum()) / len(data) * 100.\n",
    "        train_acc += acc\n",
    "        step += 1\n",
    "        if step % print_interval == 0:\n",
    "            print(\"Epoch \",epoch,\" \",len(train_loader),\" / \",step,\"batch Loss: \",loss.item(),\"  ACC: \",acc,\"%\")\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            output = model(data)\n",
    "            prediction = output.data.max(1)[1]\n",
    "            correct += prediction.eq(target.data).sum()\n",
    "\n",
    "    acc = 100. * float(correct) / len(test_loader.dataset)\n",
    "    print('Test acc: {0:.2f} \\n'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## onnx 저장\n",
    "## operator_export_type=torch.onnx.OperatorExportTypes.ONNX_ATEN_FALLBACK 꼭 해줘야함\n",
    "## unfold 연산을 \n",
    "# https://github.com/pytorch/pytorch/issues/14395#issuecomment-444697403\n",
    "\n",
    "\n",
    "# import torch.onnx\n",
    "# torch.onnx.export(model.cpu(),data.cpu(),'./stand_alone_self.onnx',export_params=False,opset_version=12,\n",
    "#                   operator_export_type=torch.onnx.OperatorExportTypes.ONNX_ATEN_FALLBACK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 모델 저장\n",
    "torch.save(model.state_dict(),\"./temp_save_model/ch.pth\")\n",
    "\n",
    "## 모델 불러오기\n",
    "model.load_state_dict(torch.load(\"./temp_save_model/ch.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "\n",
    "## 모델에 넣을데이터 출력\n",
    "img = data[7:8,:,:,:]\n",
    "print(\"인풋 이미지 텐서 사이즈\",img.shape)\n",
    "print(\"이게뭐지?(타겟값)\",target[7:8])\n",
    "img_test = img.detach().cpu().numpy()\n",
    "img_test = img_test.transpose(0,2,3,1)\n",
    "img_test = img_test[0,:,:,:]\n",
    "plt.axis(\"off\");\n",
    "plt.imshow(img_test);\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_children = list(model.children())\n",
    "print(\"모델의 모듈 구조길이 : \",len(model_children))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,module in enumerate(model_children):\n",
    "#     print(i,\"번째 모듈 \\n\",module,\"\\n\")\n",
    "model_children[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [model_children[0](img)]\n",
    "\n",
    "print(\"인풋 이미지 데이터가 첫번째 컨볼루션 레이어(스탬) 통과후 텐서 쉐이프 = \",results[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 인풋 데이터 모든 레이어 통과를 result 리스트에 저장\n",
    "for i in range(1, len(model_children)-1):\n",
    "    # 마지막 레이어의 결과를 다음 레이어로 전달\n",
    "    results.append(model_children[i](results[-1]))\n",
    "outputs = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"1번째 레이어 생김새\",\"\\n\",model_children[1],\"\\n\")\n",
    "print(\"통과후 텐서 쉐이프\",outputs[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. cnn 통과후  torch.Size([32, 64, 32, 32])\n",
    "# 2. 어텐션 통과후  torch.Size([32, 64, 32, 32])\n",
    "# 3.cnn 통과후  torch.Size([32, 256, 32, 32])\n",
    "# 숏컷 연산후 torch.Size([32, 256, 32, 32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "07b1216af5147bda247a947cdd561e2b589c1fe1b25ba98cf598d38fde5bfe85"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
