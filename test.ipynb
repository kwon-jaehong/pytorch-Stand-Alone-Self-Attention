{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from model import ResNet50, ResNet38, ResNet26\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1105b55a63fa457e80bac18c0bcc3677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/cifar-10-python.tar.gz to data\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## 데이터 셋은 CIFAR10으로\n",
    "num_classes = 10\n",
    "batch_size = 10\n",
    "num_workers = 4\n",
    "\n",
    "## 데이터셋 로드\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=(0.4914, 0.4822, 0.4465),\n",
    "        std=(0.2470, 0.2435, 0.2616)\n",
    "    )\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=(0.4914, 0.4822, 0.4465),\n",
    "        std=(0.2470, 0.2435, 0.2616)\n",
    "    )\n",
    "])\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('data', train=True, download=True, transform=transform_train),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('data', train=False, transform=transform_test),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem = False\n",
    "model = ResNet26(num_classes=num_classes, stem=stem)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 1\n",
    "best_acc = 0.0\n",
    "\n",
    "lr = 0.1\n",
    "momentum =0.9\n",
    "weight_decay = 0.0001\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1   5000  /  20 batch Loss:  9.364157676696777   ACC:  20.0 %\n",
      "Epoch  1   5000  /  40 batch Loss:  3.7577877044677734   ACC:  0.0 %\n",
      "Epoch  1   5000  /  60 batch Loss:  2.3191561698913574   ACC:  0.0 %\n",
      "Epoch  1   5000  /  80 batch Loss:  2.3303494453430176   ACC:  10.0 %\n",
      "Epoch  1   5000  /  100 batch Loss:  2.426391124725342   ACC:  10.0 %\n",
      "Epoch  1   5000  /  120 batch Loss:  2.3636438846588135   ACC:  0.0 %\n",
      "Epoch  1   5000  /  140 batch Loss:  2.285841941833496   ACC:  10.0 %\n",
      "Epoch  1   5000  /  160 batch Loss:  2.2595152854919434   ACC:  20.0 %\n",
      "Epoch  1   5000  /  180 batch Loss:  2.335702896118164   ACC:  10.0 %\n",
      "Epoch  1   5000  /  200 batch Loss:  2.3778865337371826   ACC:  0.0 %\n",
      "Epoch  1   5000  /  220 batch Loss:  2.3104355335235596   ACC:  10.0 %\n",
      "Epoch  1   5000  /  240 batch Loss:  2.1990444660186768   ACC:  20.0 %\n",
      "Epoch  1   5000  /  260 batch Loss:  2.32319974899292   ACC:  10.0 %\n",
      "Epoch  1   5000  /  280 batch Loss:  2.249321699142456   ACC:  10.0 %\n",
      "Epoch  1   5000  /  300 batch Loss:  2.3436903953552246   ACC:  10.0 %\n",
      "Epoch  1   5000  /  320 batch Loss:  2.226386308670044   ACC:  10.0 %\n",
      "Epoch  1   5000  /  340 batch Loss:  2.596191644668579   ACC:  0.0 %\n",
      "Epoch  1   5000  /  360 batch Loss:  2.384669542312622   ACC:  0.0 %\n",
      "Epoch  1   5000  /  380 batch Loss:  2.2792131900787354   ACC:  0.0 %\n",
      "Epoch  1   5000  /  400 batch Loss:  2.4883313179016113   ACC:  0.0 %\n",
      "Epoch  1   5000  /  420 batch Loss:  2.354663848876953   ACC:  0.0 %\n",
      "Epoch  1   5000  /  440 batch Loss:  2.3384151458740234   ACC:  10.0 %\n",
      "Epoch  1   5000  /  460 batch Loss:  2.243756055831909   ACC:  0.0 %\n",
      "Epoch  1   5000  /  480 batch Loss:  2.327007532119751   ACC:  10.0 %\n",
      "Epoch  1   5000  /  500 batch Loss:  2.2270240783691406   ACC:  0.0 %\n",
      "Epoch  1   5000  /  520 batch Loss:  2.035651683807373   ACC:  30.0 %\n",
      "Epoch  1   5000  /  540 batch Loss:  2.2162811756134033   ACC:  20.0 %\n",
      "Epoch  1   5000  /  560 batch Loss:  2.352165699005127   ACC:  0.0 %\n",
      "Epoch  1   5000  /  580 batch Loss:  2.154616594314575   ACC:  20.0 %\n",
      "Epoch  1   5000  /  600 batch Loss:  2.1056246757507324   ACC:  20.0 %\n",
      "Epoch  1   5000  /  620 batch Loss:  2.054661989212036   ACC:  20.0 %\n",
      "Epoch  1   5000  /  640 batch Loss:  2.1450328826904297   ACC:  10.0 %\n",
      "Epoch  1   5000  /  660 batch Loss:  2.3935132026672363   ACC:  10.0 %\n",
      "Epoch  1   5000  /  680 batch Loss:  1.926984429359436   ACC:  20.0 %\n",
      "Epoch  1   5000  /  700 batch Loss:  2.0859334468841553   ACC:  40.0 %\n",
      "Epoch  1   5000  /  720 batch Loss:  2.4160845279693604   ACC:  0.0 %\n",
      "Epoch  1   5000  /  740 batch Loss:  2.475543975830078   ACC:  10.0 %\n",
      "Epoch  1   5000  /  760 batch Loss:  2.4623501300811768   ACC:  10.0 %\n",
      "Epoch  1   5000  /  780 batch Loss:  2.199270248413086   ACC:  20.0 %\n",
      "Epoch  1   5000  /  800 batch Loss:  1.7929614782333374   ACC:  40.0 %\n",
      "Epoch  1   5000  /  820 batch Loss:  2.0643057823181152   ACC:  40.0 %\n",
      "Epoch  1   5000  /  840 batch Loss:  2.321302890777588   ACC:  10.0 %\n",
      "Epoch  1   5000  /  860 batch Loss:  2.0825278759002686   ACC:  30.0 %\n",
      "Epoch  1   5000  /  880 batch Loss:  2.3994486331939697   ACC:  0.0 %\n",
      "Epoch  1   5000  /  900 batch Loss:  2.293788433074951   ACC:  0.0 %\n",
      "Epoch  1   5000  /  920 batch Loss:  2.0836634635925293   ACC:  20.0 %\n",
      "Epoch  1   5000  /  940 batch Loss:  2.2103257179260254   ACC:  0.0 %\n",
      "Epoch  1   5000  /  960 batch Loss:  2.302253007888794   ACC:  10.0 %\n",
      "Epoch  1   5000  /  980 batch Loss:  1.9651517868041992   ACC:  30.0 %\n",
      "Epoch  1   5000  /  1000 batch Loss:  2.230788230895996   ACC:  20.0 %\n",
      "Epoch  1   5000  /  1020 batch Loss:  2.242880344390869   ACC:  0.0 %\n",
      "Epoch  1   5000  /  1040 batch Loss:  2.094705581665039   ACC:  40.0 %\n",
      "Epoch  1   5000  /  1060 batch Loss:  2.3228025436401367   ACC:  10.0 %\n",
      "Epoch  1   5000  /  1080 batch Loss:  2.404773235321045   ACC:  10.0 %\n",
      "Epoch  1   5000  /  1100 batch Loss:  1.9044519662857056   ACC:  10.0 %\n",
      "Epoch  1   5000  /  1120 batch Loss:  2.5904719829559326   ACC:  0.0 %\n",
      "Epoch  1   5000  /  1140 batch Loss:  2.1014461517333984   ACC:  40.0 %\n",
      "Epoch  1   5000  /  1160 batch Loss:  2.097278118133545   ACC:  20.0 %\n",
      "Epoch  1   5000  /  1180 batch Loss:  2.2519679069519043   ACC:  10.0 %\n",
      "Epoch  1   5000  /  1200 batch Loss:  2.327714204788208   ACC:  10.0 %\n",
      "Epoch  1   5000  /  1220 batch Loss:  1.834890365600586   ACC:  30.0 %\n",
      "Epoch  1   5000  /  1240 batch Loss:  1.8904688358306885   ACC:  30.0 %\n",
      "Epoch  1   5000  /  1260 batch Loss:  2.1874756813049316   ACC:  30.0 %\n",
      "Epoch  1   5000  /  1280 batch Loss:  2.4507033824920654   ACC:  10.0 %\n",
      "Epoch  1   5000  /  1300 batch Loss:  2.3759143352508545   ACC:  10.0 %\n",
      "Epoch  1   5000  /  1320 batch Loss:  1.7214908599853516   ACC:  40.0 %\n",
      "Epoch  1   5000  /  1340 batch Loss:  1.971245527267456   ACC:  30.0 %\n",
      "Epoch  1   5000  /  1360 batch Loss:  2.0062291622161865   ACC:  40.0 %\n",
      "Epoch  1   5000  /  1380 batch Loss:  1.92340087890625   ACC:  50.0 %\n",
      "Epoch  1   5000  /  1400 batch Loss:  1.976876974105835   ACC:  20.0 %\n",
      "Epoch  1   5000  /  1420 batch Loss:  1.9983749389648438   ACC:  0.0 %\n",
      "Epoch  1   5000  /  1440 batch Loss:  2.1334283351898193   ACC:  20.0 %\n",
      "Epoch  1   5000  /  1460 batch Loss:  1.9475862979888916   ACC:  20.0 %\n",
      "Epoch  1   5000  /  1480 batch Loss:  1.7451903820037842   ACC:  40.0 %\n",
      "Epoch  1   5000  /  1500 batch Loss:  1.9158128499984741   ACC:  30.0 %\n",
      "Epoch  1   5000  /  1520 batch Loss:  2.294095516204834   ACC:  30.0 %\n",
      "Epoch  1   5000  /  1540 batch Loss:  2.5198237895965576   ACC:  10.0 %\n",
      "Epoch  1   5000  /  1560 batch Loss:  2.2577896118164062   ACC:  30.0 %\n",
      "Epoch  1   5000  /  1580 batch Loss:  2.0403923988342285   ACC:  40.0 %\n",
      "Epoch  1   5000  /  1600 batch Loss:  2.0477442741394043   ACC:  20.0 %\n",
      "Epoch  1   5000  /  1620 batch Loss:  2.2573020458221436   ACC:  20.0 %\n",
      "Epoch  1   5000  /  1640 batch Loss:  1.8047345876693726   ACC:  50.0 %\n",
      "Epoch  1   5000  /  1660 batch Loss:  2.039177417755127   ACC:  10.0 %\n",
      "Epoch  1   5000  /  1680 batch Loss:  1.719130516052246   ACC:  40.0 %\n",
      "Epoch  1   5000  /  1700 batch Loss:  1.9084863662719727   ACC:  30.0 %\n",
      "Epoch  1   5000  /  1720 batch Loss:  2.184998035430908   ACC:  50.0 %\n",
      "Epoch  1   5000  /  1740 batch Loss:  1.89931321144104   ACC:  10.0 %\n",
      "Epoch  1   5000  /  1760 batch Loss:  1.7079492807388306   ACC:  50.0 %\n",
      "Epoch  1   5000  /  1780 batch Loss:  1.9578319787979126   ACC:  40.0 %\n",
      "Epoch  1   5000  /  1800 batch Loss:  1.6602296829223633   ACC:  50.0 %\n",
      "Epoch  1   5000  /  1820 batch Loss:  2.115469455718994   ACC:  20.0 %\n",
      "Epoch  1   5000  /  1840 batch Loss:  1.8210681676864624   ACC:  20.0 %\n",
      "Epoch  1   5000  /  1860 batch Loss:  2.1196446418762207   ACC:  10.0 %\n",
      "Epoch  1   5000  /  1880 batch Loss:  2.4139504432678223   ACC:  10.0 %\n",
      "Epoch  1   5000  /  1900 batch Loss:  2.1051154136657715   ACC:  40.0 %\n",
      "Epoch  1   5000  /  1920 batch Loss:  1.9413045644760132   ACC:  20.0 %\n",
      "Epoch  1   5000  /  1940 batch Loss:  1.9730288982391357   ACC:  20.0 %\n",
      "Epoch  1   5000  /  1960 batch Loss:  1.313016414642334   ACC:  60.0 %\n",
      "Epoch  1   5000  /  1980 batch Loss:  2.0079185962677   ACC:  20.0 %\n",
      "Epoch  1   5000  /  2000 batch Loss:  2.066999912261963   ACC:  20.0 %\n",
      "Epoch  1   5000  /  2020 batch Loss:  1.6960121393203735   ACC:  20.0 %\n",
      "Epoch  1   5000  /  2040 batch Loss:  2.1503357887268066   ACC:  0.0 %\n",
      "Epoch  1   5000  /  2060 batch Loss:  1.9645389318466187   ACC:  20.0 %\n",
      "Epoch  1   5000  /  2080 batch Loss:  1.861228585243225   ACC:  10.0 %\n",
      "Epoch  1   5000  /  2100 batch Loss:  2.3439571857452393   ACC:  10.0 %\n",
      "Epoch  1   5000  /  2120 batch Loss:  2.178867816925049   ACC:  20.0 %\n",
      "Epoch  1   5000  /  2140 batch Loss:  1.8650767803192139   ACC:  20.0 %\n",
      "Epoch  1   5000  /  2160 batch Loss:  1.933529257774353   ACC:  20.0 %\n",
      "Epoch  1   5000  /  2180 batch Loss:  2.1660213470458984   ACC:  10.0 %\n",
      "Epoch  1   5000  /  2200 batch Loss:  2.322800874710083   ACC:  30.0 %\n",
      "Epoch  1   5000  /  2220 batch Loss:  1.8099483251571655   ACC:  50.0 %\n",
      "Epoch  1   5000  /  2240 batch Loss:  2.237832546234131   ACC:  20.0 %\n",
      "Epoch  1   5000  /  2260 batch Loss:  2.0842020511627197   ACC:  10.0 %\n",
      "Epoch  1   5000  /  2280 batch Loss:  2.043318033218384   ACC:  20.0 %\n",
      "Epoch  1   5000  /  2300 batch Loss:  2.5297975540161133   ACC:  10.0 %\n",
      "Epoch  1   5000  /  2320 batch Loss:  2.2986702919006348   ACC:  10.0 %\n",
      "Epoch  1   5000  /  2340 batch Loss:  2.0976505279541016   ACC:  20.0 %\n",
      "Epoch  1   5000  /  2360 batch Loss:  2.0497403144836426   ACC:  30.0 %\n",
      "Epoch  1   5000  /  2380 batch Loss:  1.8572574853897095   ACC:  20.0 %\n",
      "Epoch  1   5000  /  2400 batch Loss:  1.9965426921844482   ACC:  20.0 %\n",
      "Epoch  1   5000  /  2420 batch Loss:  2.1876659393310547   ACC:  10.0 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16621/2935631994.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mtrain_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "print_interval = 20\n",
    "\n",
    "## 30에폭 마다 러닝레이트 줄임\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    decayed_lr = lr * (0.1 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = decayed_lr\n",
    "\n",
    "for epoch in range(1,epochs+1):\n",
    "    model.train()\n",
    "    train_acc = 0.0\n",
    "    step = 0\n",
    "    for data, target in train_loader:\n",
    "        adjust_learning_rate(optimizer, epoch)\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        ## onnx 저장\n",
    "\n",
    "\n",
    "        y_pred = output.data.max(1)[1]\n",
    "        \n",
    "        acc = float(y_pred.eq(target.data).sum()) / len(data) * 100.\n",
    "        train_acc += acc\n",
    "        step += 1\n",
    "        if step % print_interval == 0:\n",
    "            print(\"Epoch \",epoch,\" \",len(train_loader),\" / \",step,\"batch Loss: \",loss.item(),\"  ACC: \",acc,\"%\")\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            output = model(data)\n",
    "            prediction = output.data.max(1)[1]\n",
    "            correct += prediction.eq(target.data).sum()\n",
    "\n",
    "    acc = 100. * float(correct) / len(test_loader.dataset)\n",
    "    print('Test acc: {0:.2f} \\n'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## onnx 저장\n",
    "## operator_export_type=torch.onnx.OperatorExportTypes.ONNX_ATEN_FALLBACK 꼭 해줘야함\n",
    "## unfold 연산을 \n",
    "# https://github.com/pytorch/pytorch/issues/14395#issuecomment-444697403\n",
    "\n",
    "\n",
    "# import torch.onnx\n",
    "# torch.onnx.export(model.cpu(),data.cpu(),'./stand_alone_self.onnx',export_params=False,opset_version=12,\n",
    "#                   operator_export_type=torch.onnx.OperatorExportTypes.ONNX_ATEN_FALLBACK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "07b1216af5147bda247a947cdd561e2b589c1fe1b25ba98cf598d38fde5bfe85"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
